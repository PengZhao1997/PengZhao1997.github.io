<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[AAAI2019 | 旷视提出任意形状的场景文本检测方法：SPCNet]]></title>
    <url>%2F%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%2FAAAI2019-%E6%97%B7%E8%A7%86%E6%8F%90%E5%87%BA%E4%BB%BB%E6%84%8F%E5%BD%A2%E7%8A%B6%E7%9A%84%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95%EF%BC%9ASPCNet%2F</url>
    <content type="text"><![CDATA[Scene Text Detection with Supervised Pyramid Context NetworkKeyWords Plus: AAAI2019 Curved Textpaper：https://arxiv.org/abs/1811.08605reference: Xie E, Zang Y, Shao S, et al. Scene text detection with supervised pyramid context network[J]. arXiv preprint arXiv:1811.08605, 2018.Github: 未开源 引言 基于深度学习的场景文本检测方法在过去几年取得了显著成效。然而，由于自然场景的高度多样性和复杂性，当应用于在现实世界环境中捕获的图像时，目前最先进的文本检测方法仍可能产生大量假阳性结果。为了解决这个问题，作者受到Mask R-CNN的启发，文中提出了一种有效的场景文本检测模型，该模型基于特征金字塔网络（FPN）和实例分割。作者提出了一个监督金字塔上下文网络（SPCNET）来精确定位文本区域，同时抑制假阳性。 受益于语义信息的指导和共享FPN，SPCNET在引入边界额外计算的同时获得了显著提高的性能。标准数据集上的实验表明，我们的SPCNET明显优于最先进的方法。具体而言，ICDAR2013上达到92.1％，ICDAR2015为87.2％，ICDAR2017 MLT为74.1％，Total-Text上达到82.9％。 内容 为了检测任意形状的文本，通常采用基于实例分割的方法。目前实例分割方法，例如Mask R-CNN，通常被利用为多任务学习问题：（1）将前景目标建议与背景区分开，并为它们分配适当的类标签。（2）对每个前景建议进行回归和分割。 然而，简单地将Mask-RCNN转移到文本检测场景容易引起一些问题，原因如下：（1）缺乏上下文信息线索。自然场景中的假阳性往往与周围场景密切相关。例如，餐具经常出现在桌子上，并且围栏通常分批出现。然而，Mask R-CNN在一个感兴趣的区域中区分对象，缺少全局语义信息指导。因此，在没有上下文信息线索帮助的情况下，它往往会导致一些具有相似纹理信息的对象的分类错误。（2）分类评分不准确。在处理倾斜文本时，Mask R-CNN的分类分数很容易不准确。因为对于倾斜文本，Mask R-CNN基于水平建议粗略地给出分类分数，而背景占据很大比例。因此，当面对倾斜文本时，Mask R-CNN的分类得分往往较低。 本文在Mask R-CNN 的启发下，提出了一种基于语义信息的形状鲁棒的文本检测器，利用生成目标的形状分支的输出来定位文本区域。因此可以灵活地检测任意形状的文本。 为了解决FP上缺乏上下文信息线索和分类评分不准确的问题，作者提出了一个文本上下模块和重新评分机制（基于mask R-CNN）。对于上下文信息，用语义分割分支来辅助指导检测分支捕获上下文信息。通过对全局语义的特征的弥补，网络更好的区分了误报情况。 在评分机制上，对分割图上的激活值和分类评分进行补偿，得到一个融合的分数，在处理倾斜的文本时，虽然分数相对较低，但对分割图的响应较强，这也使得准确的融合得分较高重评分机制可以进一步减少FP的数量。这是因为FP在分割图上的响应很弱，导致融合分数较低。因此，分数较低的得分在推理过程中更容易被过滤掉。重评分机制的可视化结果如图1。 标签生成 文本实例的真值在图3中举例说明。与常见的实例分割数据集不同，不提供像素级文本/非文本标注。将多边形中的像素视为文本，多边形外的像素视为非文本，然后得到文本区域的实例。多边形的最小边界水平矩形将被视为边界框。以与实例生成相同的方式生成全局二值映射。 提出的方法 文章提出的方法的总体结构如图2所示。网络由五部分组成：特征金字塔网络（FPN），区域提议网络（RPN），R-CNN分支，掩模预测分支和全局文本分割预测分支。特征金字塔网络（FPN）是一种广泛应用于当前主流检测模型的特征融合结构。FPN采用自上而下的横向连接架构，从单一尺度输入构建网络内的特征金字塔。区域建议网络（RPN）生成可能包含目标建议的边界框。通过Roi-Align，所有建议的大小调整为R-CNN分支的7×7和掩模预测分支的14×14。全局文本分割分支作用于FPN的每个阶段以生成文本的语义分割图。 Text Context Module 抑制假阳性对于一般目标检测和文本检测来说是一个具有挑战性的问题。 在自然场景中，一些常规目标，例如光盘、栅栏等，很容易被检测网络视为文本。Mask R-CNN使用感兴趣区域（ROI）来分类建议是文本还是背景。然而，使用仅从一个感兴趣区域提取的特征来执行文本区域分类。由于自然场景中的误报通常不会出现意外，例如光盘更有可能出现在桌面上，引入上下文信息有助于网络提取更多的辨别功能并准确地对建议进行分类。此文的文本上下文模块（TCM）由两个子模块组成：金字塔注意模块（PAM）和金字塔融合模块（PFM）。像素图被输入TCM，TCM将文本分割结果作为输出。 Pyramid Attention Module 金字塔注意模块和SSTD相似，还在FPN从stage2到stage5之后添加了一个全局文本分割分支。它为每个FPN层生成像素级文本/非文本区域的显著图。注意模块和融合模块共享一个分支，命名为文本上下文模块，包括两个3×3卷积层和一个1×1卷积层。输出显著图包括两个通道，即文本/非文本图。 增强显著图并使用它来激活像素图上的文本区域。具体来说，以stage2为例，给出512×512的输入样本，特征映射S2∈R128×128×256。显著图的生成如下： 其中Text Context模块生成具有2个通道的显著图。然后在通道方面softmax之后，我们获得文本显著图。通过指数激活，增强图显著增强，即文本/非文本区域中的响应间隙变大。显著图将作用于特征图，如下所示： 显著性映射广播到与S2相同的256通道，操作符表示两个映射S2和显著性映射的逐像素相乘运算。 Pyramid Fusion Module 金字塔融合模块。PFM将检测特征与深度监督语义特征相结合，使网络更具辨别力，区分文本和非文本。具体而言，语义分割从单个像素的角度检查文本，并通过组合周围像素的信息来确定文本区域， 检测通过ROI对文本区域进行分类。两个分支之间存在天然的互补关系。在文本上下文模块的第一个3×3卷积层之后，得到全局文本分割的特征映射（GTF）。这些特征捕获了诸如背景和文本的语义分割等补充信息。将其引入原始特征图使得Mask R-CNN在分类任务上表现更强。具体细节如下： 其中Conv3×3是文本上下文模块中的第一个Conv层，GTF表示全局文本特征。然后“+”表示元素加法运算。 Re-Score Mechanism 对于标准Mask R-CNN推理处理，预先确定的top-K（例如，1000）边界框按分类置信度排序，然后在标准NMS处理之后，最多到顶部M（例如，300）边界框具有最高 保留分类置信度。这些边界框被馈送到Mask R-CNN作为生成预测文本实例映射的建议。此方法将一个水平边界框的分类置信度视为分数，然后人为设置阈值以过滤掉背景框。但是，此方法将过滤掉一些具有低分数的正积极像素，因为如果水平边界框包含倾斜文本实例，则它还伴随着大量背景信息。同时，将保留一些置信度相对较高的FP，我们为每个文本实例重新分配得分。可视化图如图4所示。文本实例的融合得分由两部分组成：分类得分（CS）和实例得分（IS）。形式上，给定预测的2级得分 的第i个建议的融合得分是通过以下softmax函数计算的： 其中CS由Mask R-CNN分类分支直接获得，IS是全局文本分割图上文本实例的激活值。具体来说，对于每个文本实例，它被投影到文本分割图上，包含 ，并计算文本实例区域中pi的均值： 其中Pi是文本分割图上第i个文本实例的像素值集。融合得分将分类得分与实例得分相结合，这可以有效地降低FP置信度，因为FP实例往往比分割图上的文本具有更弱的响应。 Loss Function Design 与Mask R-CNN类似，网络包括多任务。在Mask R-CNN的loss函数设计之后，在此基础上添加了全局文本分割loss。loss表达式如下： 其中Lrpn，Lcls，Lbox和Lmask是Mask R-CNN的标准损失。Lgts用于优化全局文本分割，定义如下： Lgts是Softmax损失，其中p是网络的输出预测。 多任务学习是从同一输入中学习多个互补任务的有效表示的过程，并且已经发现它可以改善两个任务的性能。该方法使网络能够通过端到端交叉训练来学习文本检测和全局文本分割，从而允许来自两个任务的梯度影响共享特征图。 实验消融学习 在ICDAR2017上的效果：文本上下文模块在保持召回相同的同时实现了4.1％的精度提高，重评分机制可以进一步提高基于TCM的3.9％的精度。 与现有方法的对比 在icdar17,15,13和total-text(多语言，多方向，水平文本，弯曲文本)上都是state-of-the-art，充分证明了该方法的有效性。 在ICDAR2015上对比一些经典方法的FP情况，可以看到我们的方法可以有效抑制FP。]]></content>
      <categories>
        <category>文本检测</category>
      </categories>
      <tags>
        <tag>文本检测</tag>
        <tag>曲线文本</tag>
        <tag>旷视</tag>
        <tag>Mask R-CNN</tag>
        <tag>SPCNet</tag>
        <tag>AAAI2019</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CVPR2017 | 旷视提出自然场景文本检测方法：EAST]]></title>
    <url>%2F%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%2FCVPR2017-%E6%97%B7%E8%A7%86%E6%8F%90%E5%87%BA%E8%87%AA%E7%84%B6%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95%EF%BC%9AEAST%2F</url>
    <content type="text"><![CDATA[EAST: An Efficient and Accurate Scene Text DetectorKeyWords Plus: CVPR2017 Multi-Oriented Textpaper：http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdfreference: Zhou X, Yao C, Wen H, et al. EAST: an efficient and accurate scene text detector[C]//Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2017: 5551-5560.Github: https://github.com/argman/EAST Abstract 先前的场景文本检测方法已经在各种基准测试中取得了很好的效果。然而，在处理具有挑战性的情况时，即使配备了深度神经网络模型，它们通常也会达不到，因为整体性能取决于方法中多个阶段和组件的相互作用。在这项工作中，我们提出了一个简单而强大的方法，可以在自然场景中快速而准确地进行文本检测。这个方法直接预测完整图像中任意方向和四边形形状的单词或文本行，消除了使用单个神经网络的不必要的中间步骤（例如，候选聚合和字分区）。我们的方法简单，可以集中精力设计损失函数和神经网络架构。对标准数据集（包括ICDAR 2015，COCO-Text和MSRA-TD500）的实验表明，所提出的算法在准确性和效率方面明显优于最先进的方法。在ICDAR 2015数据集上，所提出的算法在720p分辨率下以13.2fps达到0.7820的F分数。 Introduction 最近，提取和理解自然场景中包含的文本信息变得越来越重要和受欢迎，ICDAR系列竞赛的前所未有的大量参与者以及NIST对TRAIT 2016评估的启动证明了这一点。 文本检测作为后续过程的先决条件，在文本信息提取和理解的整个过程中起着至关重要的作用。先前的文本检测方法已经在该领域的各种基准上获得了不错的表现。文本检测的核心是区分文本和背景的函数设计。传统上，函数是手动设计来捕捉场景文本的属性，而基于深度学习的方法有效的函数直接从训练数据中学习。 图1. ICDAR 2015 文本鲁棒挑战的性能与速度。可以看出，我们的算法在准确性方面明显优于竞争对手，同时运行速度非常快。使用的硬件规格列于表6。 然而，现有的方法，无论是传统的还是基于深度神经网络的，主要由几个阶段和组件组成，这些阶段和组件可能是次优的和耗时的。因此，这些方法的准确性和效率仍远远不能令人满意。 在本文中，我们提出了一个快速准确的场景文本检测方法，它只有两个阶段。该方法使用完全卷积网络（FCN）模型，该模型直接生成单词或文本行级别预测，不包括冗余和慢速中间步骤。生成的文本预测（可以是旋转的矩形或四边形）直接进行非最大值抑制以产生最终结果。根据标准基准的定性和定量实验，与现有方法相比，该算法实现了显著提高的性能，同时运行速度更快。 具体而言，所提出的算法在ICDAR 2015（在多尺度下测试时为0.8072），在MSRA-TD500上为0.7608，在COCO-Text上为0.3945时达到了0.7820的F分数，优于以前的结果：性能最先进的算法，平均花费的时间少得多（在Titan-X GPU上，我们最佳性能模型的分辨率为720p，分辨率为13.2fps，最快模型为16.8fps）。 这项工作的贡献分为三方面： 我们提出了一种场景文本检测方法，包括两个阶段：完全卷积网络和NMS合并阶段。FCN直接生成文本区域，不包括冗余和耗时的中间步骤。 这个方法可以灵活地生成字级或线级预测，其几何形状可以是旋转框或四边形，具体取决于具体应用。 所提出的算法在精度和速度方面明显优于最先进的方法。 Related Work 场景文本的检测和识别已经成为计算机视觉领域长期积极的研究课题。众多鼓舞人心的思想和有效的方法已经进行过调查。综合评论和详细分析可以在调查论文中找到。本节将重点介绍与所提算法最相关的工作。 传统方法依赖于手动设计的特征。基于笔画宽度变换（SWT）和最大稳定极值区域（MSER）的方法通常通过边缘检测或极值区域提取来寻找候选字符。张等人，利用文本的局部对称性，并为文本区域检测设计了各种特征。FASText是一个快速文本检测系统，适应和修改了众所周知的用于笔画提取的快速关键点检测器。然而，就精度和适应性而言，这些方法落后于基于深度神经网络的方法，尤其是在处理具有挑战性的场景时，例如低分辨率和几何失真。 最近，场景文本检测领域进入了一个新的时代，基于深度神经网络的算法逐渐成为主流。黄等人，首先找到使用MSER的候选框，然后使用深度卷积网络作为强分类器来修剪误报。Jaderberg等人的方法，以滑动窗口的方式扫描图像，并使用卷积神经网络模型为每个尺度生成密集的热图。后来，Jaderberg等人，同时使用CNN和ACF来搜索候选词，并使用回归进一步细化它们。田等人，开发了垂直锚点，并构建了CNN-RNN联合模型来检测水平文本行。与这些方法不同，张等人，建议利用FCN进行热图生成，并使用分量投影进行方位估计。这些方法在标准基准测试中获得了优异的性能。然而，如图2（a-d）所示，它们主要由多个阶段和组件组成，例如通过后置滤波的假阳性去除、候选聚合、线形成和字分区。多个阶段和组件可能需要进行详尽的调整，从而导致次优性能，并增加整个流水线的处理时间。 图2. 最近几个关于场景文本检测的作品的方法比较：（a）Jaderberg等人提出的水平文字检测和识别方法。（b）Zhang等人提出的多方向文本检测流程。（c）Yao等人提出的多方向文本检测流程。（d）使用CTPN进行水平文本检测，由Tian等人提出。（e）我们的方法消除了大多数中间步骤，只包含两个阶段，比以前的解决方案简单得多。 在本文中，我们设计了一个基于FCN的深层方法，直接针对文本检测的最终目标：单词或文本行级别检测。如图2（e）所示，该模型放弃了不必要的中间组件和步骤，并允许端到端的训练和优化。 由此产生的系统配备了单个轻量级神经网络，在性能和速度方面都明显优于所有以前的方法。 Methodology 所提出的算法的关键组成部分是神经网络模型，其被训练以从完整图像直接预测文本实例及其几何形状的存在。该模型是一个完全卷积的神经网络，适用于文本检测，输出密集的逐像素单词或文本行预测。这消除了候选建议、文本区域生成和文字分区等中间步骤。后处理步骤仅包括预测几何形状的阈值和NMS。检测器被命名为EAST，因为它是一个高效精确的场景文本检测方法。 Pipeline 我们的方法的高级概述如图2（e）所示。该算法遵循DenseBox的一般设计，其中图像被馈送到FCN，并且生成多个像素级文本得分图和几何通道。 预测通道之一是得分图，其像素值在[0;1]。其余通道表示从每个像素的视图中包围该单词的几何。分数代表在相同位置预测的几何形状的置信度。 我们已经为文本区域，旋转框（RBOX）和四边形（QUAD）实验了两种几何形状，并为每个几何设计了不同的损失函数。然后将阈值应用于每个预测区域，其中分数超过预定义阈值的几何形状被认为是有效的并保存用于以后的非极大值抑制。NMS之后的结果被认为是方法的最终输出。 Network Design 在设计用于文本检测的神经网络时必须考虑几个因素。由于字区域的大小（如图5所示）变化很大，因此确定大字的存在需要来自神经网络后期的特征，而预测包含小字区域的精确几何需要早期阶段的低级信息。因此，网络必须使用不同等级的函数来满足这些要求。 HyperNet在特征映射上满足这些条件，但在大特征映射上合并大量通道会显著增加后续阶段的计算开销。 为了解决这个问题，我们采用U形的思想逐步合并特征图，同时保持上采样分支小。我们结合最终得到的网络既可以利用不同级别的函数，又可以保持较低的计算成本。 图3. 文本检测FCN的结构 我们模型的示意图如图3所示。该模型可以分解为三个部分：特征提取器主干、特征合并分支和输出层。主干可以是在ImageNet数据集上预先训练的卷积网络，具有交叉卷积和池化层。从主干中提取四个级别的特征图，表示为fi，其大小分别为输入图像的1/32，1/16，1/8和1/4。在图3中，描绘了PVANet。在我们的实验中，我们还采用了众所周知的VGG16模型，其中提取了pooling-2到pooling-5之后的特征映射。在特征合并分支中，我们逐渐合并它们： 其中gi是合并基础，hi是合并的特征映射，运算符[·;·]表示沿通道轴的连接。在每个合并阶段，来自最后一个阶段的特征映射首先被馈送到反池化层以使其大小加倍，然后与当前特征映射连接。接下来，conv1×1的瓶颈减少了通道的数量并减少了计算量，接着是融合信息的conv3×3，最终产生该合并阶段的输出。在最后一个合并阶段之后，conv3×3层产生合并分支的最终特征图并将其馈送到输出层。 每个卷积的输出通道数量如图3所示。我们将分支中的卷积通道数保持较小，这样只增加了一小部分计算开销，使网络计算效率更高。最终输出层包含几个conv1×1操作，以将32个通道的特征映射投影到1通道的得分图Fs和多通道几何图Fg中。几何输出可以是RBOX或QUAD之一，在表1中。 对于RBOX，几何形状由4个通道的轴对齐边界框（AABB）R和1个通道旋转角θ表示。R的公式与下文中的公式相同，其中4个通道分别表示从像素位置到矩形的顶部、右侧，底部、左侧边界的4个距离。 对于QUAD Q，我们使用8个数字来表示从四边形的四个角点{pi|i属于{1,2,3,4}}到像素位置的坐标偏移。由于每个距离偏移包含两个数字（Δxi;Δyi），因此几何输出包含8个通道。 表1. 输出几何设计 Label GenerationScore Map Generation for Quadrangle 不失一般性，我们只考虑几何是四边形的情况。 分数图上的四边形的正面积被设计为大致缩小的原始面积，如图4（a）所示。 对于四边形Q = {pi|i属于{1,2,3,4}}，其中pi = {xi; yi}是四边形上的顶点，顺时针顺序。为了缩小Q，我们首先计算每个顶点pi的参考长度ri为： 其中D（pi; pj）是pi和pj之间的L2距离。 我们首先缩小四边形的两个较长边，然后缩小两个较短边。 对于每对两个相对的边，我们通过比较它们的长度的平均值来确定“更长”的对。 对于每个边缘&lt;pi; p（i mod 4）+ 1&gt;，我们通过将两个端点沿边缘向内移动0.3ri和0.3r（i mod 4）+1来缩小它。 图4. 标签生成过程：（a）文本四边形（黄色虚线）和收缩四边形（绿色实心）; （b）文本分数图; （c）RBOX几何图生成; （d）每个像素到矩形边界的4个通道距离; （e）旋转角度。 Geometry Map Generation 如第二节所述。3.2，几何图可以是RBOX或QUAD之一。RBOX的生成过程如图4（c-e）所示。 对于那些文本区域以QUAD样式注释的数据集（例如，ICDAR 2015），我们首先生成一个旋转的矩形，覆盖区域最小的区域。然后对于每个具有正分数的像素，我们计算它到文本框的4个边界的距离，并将它们放到RBOX真值的4个通道中。对于QUAD真值，8通道几何图中具有正分数的每个像素的值是其从四边形的4个顶点的坐标偏移。 Loss Functions 损失可以表述为： 其中Ls和Lg分别代表得分图和几何的损失，λg衡量两次损失之间的重要性。在我们的实验中，我们将λg设置为1。 Loss for Score Map 在大多数最先进的检测方法中，通过平衡采样和硬负采样精心处理训练图像，以解决目标物体的不平衡分布。这样做可能会提高网络性能。然而，使用这些技术不可避免地引入了不可微分的阶段和更多的参数来调谐和更复杂的方法，这与我们的设计原理相矛盾。 为了便于更简单的训练过程，我们使用文中引入的class-balanced交叉熵，由下式给出： 其中Y ^ = Fs是得分图的预测，Y* 是真值。参数β是正样本和负样本之间的平衡因子，由下式给出： 这种平衡的交叉熵损失首先在Yao等人的文本检测中被采用。下文作为得分图预测的目标函数。我们发现它在实践中表现良好。 Loss for Geometries 文本检测的一个挑战是自然场景图像中文本的大小差别很大。直接使用L1或L2损失进行回归将导致较大和较长文本区域的损失偏差。由于我们需要为大文本区域和小文本区域生成准确的文本几何预测，因此回归损失应该是规模不变的。因此，我们采用RBOX回归的AABB部分中的IoU损失，以及规模归一化的平滑L1损失用于QUAD回归。RBOX 对于AABB部分，我们采用文中的IoU损失，因为它对不同尺度的物体是不变的。 其中R^表示预测的AABB几何，R* 是其对应的真值。很容易看出相交矩形的宽度和高度为|R^ 交 R*|： 其中d1、d2、d3和d4分别表示从像素到其对应矩形的顶部、右侧、底部和左侧边界的距离。并集区域由下式给出： 因此，可以容易地计算交集/并集区域。接下来，计算旋转角度的损失： 其中θ^是对旋转角度的预测，θ* 表示真值。最后，整体几何损失是AABB损失和角度损失的加权和，由下式给出： 在我们的实验中λθ设置为10。 请注意，无论旋转角度如何，我们都会计算LAABB。当角度被完美预测时，这可以看作是四边形IoU的近似值。虽然在训练期间不是这种情况，但它仍然可以为网络施加正确的梯度以学习预测R^。QUAD 我们通过添加为单词四边形设计的额外归一化项来扩展文中提出的平滑L1损失，这通常在一个方向上更长。设Q的所有坐标值都是有序集： 其中归一化项 NQ* 是四边形的短边长度，由下式给出：和 PQ 是 Q* 的所有等效四边形的集合，具有不同的顶点排序。由于公共训练数据集中的四边形标注不一致，因此需要这种排序排列。 Training 使用ADAM优化器对网络进行端到端的训练。为了加快学习速度，我们从图像中均匀地采集512x512大小，形成24小时的小批量。ADAM的学习率从1e-3开始，每27300个小批量衰减到十分之一，停在1e-5。训练网络直到性能停止改善。 Locality-Aware NMS 为了形成最终结果，在阈值处理之后存在的几何结构应该由NMS合并。一个简单的NMS算法在O（n2）中运行，其中n是候选几何的数量，这是不可接受的，因为我们正面临来自密集预测的数万个几何形状。 假设来自附近像素的几何图形往往高度相关，我们建议逐行合并几何图形，同时在同一行中合并几何图形时，我们将迭代地合并当前遇到的几何图形和最后合并的几何图形。这种改进的技术在最佳场景下以O（n）运行。即使最坏的情况与天真的情况相同，只要位置假设成立，算法在实践中运行得足够快。该过程总结在算法1中。 值得一提的是，在WEIGHTEDMERGE（g; p）中，合并四边形的坐标通过两个给定四边形的分数进行加权平均。具体而言，如果a = WEIGHTEDMERGE（g; p），则ai = V（g）gi + V（p）pi和V（a）= V（g）+ V（p），其中ai是其中之一i的下标坐标，V（a）是几何a的得分。 实际上，我们正在“平均”而不是“选择”几何形状存在一个微妙的差异，就像在标准的NMS程序中那样，作为一种投票机制，这反过来又会在播放视频时引入稳定效果。但是，我们仍然采用“NMS”这个词进行功能描述。 Experiments 为了将提出的算法与现有方法进行比较，我们对三个公共基准集进行了定性和定量实验：ICDAR2015，COCO-Text和MSRA-TD500。 Benchmark Datasets ICDAR 2015用于ICDAR 2015鲁棒阅读比赛的挑战4。它包括总共1500张图片，其中1000张用于训练，其余用于测试。文本区域由四边形的4个顶点标注，对应于本文中的QUAD几何。我们还通过拟合具有最小面积的旋转矩形来生成RBOX输出。这些图像由Google Glass以随机的方式拍摄。因此，场景中的文本可以处于任意方向，或者遭受模糊和低分辨率。我们还使用了ICDAR 2013的229张训练图像。 COCO-Text是迄今为止最大的文本检测数据集。它重用了MS-COCO数据集中的图像。共标注了63686张图像，其中选择了43486作为训练集，其余20000作为测试。字区域以轴对齐边界框（AABB）的形式标注，这是RBOX的特例。对于此数据集，我们将角度θ设置为零。我们使用与ICDAR 2015中相同的数据处理和测试方法。 MSRA-TD500是一个包含300个训练图像和200个测试图像的数据集。文本区域具有任意方向，并在句子级别注释。与其他数据集不同，它包含英文和中文文本。文本区域以RBOX格式标注。由于训练图像的数量太少而无法学习深层模型，因此我们还利用来自HUSTTR400数据集中的400幅图像作为训练数据。 Base Networks 除了COCO-Text之外，与一般物体检测的数据集相比，所有文本检测数据集相对较小，因此如果所有基准都采用单一网络，则可能会出现过拟合或欠拟合的情况。我们在所有数据集上尝试了三种不同的基本网络，具有不同的输出几何，以评估所提出的框架。表2中总结了这些网络。 VGG16在许多任务中被广泛用作基础网络，以支持随后的任务特定的微调，包括文本检测。这个网络有两个缺点：（1）该网络的接收领域很小。conv5_3的输出中的每个像素仅具有196的感受域。（2）这是一个相当大的网络。 PVANET是文中引入的轻量级网络，旨在替代Faster-RCNN框架中的特征提取器。由于GPU太小而无法充分利用计算并行性，我们还采用PVANET2x，使原始PVANET的通道加倍，利用更多的计算并行性，同时运行速度比PVANET稍慢。这在4.5中详述。最后一个卷积层输出的感受域是809，比VGG16大得多。 模型在ImageNet数据集上进行了预训练。 Qualitative Results 图5描绘了所提出的算法的几个检测示例。 它能够处理各种具有挑战性的场景，例如非均匀照明、低分辨率、方向多变和透视失真。此外，由于NMS程序中的投票机制，所提出的方法对具有各种形式的文本实例的视频表现出高度的稳定性。 所提出的方法的中间结果在图6中示出。可以看出，训练的模型产生高度精确的几何图和分数图，其中容易形成不同方向的文本实例的检测结果。 Quantitative Results 如表3和表4所示。我们的方法在ICDAR 2015和COCO-Text上大大优于以前最先进的方法。 在ICDAR 2015 Challenge 4中，当图像以其原始比例进给时，所提出的方法实现了0.7820的Fscore。当使用相同网络在多个尺度进行测试时，我们的方法在F分数中达到0.8072，在绝对值（0.8072对0.6477）方面比最佳方法高近0.16。 使用VGG16网络比较结果，当使用QUAD输出时，所提出的方法也优于之前的最佳工作0.0924，使用RBOX输出时优于0.116。同时，这些网络非常有效，如第4.5节所示。 在COCO-Text中，所提出的算法的所有三个设置都比先前的最佳性能者具有更高的准确度。具体而言，Fscore中的提升为0.0614，而召回时的提升为0.053，这证实了所提算法的优势，因为COCO-Text是迄今为止最大和最具挑战性的基准数据集。请注意，我们还将文中的结果作为参考包含在内，但这些结果实际上不是有效的基准方法，因为方法（A，B和C）用于数据标注。 与先前方法相比，所提算法的提升证明，直接针对最终目标并消除冗余过程的简单文本检测方法可以击败精心设计的方法，甚至是那些与大型神经网络模型集成的方法。 如表5所示，在MSRA-TD500上我们所有的三种设置方法都取得了优异的效果。表现最佳的Fscore（Ours + PVANET2x）略高于其他方法。与Zhang等人的方法相比，先前发表的最先进的系统，表现最佳的（Ours + PVANET2x）F分数提高了0.0208，精度提高了0.0428。 请注意，在MSRA-TD500上，加上VGG16的算法比使用PVANET和PVANET2x（0.7023与0.7445和0.7608）相比差得多，主要原因是VGG16的有效感受野小于PVANET和PVANET2x，而MSRA-TD500的评估协议要求文本检测算法输出行级而不是字级预测框。 此外，我们还根据ICDAR 2013基准评估了Ours + PVANET2x。它在召回率，精度和F值方面达到0.8267,0.9264和0.8737，与之前的最新方法相当，后者在召回率，精度和F值方面分别获得0.8298,0.9298和0.8769。 Speed Comparison 表6中展示了整体速度比较。 我们报告的数字是使用我们表现最佳的网络，通过ICDAR 2015数据集以原始分辨率（1280x720）运行500个测试图像的平均值。这些实验是在服务器上使用具有Maxwell架构和Intel E5-2670 v3 @ 2.30GHz CPU的单个NVIDIA Titan X图形卡进行的。对于所提出的方法，后处理包括阈值处理和NMS，而其他的应该参考他们的原始论文。 虽然所提出的方法明显优于最先进的方法，但计算成本保持很低，归因于简单而有效的流水线。从Tab可以看出。 6，我们方法的最快设置以16.8 FPS的速度运行，而最慢设置以6.52 FPS运行。即使是性能最佳的型号Ours + PVANET2x也能以13.2 FPS的速度运行。这证实了我们的方法是最有效的文本检测器之一，可以在基准测试中实现最先进的性能。 Limitations 检测器可以处理的文本实例的最大大小与网络的感受野成比例。这限制了网络预测更长文本区域的能力，例如跨越图像的文本行。 此外，该算法可能会漏掉或给出垂直文本实例的不精确预测，因为它们仅占ICDAR 2015训练集中的一小部分文本区域。 Conclusion and Future Work 我们已经提出了一个场景文本检测器，它使用单个神经网络直接从完整图像生成单词或行级预测。通过结合适当的损失函数，检测器可以根据具体应用预测文本区域的旋转矩形或四边形。在标准基准测试上的实验证实，所提出的算法在准确性和效率方面基本上优于先前的方法。 未来研究的可能方向包括：（1）调整几何公式以允许直接检测弯曲文本；（2）将检测器与文本识别器集成；（3）将思想扩展到一般物体检测。]]></content>
      <categories>
        <category>文本检测</category>
      </categories>
      <tags>
        <tag>文本检测</tag>
        <tag>旷视</tag>
        <tag>CVPR2017</tag>
        <tag>多方向文本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[未知2018 | 北理、旷视、北大联合提出PAN，用于语义分割]]></title>
    <url>%2F%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%2F%E6%9C%AA%E7%9F%A52018-%E5%8C%97%E7%90%86%E3%80%81%E6%97%B7%E8%A7%86%E3%80%81%E5%8C%97%E5%A4%A7%E8%81%94%E5%90%88%E6%8F%90%E5%87%BAPAN%EF%BC%8C%E7%94%A8%E4%BA%8E%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%2F</url>
    <content type="text"><![CDATA[Pyramid Attention Network for Semantic SegmentationKeyWords Plus: 未知2018 金字塔注意力网络paper：https://arxiv.org/pdf/1805.10180.pdfreference: Li, Hanchao, et al. “Pyramid attention network for semantic segmentation.” arXiv preprint arXiv:1805.10180 (2018).Github: https://github.com/JaveyWang/Pyramid-Attention-Networks-pytorch 引言 随着卷积神经网络 (CNN) 的发展，层次特征的丰富性及端到端的训练框架可用性，逐像素（pixel-wise）的语义分割问题的研究取得了显著的进步。但是，现有的研究对于高维度特征表征的编码效果仍不理想，导致原始场景中上下文像素的空间分辨率遭受损失。 图1：VOC 数据集的可视化结果 在图1中，正如我们所看到的，FCN 模型难以对小目标和细节进行预测。在第一排中自行车的手柄在预测中丢失了，而第二排中出现了错误的动物类别预测。我们的特征金字塔注意力模块 (FPA) 和全局注意力上采样 (GAU) 模块旨在扩大目标感受野并有效地恢复像素的定位细节。 如图１所示，全卷积神经网络 (Full Convolutional Network，FCN) 缺乏对场景中小部件的预测能力，图中第一排自行车的手柄消失了，而第二排中的羊被误认为牛。这对语义分割任务提出了挑战。 首先是多尺度目标的存在会加大语义分割任务中类别分类的困难。为了解决这个问题，PSPNet 或 DeepLab 系统提出空间金字塔结构，旨在不同的网格尺度或扩张率下 (称之为空间金字塔池化，ASPP)，融合多尺度的特征信息。在 ASPP 模块中，扩张卷积是一种稀疏计算，这可能会导致产生网格伪像 (grid artifacts)。而 PSPNet 中提出的金字塔池化模块则可能会丢失像素级别的定位信息。受 SENet 和 Parsenet 的启发，本文尝试从 CNN 的高层次特征中提取出准确的像素级注意力特征。图2展示了本文提出的特征金字塔注意力模块 (Feature Pyramid Attention，FPA)的能力，它能够扩大感受野的范围并有效地实现小目标的分类。 另一个问题是，高层次的特征在对类别进行准确分类时非常有效，但在重组原始分辨率的二类预测问题方面比较薄弱。一些 U 型网络，如 SegNet，Refinenet 以及 Large Kernel Matters 能够在复杂的解码器模块中使用低层次信息来帮助高层次特征恢复图像细节。但是，这些方法都很耗时，运行效率不高。为解决这个问题，本文提出了一种称为 Global Attention Upsample (GAU) 方法，这是一个有效的解码器模块，在不需要耗费过多计算资源的情况下，它可以提取高层次特征的全局上下文信息，作为低阶特征的加权计算的指导。 总的来说，本文主要有以下三个贡献： 提出了一个特征金字塔注意力模块 FPA，可以在基于 FCN 的像素预测框架中嵌入不同尺度的上下文特征信息。 开发了一个高效的解码器模块 Global Attention Upsample（GAU），用于处理图像的语义分割问题。 结合特征金字塔注意力模块和全局注意力上采样模块，本文提出的金字塔注意力网络在 VOC2012 和 cityscapes 的测试基准中取得了当前最佳的性能。 模型方法特征金字塔注意力模块FPA 基于以上观察，本文提出了特征金字塔注意力模块 (FPA)，该模块能够融合来自 U 型网络 (如特征金字塔网络 FPN) 所提取的三种不同尺度的金字塔特征。为了更好地提取不同尺度下金字塔特征的上下文信息，分别在金字塔结构中使用 3×3, 5×5, 7×7 的卷积核。由于高层次特征图的分辨率较小，因此使用较大的内核并不会带来太多的计算负担。随后，金字塔结构逐步集成不同尺度下的特征信息，这样可以更准确地结合相邻尺度的上下文特征。然后，经过 1×1 卷积处理后，由 CNN 所提取的原始特征通过金字塔注意力特征进行逐像素相乘。此外，还引入了全局池化分支来联结输出的特征，这将进一步提高 FPA 模块的性能。整体的模块结构如下图所示。得益于空间金字塔结构，FPA 模块可以融合不同尺度的上下文信息，同时还能为高层次的特征图提供更好的像素级注意力。 图2：特征金字塔注意力模块结构 在图2中，(a) 空间金字塔池结构。(b) 特征金字塔注意力模块。 ‘4×4，8×8，16×16，32×32’ 分别代表特征映射的不同分辨率。虚线框表示全局池化分支。蓝色和红色的线条分别代表下采样和上采样运算符。 全局注意力上采样模块GAU 本文提出的全局注意力上采样模块 (Global Attention Upsample，GAU)，通过全局池化过程将全局上下文信息作为低层特征的指导，来选择类别的定位细节。具体地说，本文对低层次特征执行 3×3 的卷积操作，以减少 CNN 特征图的通道数。从高层次特征生成的全局上下文信息依次经过 1×1 卷积、批量归一化 (batch normalization) 和非线性变换操作 (nonlinearity)，然后再与低层次特征相乘。最后，高层次特征与加权后的低层次特征相加并进行逐步的上采样过程。本文的 GAU 模块不仅能够更有效地适应不同尺度下的特征映射，还能以简单的方式为低层次的特征映射提供指导信息。模块的结构示意图如图3所示。 图3：全局注意力上采样模块 金字塔注意力网络PAN 结合特征金字塔注意力模块 (FPA) 和全局注意力上采样模块 (GAU)，本文提出金字塔注意力网络 (PAN)，其结构示意图如下图所示。本文使用在 ImageNet 数据集上预训练好的 ResNet-101 模型，辅以扩张（空洞）卷积策略来提取特征图。具体地说，本文在 res5b 模块上应用扩张率为 2 的扩张卷积，以便 ResNet 输出的特征图大小为原输入图像的1/16，这与 DeepLabv3+ 模型中的设置是一致的。正如 PSPNet 和 DUC 模型那样，本文用三个 3×3 卷积层来取代原 ResNet-101 模型中的 7×7 卷积。此外，本文使用 FPA 模块来收集 ResNet 的输出中密集的像素级注意力信息。结合全局的上下文信息，经 GAU 模块后，生成最终的预测图。 图4：金字塔注意力网络结构 在图4中，本文使用 ResNet-101 模型来提取密集的特征。然后，分别执行 FPA 模块和 GAU 模块进行准确的像素预测并获取目标定位的细节。蓝线和红线分别代表下采样和上采样运算符。 本文将 FPA 模块视为编码器和解码器结构之间的中心模块。即使没有全局注意上采样模块，FPA 模块也能够进行足够准确的像素级预测和类别分类。在实现 FPA 模块后，将 GAU 模块视为一种快速有效的解码器结构，它使用高层次的特征来指导低层次的信息，并将二者结合起来。 实验结果 在 PASCAL VOC2012 和 cityscapes 数据集上分别评估了本文提出的方法。 消融实验FPA 模块 表1中，实验分别对池化类型、金字塔结构、卷积核大小、全局池化四种设置进行了 Ablation Experiments 分析，结果如下：其中 AVE 表示平均池化策略，MAX 表示最大池化，C333 代表全部使用 3×3 的卷积核，C357 表示所使用的卷积核分别为 3×3、5×5 和 7×7，GP 代表全局池化分支，SE 表示使用 SENet 注意力模块。 池化类型：在这项工作中，作者发现 AVE 的性能要优于 MAX：对于 3×3 的卷积核设置，AVE 的性能能达到 77.54%，优于 MAX 所取得的77.13%。 金字塔结构：作者提出的模型在验证集上能取得 72.60％ 的 mIoU。此外，使用 C333 和 AVE 时，模型的性能能够从 72.6％ 提升至 77.54％。还使用 SENet 注意力模块来取代金字塔结构，进一步对比评估二者的性能。实验结果如下表1所示，与 SENet 注意力模块相比，C333 和 AVE 设置能将性能提高了近1.8％。 卷积核大小：对于使用平均池化的金字塔结构，本文使用 C357 取代 C333 卷积核设置，金字塔结构中特征映射的分辨率为 16×16，8×8，4×4。实验结果表明，模型性能能够从 77.54％ 提高至 78.19％。 全局池化：本文进一步在金字塔结构中添加全局池化分支以提高模型性能。实验结果表明，在最佳设置下模型能够取得 78.37 的 mIoU 和 95.03% 的 Pixel Acc。 表1：不同设置下 FPA 模块的性能 GAU 模块 首先，评估 ResNet101+GAU 模型，然后将 FPA 和 GAU 模块结合并在 VOC 2012 验证集中评估我们的模型。作者分别在三种不同的解码器设置下评估模型：(1) 仅使用跳跃连接的低级特征而没有全局上下文注意力分支。(2) 使用 1×1 卷积来减少 GAU 模块中的低层次特征的通道数。(3) 用 3×3 卷积代替 1×1 卷积减少通道数。实验结果如表2所示。 表2：不同解码器设置下的模型性能 此外，作者还比较了ResNet101+GAU 模型、Global Convolution Network 和 Discriminate Feature Network，实验结果如表3所示。 表3：作者提出的模型与其他模型的比较结果 PASVAL VOC 2012 数据集 结合 FPA 模块和 GAU 模块的最佳设置，本文在 PASVAL VOC 2012 数据集上评估了作者提出的金字塔注意力网络 (PAN)。实验结果如表4和表5所示。可以看到，PAN 取得了84.0% mIoU，超过现有的所有方法。 表4：在 VOC 2012 数据集上模型的性能 表5：在 PASVAL VOC 2012 测试集上单类别的实验结果 Cityscapes 数据集 Cityscapes 数据集包含 30 个类别，其中 19 个用于我们的模型训练和评估。整个数据集 5000 个带细粒度标注的图像和 19998 个带粗粒度标注的图像。具体地说，作者将细粒度图像分为训练集、验证集和测试集，分别有 2979、500 和 1525 张图像。在训练期间，作者没有使用带粗粒度标注的数据集，所使用的图像尺寸为 768×768。同样地，作者以 ResNet101 作为基础模型，实验结果如表6列出。 表6：Cityscapes 测试集上模型的性能 结论 在本文中，作者提出了一种金字塔注意力网络，用于处理图像语义分割问题。作者设计了特征金字塔注意力模块 (FPA) 和全局注意力上采样模块 (GAU)。FPA 模块能够提供像素级注意力信息并通过金字塔结构来扩大感受野的范围。GAU 模块能够利用高层次特征图来指导低层次特征恢复图像像素的定位。实验结果表明，作者所提出的方法在 PASCAL VOC 2012 语义分割任务实现了当前最佳的性能。]]></content>
      <categories>
        <category>语义分割</category>
      </categories>
      <tags>
        <tag>旷视</tag>
        <tag>PAN</tag>
        <tag>语义分割</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WACV2018 | 微软亚研院提出带PAN的基于Mask R-CNN的场景文本检测方法]]></title>
    <url>%2F%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%2FWACV2018-%E5%BE%AE%E8%BD%AF%E4%BA%9A%E7%A0%94%E9%99%A2%E6%8F%90%E5%87%BA%E5%B8%A6PAN%E7%9A%84%E5%9F%BA%E4%BA%8EMask-R-CNN%E7%9A%84%E5%9C%BA%E6%99%AF%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Mask R-CNN with Pyramid Attention Network for Scene Text DetectionKeyWords Plus: WACV2018 Curved Textpaper：https://arxiv.org/pdf/1811.09058.pdfreference: Huang Z , Zhong Z , Sun L , et al. Mask R-CNN with Pyramid Attention Network for Scene Text Detection[J]. 2018.Github: 未开源 创新点 本文提出了一种新的基于Mask R-CNN的文本检测方法，该方法能够以统一的方式鲁棒地检测来自自然场景图像的多方向和弯曲文本。为了增强Mask R-CNN用于文本检测任务的特征表示能力，使用金字塔注意力网络（PAN）作为Mask R-CNN的新主干网络。 模型 基于Mask R-CNN的文本检测网络由四个模块组成： PAN特征金字塔主干网络，负责在整个图像上计算多尺度卷积特征金字塔； 产生矩形文本proposal的区域提议网络（RPN）； Fast R-CNN检测器，对提取的proposal进行分类并输出相应的四边形边界框； Mask预测网络，用于预测输入proposal的文本掩模。 PAN金字塔注意力网络 金字塔注意力网络（PAN）主要由两个模块组成，即Feature Pyramid Attention（FPA）模块和Global Attention Up-sample（GAU）模块。FPA模块对高级特征执行空间金字塔注意力，并结合全局池化以学习更好的高级特征表示。GAU模块附加在每个解码器层上，以提供全局上下文作为低级特征的指导，来选择类别局部细节。 FPA结构 在ResNet50或ResNeXt50之上构建PAN； 将ResNet50或ResNeXt50中Res-4层的输出特征图作为输入，在其上分别以3,6,12的采样率执行3×3扩张（空洞）卷积，以更好地提取上下文信息。将这三个特征图连接起来，并通过1×1卷积层降维处理。 FPA在输入Res-4上进一步形成1×1卷积，其输出与上述上下文特征图对应位置相乘。 提取的特征与全局池化分支的输出特征加在一起，以获得最终的金字塔注意特征。 GAU结构 对低级特征执行3×3卷积，以减少来自CNN的特征图的通道。 对高级特征进行全局池化后，通过1×1卷积与实例归一化和ReLU非线性，然后乘以低级特征。 通过上采样的高级特征与加权的低级特征相加，以生成GAU特征。 通过上述FPA和GAU模块，构建了一个具有三个级别的强大特征金字塔，即P2、P3和P4，其大小分别为原图的1/4、1/8和1/16。整体PAN架构为： RPN区域提议网络 三个RPN分别连接到P2、P3和P4，每个RPN在相应的金字塔等级上密集地滑动一个小网络，以执行文本/非文本分类和边界框回归。 小网络实现为3×3卷积层，后面是两个1×1卷积层，分别用于预测文本分数和矩形边界框位置。 具体来说： 通过使用6个纵横比{0.2, 0.5, 1.0, 2.0, 4.0, 8.0}和一个尺度{32,64,128}，在{P2，P3，P4}的每个金字塔级别上的每个滑动位置设计6个anchor。将所有三个RPN的检测结果聚合在一起以构建proposal集{D}。 然后，使用标准的非极大抑制（NMS）算法，其IoU阈值为0.7，以删除{D}中的冗余提议。 最后，为Faster R-CNN和Mask预测网络选择评分 Top-N 的 propoasl。在训练和测试阶段 N 均设为2000。 Fast R-CNN &amp; Mask预测网络 为实现这一目标，提出了一种Skip-RoIAlign方法来融合P2、P3和P4级别的特征。 具体来说： 首先，对于每个propoasl，分别在P2、P3和P4金字塔等级上应用ROIAlign，得到三个固定大小为7×7的特征图； 然后，连接这些特征图，并且用1×1卷积层进行降维处理，以获得最终的ROI特征； 最后，将这些ROI特征送到Fast RCNN和Mask预测网络中进行预测。 对于Fast RCNN，将ROI特征进行全局平均池化后，进行文本/非文本分类、四边形边界框回归。 对于Mask预测网络，在ROI特征后进行四个连续的3×3卷积层，之后上采样到14×14大小的特征图，进行Mask预测。 损失函数RPN的的多任务损失函数 每个单独的RPN有两个兄弟输出层，即文本/非文本分类层和矩形边界框回归层。多任务损失函数可表示如下： 文本/非文本分类loss为一个softmax loss； 矩形边界框回归loss为一个smooth-L1 loss； Lambda_loc为平衡参数，设置为3； RPN网络的总损失LRPN为三个单独的RPN网络的总和。 Fast R-CNN的多任务损失函数 有两个输出层，即文本和非文本分类层和四边形边界框回归层。 文本/非文本分类loss为一个softmax loss； 四边形边界框回归loss为一个smooth-L1 loss； Lambda_loc为平衡参数，设置为1。 Mask预测网络的损失函数 采用标准的二值交叉熵损失: 全局损失函数 Lambda_mask为Mask分支的平衡参数，设置为0.03125。 实验数据集 ICDAR-2017 MLT 用于多方向文本检测； ICDAR-2015 用于多方向文本检测； SCUT-CTW 1500 用于曲线文本检测。 实验细节 对于ICDAR-2017 MLT和ICDAR-2015数据集，直接使用Fast R-CNN模块预测的四边形边界框作为最终检测结果， 对于弯曲文本检测数据集SCUT-CTW 1500，使用由Mask预测网络预测的文本Mask为最终检测结果。 消融实验 ResNeXt50比ResNet50的效果更好； PAN比FPN的效果更好。 对比实验 实验结果]]></content>
      <categories>
        <category>文本检测</category>
      </categories>
      <tags>
        <tag>文本检测</tag>
        <tag>曲线文本</tag>
        <tag>Mask R-CNN</tag>
        <tag>PAN</tag>
        <tag>WACV2018</tag>
        <tag>微软亚研院</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[曲文检测论文汇总（2018.12.26）]]></title>
    <url>%2F%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%2F%E6%9B%B2%E6%96%87%E6%A3%80%E6%B5%8B%E8%AE%BA%E6%96%87%E6%B1%87%E6%80%BB%EF%BC%882018-12-26%EF%BC%89%2F</url>
    <content type="text"><![CDATA[方法和思路总结 从趋势和效果来看，基本确定：用instance-segmentation思路来做 目前已用的框架来看 检测：Faster RCNN、R-FCN 分割：Mask R-CNN、FPN、FCIS 目前在这个方面，探讨的比较多的两个instance-segmentation用在文字上的问题 多边形表示mask 多scale（FPN，低高层特征进行fusion） 黏连 可以参考的几个点 对gt做shrink attention PAN FCIS的PSROI也不错 用于做实验对比的数据集 CTW1500 Total-Text ICDAR15 ICDAR17-MLT MSRA-TD500 论文列表 Yuliang Liu_2017_Detecting Curve Text in the Wild_New Dataset and New Solution 方法名称：CTD+TLOC Shangbang Long_ECCV2018_TextSnake_A Flexible Representation for Detecting Text of Arbitrary Shapes 方法名称：TextSnake Yuchen Dai——【2017】Fused Text Segmentation Networks for Multi-Oriented Scene Text Detection 方法名称：FTSN Jun Du——【ICPR2018】Sliding Line Point Regression for Shape Robust Scene Text Detection 方法名称：SLPR XiangLi——【2018】Shape Robust Text Detection with Progressive Scale Expansion Network 方法名称：PSENet Zhida Huang——【2018】Mask R-CNN with Pyramid Attention Network for Scene Text Detection 方法名称：Mask-PAN Yongchao Xu——【2018】TextField_Learning A Deep Direction Field for Irregular Scene Text Detection 方法名称：TextField Enze Xie——【AAAI2019】Scene Text Detection with Supervised Pyramid Context Network 方法名称：SPCNET Jiaming Liu——【2019】Detecting Text in the Wild with Deep Character Embedding Network 方法名称：CENet Chuhui Xue——【arxiv2019】MSR_Multi-Scale Shape Regression for Scene Text Detection 方法名称：MSR 方法详细描述CTD+TLOC论文：Yuliang Liu_2017_Detecting Curve Text in the Wild_New Dataset and New Solution亮点： 第一篇做曲文检测，还提出一个数据集CTW1500 使用14个点的多边形来表示曲文 提出了一个结合CNN-RPN+RNN的检测方法专门做曲文检测 方法概述 针对曲文检测，基于RPN进行修改，除了学习text/non-text分类，多边形的bounding box回归（x1,y1,x2,y2），增加了14个点的回归，最后再进行后处理（去噪+nms），得到最终输出。 TextSnake论文：Shangbang Long_ECCV2018_TextSnake_A Flexible Representation for Detecting Text of Arbitrary Shapes亮点： 提出一个新的曲线文本表示方法TextSnake（由圆盘序列组成） 提出了一个新的曲文检测方法，并且精度比之前的高40%+（Total-Text数据集）Fig. 2. Illustration of the proposed TextSnake representation. Text region (in yellow) is represented as a series of ordered disks (in blue), each of which is located at the center line (in green, a.k.a symmetric axis or skeleton) and associated with a radius r and an orientation θ. In contrast to conventional representations (e.g., axis-aligned rectangles, rotated rectangles and quadrangles), TextSnake is more flexible and general, since it can precisely describe text of different forms, regardless of shapes and lengths. 方法概述 针对曲文检测，提出一个新的曲线文本表示方法TextSnake：用一个有序的圆盘序列来表示文字，先用FCN检测文本区域、文本中心线、以及每个点的圆盘半径、方向，然后利用文本区域mask和中心线mask得到text instance segmentation。在每个text-instance上，交替进行点中心化和点扩展，得到文本中心点序列。最后结合圆盘半径，得到文本区域的TextSnake表示并进行union得到最终的文本区域。 FTSN论文：Yuchen Dai——【2017】Fused Text Segmentation Networks for Multi-Oriented Scene Text Detection亮点： 比较早的一篇用FCIS做曲文检测的方法 提出Mask NMS 方法概述 针对曲文检测，采用instance-segmentation思路，基于FCIS框架，基本没特别改动，增加了一个Mask NMS。 检测流程是：使用FCIS得到instance-segmentation mask，然后再用Mask NMS，最后利用Mask得到多边形。Fig. 2. The proposed framework consists of three parts: feature extraction, feature fusion along with region proposing and text instance prediction. The dashed line represents a convolution with 1x1 kernel size and 1024 output channels. The line in red is for upsampling operation and blue lines indicate on which feature maps PSROIPooling are performed using given ROIs. Mask NMS实际上就是把IOU-overlap换成两个Mask的Intersection的像素点总数，分母的union area换成两个polygon的max_area。mask maximum-intersection (MMI): SLPR论文：Jun Du——【ICPR2018】Sliding Line Point Regression for Shape Robust Scene Text Detection亮点： 基于检测框架进行修改，只需增加回归点的纵坐标或横坐标，是对CTD+TLOC的简化和改进。 方法概述 针对曲文检测，采用object-detection思路，基于Faster R-CNN/R-FCN框架，增加了沿x/y轴均匀划线与多边形交点的纵/横坐标的回归（14个点，仅回归x或y坐标），最后把点串起来得到多边形。 PSENet论文：XiangLi——【2018】Shape Robust Text Detection with Progressive Scale Expansion Network亮点： 利用不同shrink的segmentation来解决text-instance的黏连问题，很有新意； 提出一个自己设计的多个score map逐步扩展算法 方法概述 针对曲文检测，采用instance-segmentation思路，基于FPN框架进行修改，将其用在曲线文本检测上。 文章提出了曲文检测的当前两大问题： 目前已有的文本表示方法（正矩形，斜矩形，四边形）无法满足任意形状的文本检测； 解决思路：用分割来做。 已有的分割方法的最大问题在于紧邻的text instance容易黏连。 解决思路：对文本区域（gt-dt）进行不同程度的shrink，然后逐步扩展。Figure 1: The results of different methods, best viewed in color. (a) is the original image. (b) refers to the result of bounding box regression-based method, which displays disappointing detections as the red box covers nearly more than half of the context in the green box. (c) is the result of semantic segmentation, which mistakes the 3 text instances for 1 instance since their boundary pixels are partially connected. (d) is the result of our proposed PSENet, which successfully distinguishs and detects the 4 unique text instances. 整个检测方法的流程是：使用FPN网络得到多个shrink程度不一样的segmentation map，再把多个map进行逐步扩展得到最终的map。Figure 2: Illustration of our overall pipeline. The left part is implemented from FPN [16]. The right part denotes the feature fusion and the progressive scale expansion algorithm Mask-PAN论文：Zhida Huang——【2018】Mask R-CNN with Pyramid Attention Network for Scene Text Detection亮点： 基于Mask RCNN进行修改，可做四边形回归 首次将PAN用在文本检测上 方法概述 针对曲文检测，采用Instance-segmentation思路，基于Mask-RCNN进行修改，将其用在曲线文本检测上。 改进的点在于三个： 在backbone网络中加入PAN（Pyramid Attention Network，由Feature Pyramid Attention和Global Attention Up-Sample两个部分组成），使得特征对scale大小鲁棒性更强； 将Mask-RCNN的regression分支由box回归（4个值）改为polygon回归（8个值），使其可以用做四边形回归（但还是不能用来做曲文的回归，曲文用的是mask的多边形框； 参照ION的思想，提出Skip-RoiAlign在多层进行融合Figure 1: Architecture of our Mask R-CNN based text detector, which consists of a PAN backbone network, a region proposal network, a Fast R-CNN detector and a mask prediction network. TextField论文：Yongchao Xu——【2018】TextField_Learning A Deep Direction Field for Irregular Scene Text Detection亮点： 提出的TextField方法非常新颖，用点到最近boundary点的向量来区分不同instance 方法概述 针对曲文检测，采用Instance-segmentation思路，提出一种对于分割点的新的表示方法TextField，旨在解决text instance的黏连问题。 TextField是一个二维的向量v，用来表示分割score map上的每一个点，它的含义是：每个text像素点到离自己最近的boundary点的向量。它的属性包括： 非text像素点=(0, 0)，text像素点!=(0, 0) 向量的magnitude，可以用来区分是文字/非文字像素点 向量的direction，可以用来进行后处理帮助形成文本块 具体检测流程是：用一个VGG+FPN网络学习TextField的两张score map图，然后这两张图上做关于超像素、合并、形态学等后处理来得到text instance。Fig. 3: Pipeline of the proposed method. Given an image, the network learns a novel direction field in terms of a two-channel map, which can be regarded as an image of two-dimensional vectors. To better show the predicted direction field, we calculate and visualize its magnitude and direction information. Text instances are then obtained based on these information via the proposed post-processing using some morphological tools. SPCNET论文：Enze Xie——【AAAI2019】Scene Text Detection with Supervised Pyramid Context Network亮点： 基于Mask R-CNN进行修改，加Attention机制，结合global信息 利用Mask的分数来进行Re-score 方法概述 针对曲文检测，采用Instance-segmentation思路，基于Mask R-CNN进行修改，将其用在曲线文本检测上。 文章的motivation认为，已有的Mask R-CNN用在文本检测上有两个问题： 第一，每个ROI单独做box regression等，缺乏不同region间的context信息（例如，盘子经常出现在桌子上）； 第二，Mask R-CNN的box针对水平文字，不利于倾斜文本，因为背景像素点占了很大比例（还有，比如用box后两行text的box会有较大覆盖）。 作者提出的解决办法是： 针对问题一，提出一个Text Context Module，加入SSTD的Attention机制并把global信息和local信息进行fusion； 针对问题二，提出一种Re-score Mechanism，利用Mask的score和box的score进行平均来解决倾斜文本的分类分数错误问题。 整个检测流程是：用Mask-RCNN+Attention网络进行inference，后处理用Mask的分数Re-socre，利用得到的mask来得到最后的检测结果（minAreaRect）。Figure 2: The architecture of our method. (a) The Feature Pyramid Network (FPN) architecture. (b) Pyramid Feature fusion via TCM. (c) Mask R-CNN branch for text classification, bounding box regression and instance segmentation. (d) The proposed Text-Context Module(TCM). Dotted line indicates the text semantic segmentation branch. The text segmentation map is upsampled to the input image size and calculates the loss with Ground Truth. CENet论文：Jiaming Liu——【2019】Detecting Text in the Wild with Deep Character Embedding Network亮点： 通过将文本的字符合并问题转成字符embedding问题，利用一个网络来学习字符间的连接关系方法概述 针对任意文本检测（水平、倾斜、曲文），采用从字符到文本行的自底向上的pipeline。先用一个网络CENet学习两个任务，包括单个字符的检测，以及一个字符对的embedding向量（表示两个字符是否可以构成一个pair）。然后再用一个字符分类阈值提取检测到的字符，和一个合并阈值提取group的字符对。最后利用WordSup中的文本线生成算法（图模型+一阶线性模型）得到文本行。实际test时步骤： 运行CENet，得到字符候选集合+字符对候选集合 利用分数阈值s过滤非字符噪声 对每个字符运用r-KNN，查找local的character pairs（参数d、k） 使用piecewise linear model（分段线性拟合）来得到character group的最外接任意多边形Fig. 2. Overall process of the model. Blue bounding boxes in \character proposals” are character candidates with high confidence scores. \Character Clusters” is the character clusters in embedding space, where candidates in the same cluster use the same color. The final detected words represented in quadrangles are shown in \Detected text”. Better view in color. MSR论文：Chuhui Xue——【arxiv2019】MSR_Multi-Scale Shape Regression for Scene Text Detection亮点： multi-scale网络中利用FPN的up-sampling把多个不同scale得到的结果进行融合（concat + uppooling） boundary-point regression部分直接预测点与最近的boundary point的dx和dy，思路清晰且易实现方法概述 针对任意文本检测（水平、倾斜、曲文），通过网络来regress文字的边界像素点来得到text region。整个检测的流程包括： 特征提取：通过一个类似于Image Pyramid的多通道多尺度网络来提取不同scale的图像特征（FPN框架） 目标预测：预测包括三个分支 text region的classification分支 与nearest boundary point之间的x的dis 与nearest boundary point之间的y的dis 结果输出：利用Alpha-Shape Algorithm从boundary point set中得到外边界凸多边形Fig. 1: Scene text detection using the proposed multi-scale shape regression network (MSR): For scene texts with arbitrary orientations and shapes in (a), MSR first predicts dense text boundary points (in red color) as shown in (b) and then locates texts by a polygon (in green color) that encloses all boundary points of each text instance as shown in (c). 总结NMS方法 locality-aware NMS ：X. Zhou, C. Yao, H. Wen, Y. Wang, S. Zhou, W. He, and J. Liang, “East: An efficient and accurate scene text detector,” arXiv preprint arXiv:1704.03155, 2017. inclined NMS ：Y. Jiang, X. Zhu, X. Wang, S. Yang, W. Li, H. Wang, P. Fu, and Z. Luo, “R2cnn: Rotational region cnn for orientation robust scene text detection,” arXiv preprint arXiv:1706.09579, 2017. Mask-NMS ：Y. Dai, Z. Huang, Y. Gao, and K. Chen, “Fused text segmentation networks for multi-oriented scene text detection,” arXiv preprint arXiv:1709.03272, 2017. polygonal NMS(PNMS) ：L. Yuliang, J. Lianwen, Z. Shuaitao, and Z. Sheng, “Detecting curve text in the wild: New dataset and new solution,” arXiv preprint arXiv:1712.02170, 2017. PlusPS Vatti clipping algorithm用于对多边形进行shrink ：Bala R Vatti. A generic solution to polygon clipping. Communications of the ACM, 1992. RamerDouglas-Peucker algorithm用于利用mask得到多边形 ：Urs Ramer. An iterative procedure for the polygonal approximation of plane curves. CGIP, 1972.]]></content>
      <categories>
        <category>文本检测</category>
      </categories>
      <tags>
        <tag>文本检测</tag>
        <tag>曲线文本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CVPR2018 | 旷视科技提出通过角点定位与区域分割来检测多方向的文本]]></title>
    <url>%2F%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%2FCVPR2018-%E6%97%B7%E8%A7%86%E7%A7%91%E6%8A%80%E6%8F%90%E5%87%BA%E9%80%9A%E8%BF%87%E8%A7%92%E7%82%B9%E5%AE%9A%E4%BD%8D%E4%B8%8E%E5%8C%BA%E5%9F%9F%E5%88%86%E5%89%B2%E6%9D%A5%E6%A3%80%E6%B5%8B%E5%A4%9A%E6%96%B9%E5%90%91%E7%9A%84%E6%96%87%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[Multi-Oriented Scene Text Detection via Corner Localization and Region SegmentationKeyWords Plus: CVPR2018 Multi-Oriented Textpaper：https://arxiv.org/pdf/1802.08948.pdfreference: Lyu P, Yao C, Wu W, et al. Multi-oriented scene text detection via corner localization and region segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 7553-7563.Github: https://github.com/lvpengyuan/corner 方法概括方法概述 该方法用一个端到端的网络实现文本检测整个过程。除了基础卷积网络（backbone）外，包括两个并行分支和一个后处理过程。第一个分支是通过一个DSSD网络进行角点检测来提取候选文本区域，第二个分支是利用类似于RFCN进行网格划分的方式来做position-sensitive的segmentation。后处理过程是利用segmentation的score map来综合得分，过滤角点检测得到的候选区域中的噪声。 背景（文本检测三大难点） 多方向 长宽比多变 文本的粒度多样（包括字符、单词、文本行等多种形式） 文章亮点 检测不是用一般的object detection的框架来做，而是用corner point detection来做，可以更好地解决文本方向任意、文本长宽比多变的问题。 分割用的是position sensitive segmentation，仿照RFCN划分网格的思路，把位置信息融合进去，对于检测单词这种细粒度的更有帮助。 把检测和分割两大类的方法整合起来，进行综合打分的pipeline，这可以使检测精度更高。 方法细节主要流程 用一个检测网络完成整个检测过程，该网络分为以下几个部分： backbone：基础网络，用于特征提取（不同分支特征共享）。 corner detection：用来生成候选检测框，是一个独立的检测模块，类似于RPN的功能。 Position Sensitive Segmentation：整张图逐像素的打分，和一般分割不同的是输出4个score map，分别对应左上、右上、右下、左下四个不同位置的得分。 Scoring + NMS：综合打分，利用（2）的框和（3）的score map综合打分，去掉非文本框，最后再接一个NMS。 网络结构 Backbone取自DSSD = VGG16(pool5) + conv6(fc6) + conv7(fc7) + 4conv + 6 deconv (with 6 residual block)。 Corner Point Detection是类似于SSD，从多个deconv的feature map上单独做detection得到候选框，然后多层的检测结果串起来，nms后为最后的结果。 损失： Corner Detection思路说明 Step1: 用DSSD框架（任何一个目标检测的框架都可以）找到一个框的四个角点，然后整张图的所有角点都放到一个集合中。 Step2: 把集合中的所有角点进行组合得到所有候选框。 网络结构 Fi表示backbone结构中的后面几个deconv得到的feature map（每层都单独做了detection）。 w, h是feature map大小，k是defalt box的个数，q表示角点类型，这里q = 4，即每个位置（左上、右上、右下、左下）都能单独得到2个score map和4个offset map。 角点信息 实际上是一个正方形，正方形中心为gt框（指的是文本框）的顶点，正方形的边长 = gt框的最短边。 corner detection对每一种角点（四种）单独输出corner box，可以看做是一个四类的目标检测问题。 角点如何组合成文本框？ 由于角点不但有顶点位置信息，也有边长信息，所以满足条件的两个corner point组合起来可以确定一个文字框。 具体组合思路如下： 一个rotated rectangle可以由两个顶点+垂直于两个顶点组成的边且已知长度的边来确定。 由于角点的顶点类型确定，所以短边方向也是确定的，例如左上-左下连边确定短边在右边。 垂直的边的长度可以取两个角点的正方形边长的平均值。 可以组合的两个corner point满足条件如下： 角点分数阈值&gt;0.5； 角点的正方形边长大小相似（边长比&lt;1.5）； 框的顶点类型和位置先验信息（例如，“左上”、“左下”的角点的x应该比“右上”、“右下”小）。 损失 得分分支（score branch）的损失：Cross Entropy损失。 偏移分支（offset branch）的损失：Smooth L1损失。 Poosition Sensitive Segmentation 思路说明 把一个文字框划分成g * g个网格的小框（一个bin），每个bin做为一类，把一个text/non-text的二类问题变成g * g个二类问题。 网络结构 损失：dice损失 Scoring 目的 利用segmentation得到的g * g张score map来进一步判断corner detection得到的那些候选框是否是文字，过滤噪声框。 思路说明 把detection得到的框先划分成g * g个网格（bin），每个bin对应各自的segmentation score map，然后把对应的score map里对应bin位置的那些前景点的像素值取平均（底下P的分母是C，不是直接R的面积，或者bin的面积可以看出只取前景点，即score&gt;0的点）作为每个bin的分数，最后把g * g个bin的分数取平均为初始文字框的分数。 算法伪代码 为什么scoring这么复杂，不直接用区域的像素平均值？ 采用position sensitive segmentation的思路决定了最后scoring的时候也必须划网格单独取值再平均。至于之所以只取前像素点是为了平均时不受背景点影响，更加精确。 注意一点 该文章的分割分支与角点分支（也可以叫做回归分支，候选目标检测分支，作用类似于RPN，用来回归候选框）的组合方式与其他的目标检测方法（例如RPN，SSD）中分割与回归分支的组合方式完全不同。对于RPN或SSD，其回归分支与分割分支共同作用才能得到候选框，分割分支的score map上的每个像素点值决定了回归分支中某几个框是否是有效的（目标框还是背景框）。而这篇文章的分割分支和回归分支除了共用特征外，以及最后把损失都加到损失层外，两个分支是完全独立的！也就是说，回归分支可以换成任何一个可以生成候选框的目标检测分支（RPN，SSD，甚至Faster R-CNN），分割分支可以换成任何一个可以生成目标置信概率图的分支，而把这两个分支加上最后的综合打分就可以让分割分支去帮助目标检测分支进一步过滤噪声（但对框的生成没有影响）。这种框架是把目前的两大类文字检测方法（基于目标检测的框架，和基于分割的框架）综合起来，可以提高检测精度。 问题搜集 为什么要用position-sensitive segmentation？从结果来讲，和普通的segmentation而言，到底好在哪里？是否是更适合各种粒度的文字（字符，单词，文本行）? 文章中用角点检测来检测文字和一般用目标检测的方法来检测文字，其优势和劣势在哪里? 优势 角点检测里每个角点都是独立的，所以在多个特征层上的detection的角点可以全部放到一个集合里再去两两组合获得文字框，而不采用每个特征层单独做直接得到文字框后再多层融合。这样好处在于，同一个框的不同角点可以是从不同的特征层上检测到的，即使在某一层上某个角点漏了，但有其他层可以帮忙将其找回来。 角点检测不用考虑方向性，所以可以用任何一般的目标检测框架而无需修改（加方向参数等）直接用于检测角点。 角点检测不用考虑文字框的长宽比大小，对于类似RPN等基于anchor的其defaut box的长宽比和scale不好设置，尤其针对长文本，角点检测则不用考虑这个问题。 可能潜在的问题 如果角点很多，那么这种两两组合的可能性很多，使得检测框特别多，对后续nms等压力较大。 由于任意两个角点都组合，所以可能导致很多无关的框、没有意义的框都被当做候选框（比如从左上顶点和右下顶点组成的框）。 我认为文章中疑问或者有问题的点 Corner Point Prediction的offset输出没有必要是4维的，因为已知是正方形的情况下，只需x1，y1，s三维就好了（w = h = s）。 部分存在歧义，没有说清楚：We determine the relative position of a rotated rectangle by the following rules: 1) the x-coordinates of top left and bottom-left corner points must less than the x coordinates of top-right and bottom-right corner points; 2)the y-coordinates of top-left and top-right corner points must less than the y-coordinates of bottom-left and bottom right corner points. 实验结果 深度框架：Pytorch 结果与速度说明 Nvidia Titan Pascal GPU ICDAR2013：图像大小resize成512 * 512，100ms/每张图，F值=85.5%/88.0%（多尺度）。 MSRA-TD500：图像大小768 * 768，5.7FPS，F值=81.5%。 MLT：768 * 768，F值=72.4%。 ICDAR2015：768 * 1280，1FPS，F值= 84.3%。 COCO-Text：768 * 768，IOU=0.5，F值=42.5%。 ICDAR2013 ICDAR2015 MSRA-TD500 COCO-Text MLT 总结 用角点检测来做目标检测问题很有新意。 Position-sensitive segmentation和一般的segmentation不太一样，虽然不是完全理解用这个的原因（融合位置信息？）。 做一次分割，再做一次目标检测，两个共同来打分，这个思路很有意思，不管是用什么做分割或检测，不管分割和检测是否共用网络基础结构，不管分割和检测之间是否有关系，这个框架都很可取。]]></content>
      <categories>
        <category>文本检测</category>
      </categories>
      <tags>
        <tag>文本检测</tag>
        <tag>四边形文本</tag>
        <tag>旷视</tag>
        <tag>角点定位与区域分割</tag>
        <tag>CVPR2018</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN网络结构的发展：从LeNet到EfficientNet]]></title>
    <url>%2FCNN%2FCNN%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E7%9A%84%E5%8F%91%E5%B1%95%EF%BC%9A%E4%BB%8ELeNet%E5%88%B0EfficientNet%2F</url>
    <content type="text"><![CDATA[CNN基本部件介绍局部感受野 在图像中局部像素之间的联系较为紧密，而距离较远的像素联系相对较弱。因此，其实每个神经元没必要对图像全局进行感知，只需要感知局部信息，然后在更高层局部信息综合起来即可得到全局信息。卷积操作即是局部感受野的实现，并且卷积操作因为能够权值共享，所以也减少了参数量。 池化 池化是将输入图像进行缩小，减少像素信息，只保留重要信息，主要是为了减少计算量。主要包括最大池化和均值池化。 激活函数 激活函数的作用是用来加入非线性。常见的激活函数有sigmod，tanh，relu，前两者常用在全连接层，relu常见于卷积层。 全连接层 全连接层在整个卷积神经网络中起分类器的作用。在全连接层之前需要将之前的输出展平。 经典网络结构1、LeNet5 由两个卷积层，两个池化层，两个全连接层组成。卷积核都是5×5，stride=1，池化层使用maxpooling。 2、AlexNet 模型共八层（不算input层），包含五个卷积层、三个全连接层。最后一层使用softmax做分类输出。AlexNet使用了ReLU做激活函数；防止过拟合使用dropout和数据增强；双GPU实现；使用LRN。 3、VGG 全部使用3×3卷积核的堆叠，来模拟更大的感受野，并且网络层数更深。VGG有五段卷积，每段卷积后接一层最大池化。卷积核数目逐渐增加。 总结：LRN作用不大；越深的网络效果越好；1×1的卷积也很有效但是没有3×3好。 4、GoogLeNet(inception v1) 从VGG中了解到，网络层数越深效果越好。但是随着模型越深参数越来越多，这就导致网络比较容易过拟合，需要提供更多的训练数据；另外，复杂的网络意味更多的计算量，更大的模型存储，需要更多的资源，且速度不够快。GoogLeNet就是从减少参数的角度来设计网络结构的。 GoogLeNet通过增加网络宽度的方式来增加网络复杂度，让网络可以自己去应该如何选择卷积核。这种设计减少了参数 ，同时提高了网络对多种尺度的适应性。使用了1×1卷积可以使网络在不增加参数的情况下增加网络复杂度。 Inception-v2 在v1的基础上加入batch normalization技术，在tensorflow中，使用BN在激活函数之前效果更好；将5×5卷积替换成两个连续的3×3卷积，使网络更深，参数更少。 Inception-v3 核心思想是将卷积核分解成更小的卷积，如将7×7分解成1×7和7×1两个卷积核，使网络参数减少，深度加快。 Inception-v4 引入了ResNet，使训练加速，性能提升。但是当滤波器的数目过大（&gt;1000）时，训练很不稳定，可以加入activate scaling因子来缓解。 5、Xception 在Inception-v3的基础上提出，基本思想是通道分离式卷积，但是又有区别。模型参数稍微减少，但是精度更高。Xception先做1×1卷积再做3×3卷积，即先将通道合并，再进行空间卷积。depthwise正好相反，先进行空间3×3卷积，再进行通道1×1卷积。核心思想是遵循一个假设：卷积的时候要将通道的卷积与空间的卷积进行分离。而MobileNet-v1用的就是depthwise的顺序，并且加了BN和ReLU。Xception的参数量与Inception-v3相差不大，其增加了网络宽度，旨在提升网络准确率，而MobileNet-v1旨在减少网络参数，提高效率。 6、MobileNet系列V1 使用depthwise separable convolutions；放弃pooling层，而使用stride=2的卷积。标准卷积的卷积核的通道数等于输入特征图的通道数；而depthwise卷积核通道数是1；还有两个参数可以控制，a控制输入输出通道数；p控制图像（特征图）分辨率。 V2 相比v1有三点不同： 引入了残差结构； 在dw之前先进行1×1卷积增加feature map通道数，与一般的residual block是不同的； pointwise结束之后弃用ReLU，改为linear激活函数，来防止ReLU对特征的破环。这样做是因为dw层提取的特征受限于输入的通道数，若采用传统的residual block，先压缩那dw可提取的特征就更少了，因此一开始不压缩，反而先扩张。但是当采用扩张-卷积-压缩时，在压缩之后会碰到一个问题，ReLU会破环特征，而特征本来就已经被压缩，再经过ReLU还会损失一部分特征，应该采用linear。 V3 互补搜索技术组合：由资源受限的NAS执行模块集搜索，NetAdapt执行局部搜索；网络结构改进：将最后一步的平均池化层前移并移除最后一个卷积层，引入h-swish激活函数，修改了开始的滤波器组。 V3综合了v1的深度可分离卷积，v2的具有线性瓶颈的反残差结构，SE结构的轻量级注意力模型。 7、EffNet EffNet是对MobileNet-v1的改进，主要思想是：将MobileNet-1的dw层分解成两个3×1和1×3的dw层，这样，第一层之后就采用pooling，从而减少第二层的计算量。EffNet比MobileNet-v1和ShuffleNet-v1模型更小，精度更高。 8、EfficientNet 研究网络设计时在depth, width, resolution上进行扩展的方式，以及之间的相互关系。可以取得更高的效率和准确率。 9、ResNet VGG证明更深的网络层数是提高精度的有效手段，但是更深的网络极易导致梯度弥散，从而导致网络无法收敛。经测试，20层以上会随着层数增加收敛效果越来越差。ResNet可以很好的解决梯度消失的问题（其实是缓解，并不能真正解决），ResNet增加了shortcut连边。 10、ResNeXt 基于ResNet和Inception的split+transform+concate结合。但效果却比ResNet、Inception、Inception-ResNet效果都要好。可以使用group convolution。一般来说增加网络表达能力的途径有三种：1.增加网络深度，如从AlexNet到ResNet，但是实验结果表明由网络深度带来的提升越来越小；2.增加网络模块的宽度，但是宽度的增加必然带来指数级的参数规模提升，也非主流CNN设计；3.改善CNN网络结构设计，如Inception系列和ResNeXt等。且实验发现增加Cardinatity即一个block中所具有的相同分支的数目可以更好的提升模型表达能力。 11、DenseNet DenseNet通过特征重用来大幅减少网络的参数量，又在一定程度上缓解了梯度消失问题。 12、SqueezeNet 提出了fire-module：squeeze层+expand层。Squeeze层就是1×1卷积，expand层用1×1和3×3分别卷积，然后concatenation。squeezeNet参数是alexnet的1/50，经过压缩之后是1/510，但是准确率和alexnet相当。 13、ShuffleNet系列V1 通过分组卷积与1×1的逐点群卷积核来降低计算量，通过重组通道来丰富各个通道的信息。Xception和ResNeXt在小型网络模型中效率较低，因为大量的1×1卷积很耗资源，因此提出逐点群卷积来降低计算复杂度，但是使用逐点群卷积会有副作用，故在此基础上提出通道shuffle来帮助信息流通。虽然dw可以减少计算量和参数量，但是在低功耗设备上，与密集的操作相比，计算、存储访问的效率更差，故shufflenet上旨在bottleneck上使用深度卷积，尽可能减少开销。 V2 使神经网络更加高效的CNN网络结构设计准则： 输入通道数与输出通道数保持相等可以最小化内存访问成本。 分组卷积中使用过多的分组会增加内存访问成本。 网络结构太复杂（分支和基本单元过多）会降低网络的并行程度。 element-wise的操作消耗也不可忽略。 14、SENet 15、SKNet]]></content>
      <categories>
        <category>CNN</category>
      </categories>
      <tags>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CVPR2019 | 商汤提出金字塔掩模文本检测器：PMTD]]></title>
    <url>%2F%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%2FCVPR2019-%E5%95%86%E6%B1%A4%E6%8F%90%E5%87%BA%E9%87%91%E5%AD%97%E5%A1%94%E6%8E%A9%E6%A8%A1%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E5%99%A8%EF%BC%9APMTD%2F</url>
    <content type="text"><![CDATA[Pyramid Mask Text DetectorKeyWords Plus: CVPR2019 Quadrilateral Textpaper：https://arxiv.org/pdf/1903.11800.pdfreference: Liu, Jingchao &amp; Liu, Xuebo &amp; Sheng, Jie &amp; Liang, Ding &amp; Li, Xin &amp; Liu, Qingjie. (2019). Pyramid Mask Text Detector.Github: 未开源 本文是商汤和香港中文大学联合发表并于 2019.03.28 挂在 arXiv 上，本文的方法在 ICDAR2017 MIT 数据集上，相比于之前最高的精确率提升了 5.83% 百分点，达到 80.13%；在 ICDAR2015 数据集上，提升了 1.34% 个百分点，达到 89.33%。 论文主要思想 本文提出了 Pyramid Mask 文本检测器，简称 PMTD。它主要做了如下工作： 提出了软语义分割的训练数据标签。与现有的基于 Mask RCNN 方法（文本区域内的像素标签为 0 或 1）不同，本文针对文本区域和背景区域提出了软语义分割（soft semantic segmentation），文本行区域内的像素标签值范围在 0-1 之间，不同位置的像素标签值是由其当前位置到文本边界框的距离决定的，这样做的好处是可以考虑训练数据的形状和位置信息，同时可以一定程度上缓解文本边界区域的一些背景干扰； 提出通过平面聚类的方法构建最终的文本行。通过像素坐标及对应像素点的得分构建 3D 点集合，然后通过金字塔平面聚类的迭代方法得到最终的文本行。 实验 文中做了两个实验：baseline 和 PMTD。baseline 是基于 Mask RCNN 的，主干提取特征网络采用的是 ResNet50，网络结构采用了 FPN。相比原生的 Mask RCNN，做了 3 方面修改：1）数据增广；2）RPN anchor；3）OHEM。 baseline存在的问题 没有考虑普通文本一般是四边形，仅按照像素进行分类，丢失了与形状相关的信息； 将文本行的四边形的标定转换为像素级别的 groundtruth 会造成 groundtruth 不准的问题； 在 Mask R-CNN 中是先得到检测的框，然后对框内的物体进行分割，如果框的位置不准确，这样会导致分割出来的结果也不会准确。 PMTD所做的改进 PMTD 是针对 baseline 中存在的问题提出的改进，主要包括： 网络结构的改进：PMTD 采用了更大的感受野来获取更高的准确率，为了获取更大的感受野，本文通过改变 mask 分支，将该分支中的前 4 个卷积层改成步长为 2 的空洞卷积，因为反卷积操作会带来棋盘效应，所以这里采用双线性采样＋卷积层来替换反卷积层； 对于训练标签生成部分，使用了金字塔标签生成，具体做法是：文本行的中心点为金字塔的顶点（score=1），文本行的边为金字塔的底边，对金字塔的每个面中应该包含哪些像素点采用双线性插值的方法。 最终文本行的生成 文中使用了平面聚类的方法，用于迭代回归从已学习到的 soft text mask 寻找最佳的文本行的边界框。在具体操作时，可以看成与金字塔标签生成的反过程。]]></content>
      <categories>
        <category>文本检测</category>
      </categories>
      <tags>
        <tag>文本检测</tag>
        <tag>CVPR2019</tag>
        <tag>PMTD</tag>
        <tag>商汤</tag>
        <tag>四边形文本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ECCV2018 | 旷视科技提出弯曲文本表示TextSnake]]></title>
    <url>%2F%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%2FECCV2018-%E6%97%B7%E8%A7%86%E7%A7%91%E6%8A%80%E6%8F%90%E5%87%BA%E5%BC%AF%E6%9B%B2%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BATextSnake%2F</url>
    <content type="text"><![CDATA[TextSnake: A Flexible Representation for Detecting Text of Arbitrary ShapesKeyWords Plus: ECCV2018 Curved Textpaper：https://arxiv.org/abs/1807.01544reference: Long S, Ruan J, Zhang W, et al. Textsnake: A flexible representation for detecting text of arbitrary shapes[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 20-36.Github: https://github.com/princewang1994/TextSnake.pytorch 背景 近年来，从自然场景中提取文本信息，即场景文本检测与识别，成为了学术研究的热点。究其原因有二，应用前景和学术价值。一方面，场景文本检测与识别在一系列的实际应用中发挥着日益重要的作用，比如场景理解，产品搜索，自动驾驶等；另一方面，场景文本自身的独特属性使其有别于一般物体。 作为文本信息提取的前提条件之一，文本检测在深度神经网络和大型数据集的助力之下，取得了长足进展，出现了大量创新性工作，并在基准数据集上取得了优异的表现。 但是，现有大多数的文本检测方法有一个共同的假设：文本实例的形状大体上是线性的，因此可以采用相对简单的表征（轴对齐矩形，旋转矩形，四边形）去描述它们。尽管存在不少进步，但是在处理不规则形状的文本实例时，依然会暴漏出短板。如图 1 所示，对于带有透视变形（perspective distortion）的曲形文本（curved text）来讲，传统的表征方法在精确估计几何属性方面显得力不从心。 图 1：文本实例不同表征方法的对比。（a）轴对齐矩形。（b）旋转矩形。（c）四边形。（d）TextSnake。 设计思想 事实上，曲形文本的情况在现实世界中很常见。本文提出一种更为灵活的表征，可以很好地拟合任意形状的文本，比如水平文本，多方向文本，曲形文本。这种表征通过一系列有序、彼此重叠的圆盘（disk）描述文本，每个圆盘位于文本区域的中心轴上，并带有可以变化的半径和方向。由于其在适应文本结构多样性方面的优异表现，就像蛇一样改变形状适应外部环境，该方法被命名为 TextSnake。文本实例的几何属性（比如中心轴点，半径，方向）则通过一个全卷积网络（FCN）进行评估。 除了 ICDAR 2015 和 MSRA-TD500 之外，TextSnake 的有效性还在 Total-Text 和 SCUT-CTW1500 （两个新公布的针对曲形文本的数据集）上获得了验证，并取得了当前最优的表现；此外，该方法还在水平文本和多方向文本上超越先前方法，即使是在单一尺度测试模式之下。具体而言，TextSnake 获得显著提升，在 Total-Text 数据集上 F-measure 超越基线 40%。 总结一下，本文贡献主要有 3 个方面：（1）本文提出一种灵活而通用的表征，可用于任意形状的场景文本；（2）基于这一表示，本文提出一种有效的场景文本检测方法；（3）该方法在包含若干个不同形式（水平，多方向，曲形）的文本实例数据集上取得了当前最优的结果。 方法表征 图 2：TextSnake 图示。 TextSnake 将一个文本区域（黄色）表征为一系列有序而重叠的圆盘（蓝色），其中每个圆盘都由一条中心线（绿色，即对称轴或骨架）贯穿，并带有可变的半径 r 和方向 θ 。直观讲，TextSnake 能够改变其形状以适应不同的变化，比如旋转，缩放，弯曲。 从数学上看，包含若干个字符的文本实例 t 可被看作是一个序列 S(t) 。S(t) = {D_0,D_1,··· ,D_i,··· ,D_n} ，其中 D_i 表示第 i 个圆盘，n 表示圆盘的数量。每个圆盘 D 带有一组几何属性， r 被定义为 t 的局部宽度的一半，方向 θ 是贯穿中心点 c 的中心线的正切。由此，通过计算 S(t) 中圆盘的重合，文本区域 t 可轻易被重建。 注意，圆盘并非一一对应于文本实例的字符。但是圆盘序列的几何属性可以改正不规则形状的文本实例，并将其转化为对文本识别器更加友好的矩形等。### Pipeline图 3：方法框架图：网络输出与后处理。 为检测任意形状的文本，本文借助 FCN 模型预测文本实例的几何属性。基于 FCN 的网络预测文本中心线（TCL），文本区域（TR）以及几何属性（包括 r，cosθ，sinθ）的分值图。由于 TCL 是 TR 的一部分，通过 TR 而得到 Masked TCL。假定 TCL 没有彼此重合，需要借助并查集（disjoint set）执行实例分割。Striding Algorithm 用于提取中心轴点，并最终重建文本实例。### 架构图 4：网络架构。蓝色方块表示 VGG-16 的卷积阶段。 在 FPN 和 U-net 的启发下，本文提出一个方案，可逐渐融合来自主干网络不同层级的特征。主干网络可以是用于图像分类的卷积网络，比如 VGG-16/19 和 ResNet。这些网络可以被分成 5 个卷积阶段（stage）和若干个额外的全连接层。本文移除全连接层，并在每个阶段之后将特征图馈送至特征融合网络。出于与其他网络进行公平而直接对比的考虑，本文选择 VGG-16 作为主干网络。### 预测 馈送之后，网络输出 TCL，TR 以及几何图。对于 TCL 和 TR，阈值分别设为 T_tcl 和 T_tr；接着，TCL 和 TR 的交叉点给出 TCL 最后的预测。通过并查集，可以有效把 TCL 像素分割进不同的文本实例。最后，Striding Algorithm 被设计以提取用来表示文本实例形状和进程（course）的有序点，同时重建文本实例区域。图 5：后处理算法图示。 Striding Algorithm 的流程如图 5 所示。它主要包含 3 个部分：Act(a)Centralizing ，Act(b) Striding 和 Act(c)Sliding 。首先，本文随机选择一个像素作为起点，并将其中心化。接着，搜索过程分支为两个相反的方向——striding 和 centralizing 直到结束。这一过程将在两个相反方向上生成两个有序点，并可结合以生成最终的中心轴，它符合文本的进程，并精确描述形状。## 实验 在标准数据集上评估了 TextSnake 的场景文本检测能力，并与先前同类方法进行了对比，数据集主要有 SynthText，TotalText，CTW1500，ICDAR 2015，MSRA-TD500。### Total-Text &amp; CTW1500 Total-Text &amp; CTW1500 数据集上展开的是有关曲形文本的实验，其优异表现证明了TextSnake 在处理曲形文本方面的有效性。表 1 &amp; 表 2 分别是两个数据集上不同方法的量化结果。表 1：Total-Text 上不同方法的量化结果。TextSnake 在精度、查全率、F 值上分别取得了82.7%，74.5%，78.4% 的成绩，大幅超越先前方法。表 2：CTW1500 上不同方法的量化结果。TextSnake 在精度、查全率、F 值上分别取得了 67.9%，85.3%，75.6% 的成绩。 ICDAR 2015 ICDAR 2015 上进行的是有关偶然场景文本的实验。在单一尺度测试中，TextSnake 超越了绝大多数现有方法（包括那些在多尺度中评估的方法），这证明了 TextSnake 的通用性以及已经可用于复杂场景的多方向文本。 表 3：ICDAR 2015 上不同方法的量化结果。∗ 表示多尺度，† 表示模型的主干网络不是 VGG-16。 MSRA-TD500 本文在 MSRA-TD500 上进行有关长直文本线的实验。其中 TextSnake 的 F 值 78.3% 优于其他方法。 表 4：MSRA-TD500 上不同方法的量化结果。† 表示模型的主干网络不是 VGG-16。 分析与讨论 TextSnake 之所以出类拔萃，在于其对文本实例的进程及形状的精确描述具有预测的能力（见图 8）。而这一能力来自对 TCL 进行的预测，它要比整个文本实例窄很多。这样做有两个优势：1）纤细的 TCL 可以更好地描述进程和形状；2）TCL 彼此不会重叠，因此实例分割得以一种十分简单而直接的方式完成，由此简化 pipeline。 图 8：TextSnake 定性结果。上：已检测文本轮廓（黄色）和groundtruth注解（绿色）。下：TR（红色）和 TCL（黄色）的分值合图。从左到右图像分别来自 ICDAR 2015，TotalText，CTW1500 和 MSRA-TD500。 此外，本文还利用局部几何属性描绘文本实例的结构，把已预测的曲形文本实例转化为规范形式，这大大减轻了后续识别阶段的工作。 图 9：通过已预测的几何属性把文本实例转化为规范形式。 为进一步验证 TextSnake 的泛化能力，本文在不包含曲形文本的数据集上进行了训练和微调，并在含有曲形文本的两个数据集上做了评估。在没有曲形文本微调的情况下，TextSnake 依然表现良好，并显著超越其他三个竞争者 SegLink，EAST 和 PixelLink，这要归功于 TextSnake 作为灵活表征的优秀泛化能力（见表 5）。 表 5：不同方法下的交叉验证集结果对比。 TextSnake 把文本看作一个局部元素的集合，而不是一个整体，并通过整合元素的方式做决策。因此，TextSnake 最后的预测可以保持文本进程和形状的最大量信息，这是该算法胜任不同形状文本实例的主要原因。 结论 本文提出一种全新而灵活的表征——TextSnake，可以描述任意形状的场景文本，包括水平文本，多方向文本和曲形文本。基于 TextSnake 的文本检测新方法已在两个新开源的曲形文本数据集（Total-Text 和 SCUT-CTW1500）和两个经典数据集（ICDAR 2015 和 MSRA-TD500）上取得了当前最优或有竞争力的结果，证实了该方法的有效性。未来，本文作者将尝试开发一个针对任意形式文本的端到端识别系统。]]></content>
      <categories>
        <category>文本检测</category>
      </categories>
      <tags>
        <tag>文本检测</tag>
        <tag>曲线文本</tag>
        <tag>旷视</tag>
        <tag>TextSnake</tag>
        <tag>ECCV2018</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CVPR2019 | NAVER提出字符级别的文本检测网络：CRAFT]]></title>
    <url>%2F%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%2FCVPR2019-NAVER%E6%8F%90%E5%87%BA%E5%AD%97%E7%AC%A6%E7%BA%A7%E5%88%AB%E7%9A%84%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E7%BD%91%E7%BB%9C%EF%BC%9ACRAFT%2F</url>
    <content type="text"><![CDATA[Character Region Awareness for Text DetectionKeyWords Plus: CVPR2019 Curved Textpaper：https://arxiv.org/abs/1904.01941reference: Baek Y, Lee B, Han D, et al. Character Region Awareness for Text Detection[J]. arXiv preprint arXiv:1904.01941, 2019.NAVER：line的母公司，韩国的最大的互联网公司，字符级别的文字检测，采用了CAM热力图的操作去检测每一个字符。Github: 未开源 Introduction 字符级别的文本检测网络，用分水岭算法生成label，采用heatmaps去得到激活值最大的目标区域，有点类似attention。 1、论文创新点 提出了一种字符级别的文本检测算法; 预测得到:1.The character region score 2. Affinity score. The region score is used to localize individual characters in the image, and the affinity score is used to group each character into a single instance. Propose a weakly-supervised learning framework that estimates character-level groundtruths in existing real word-level datasets. 2、算法主体 该论文主要预测每个字符区域和字符之间的紧密程度，因为没有字符级别的标注，所以模型训练是在弱监督的方式下。网络的backbone采用VGG16，之后接上采样，最终输出两个通道：the region score and the affinity score。 训练在弱监督学习的方式下，首先有人造合成的数据集，具有字符级别的label；然后real image没有字符级别的标注时，网络检测合成产生label再进行训练。如上图所示，对真实场景中的数据集和人造合成的数据集分别有不同的训练方式。 3、label generation 分别产生Region Score GT和Affinity Score GT。 The following steps to approximate and generate the ground truth for both the region score and the affinity score:1) prepare a 2-dimensional isotropic Gaussian map;2) compute perspective transform between the Gaussian map region and each character box;3) warp Gaussian map to the box area. 使用小感受野也能预测大文本和长文本，只需要关注字符级别的内容而不需要关注整个文本实例。 分三步产生字符级别的label： 抠出文本级别的内容； 预测region score区域； 运用分水岭算法； 得到字符基本的文字框; 贴上文字框; 为了防止在弱监督方式下产生的错误label带偏网络，该论文提出了一种评价方式: 4、Post-processing 规则文本后处理可以分为以下几步： 首先对0-1之间的概率图进行取阈值计算； 使用 Connected Component Labeling(CCL) 进行区域连接； 最后使用 OpenCV 的 MinAreaRect 框出最小的四边形区域。 不规则文本检测后处理可以分为以下几步（如上图所示）： 先找到扫描方向的局部最大值（blue line）； 连接所有the local maxima上的中心点叫做中心线； 然后将the local maxima lines旋转至于中心线垂直 the local maxima lines上的端点是文本控制点的候选点，为了能更好的覆盖文本，将文本最外端的两个控制点分别向外移动the local maxima lines的半径长度最为最终的控制点。 5、Experiment Results 6、Conclusion and Future work 个人观点：不太受感受野的限制，只关注单个文字，对于长文本和不规则文本不必特意去设置相应大小的卷积提升感受野。]]></content>
      <categories>
        <category>文本检测</category>
      </categories>
      <tags>
        <tag>文本检测</tag>
        <tag>CRAFT</tag>
        <tag>CVPR2019</tag>
        <tag>NAVER</tag>
        <tag>曲线文本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CVPR2019 | 百度提出LOMO文本检测算法]]></title>
    <url>%2F%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%2FCVPR2019-%E7%99%BE%E5%BA%A6%E6%8F%90%E5%87%BALOMO%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Look More Than Once: An Accurate Detector for Text of Arbitrary ShapesKeyWords Plus: CVPR2019 Curved Textpaper：https://arxiv.org/abs/1904.06535reference: Zhang C, Liang B, Huang Z, et al. Look More Than Once: An Accurate Detector for Text of Arbitrary Shapes[J]. arXiv preprint arXiv:1904.06535, 2019.Github: 未开源 简介 这是百度和厦门大学合作的一篇文章。由于受CNN感受野的限制以及用于描述文本的类似矩形框或者四边形这样的简单的表示方法，当处理更加有挑战的文本实例时，比如极其长的文本和任意形状的文本，之前的一些方法就不适用了。而本文提出的这种方法，正是为了解决这两个问题。 本文提出了一种文本检测器，即LOMO（Look More Than Once），它可以逐步地调整文本（或者换句话说，不止看一次）。LOMO由直接回归器（DR）、迭代细化模块（IRM）和形状表示模块（SEM）组成。首先，DR分支生成四边形的文本proposals。接下来，IRM基于提取的初步proposals的特征块，通过迭代细化逐步感知整个长文本。最后，通过考虑文本实例的几何属性，包括文本区域、文本中心线和边界偏移，引入SEM来得到更加精确的不规则的多边形的文本表示。 论文主要贡献（1）提出了一个迭代细化模块(IRM)，它改善了长文本检测的性能。（2）引入实例级形状表示模块(SEM)，用于解决任意形状的场景文本的检测问题。（3）加入迭代细化模块和形状表示模块的LOMO可以以端到端的方式训练，并在几个包括不同形式的文本实例（有向的、长的、多语言的和弯曲的）的基准数据集上实现了最佳的性能。 网络结构（1）将图像输入到backbone中，抽取 DR, IRM 和 SEM 三个分支的共享特征图。backbone用的是ResNet50 + FPN，对ResNet50中的阶段2，阶段3，阶段4和阶段5的特征图进行特征融合，得到大小是输入图像的1/4，通道是128的特征图。并由之后的DR，IRM和SEM三个分支共享该特征图。（2）采用类似 EAST 和 Deep Regression 的一个direct regression network作为 DR 分支，以一种逐像素的方式去预测单词或文本行的四边形。通常，由于感受野的限制，DR分支很难检测到长文本。如图2（2）所示的蓝色四边形。（3）IRM可以从DR或其自身的输出迭代地细化输入proposals，以使初步文本proposals被完善，以更完整地覆盖文本实例，更接近真实边界框。如图2（3）所示的绿色四边形。（4）为了获得文本的紧凑表示，尤其是不规则的文本，因为四边形的建议形式会覆盖太多的背景区域，通过学习文本的几何性质包括文本区域，文本中心线和边界偏移（中心线和上/下边界线之间的距离），SEM会重建文本实例的形状表示。如图2（4）所示的红色四边形。 Direct Regressor(DR)：直接回归DR–&gt;文本/非文本分类和位置回归 DR模块采用的是一个fully convolutional sub-network。基于共享特征图，计算文本/非文本置信度、和包含该正样本像素的矩形的4个角点的偏移量。DR分支的损失函数由两部分组成：文本/非文本分类和位置回归。 将文本/非文本分类视为在1/4下采样分数图上的二值分割。 第一部分文本分类的损失函数使用的是scale-invariant dice-coefficient函数，用于提升 DR 的尺度泛化能力。其中y是0/1标签图，y^是预测分数图，sum是2D空间上的累积函数。此外，w是二维权重图。正位置的值通过将它们所属的四边形的短边分开的归一化常数l来计算，而负位置的值被设定为1.0。在实验中将常数l设置为64。 第二部分位置回归的损失函数采用的是smooth L1 loss。将这两项结合到一起，DR的整个损失函数可以表示为： 其中超参数λ平衡两个损失项，在实验中设置为0.01。 Iterative Refinement Module(IRM)：迭代细化模块IRM IRM的设计参考基于区域的物体检测，但只有边界框回归任务。使用RoI变换层来提取输入文本四边形的特征块，而不是RoI pooling层或RoI align层。与后两者相比，前者可以在保持纵横比不变的情况下提取四边形建议的特征块。 此外，靠近角点的位置可以在同一感受野内感知到更准确的边界信息。因此，引入角点注意力机制来回归每个角点的坐标偏移。 对于一个DR生成的文本四边形，将其与共享特征图一起输入到RoI变换层，获得1×8×64×128特征块。然后，三个3×3卷积层以进一步提取丰富的上下文信息fr。接下来，使用1×1卷积层和sigmoid层来学习4个角点的注意力图ma。每个角点注意力图上的值表示支持相应角点的偏移回归的贡献权重。 使用fr和ma，可以通过逐点生成和reduce_sum操作来提取4个角点回归特征（图3中绿色的4X{1X1X1X128}特征图）： 其中fci表示形状为1×1×1×128的第i个角点回归特征，mia是第i个角点注意力特征图。 最后，预测输入四边形和真值文本框之间的4个角点的偏移。 在训练阶段，保留来自于DR的K个初步的检测四边形。corner regression loss可以表示为： Shape Expression Module（SEM）：形状表示模块SEM SEM是完全卷积网络，随后是RoI变换层。学习文本的几何属性，包括文本区域、文本中心线和边界偏移（文本中心线和上/下文本边界线之间的偏移），以重建文本实例的精确形状表示。 文本区域是一个二值 mask, 里面的 foreground pixels （比如在多边形标注区域内部的）标记为 1，background pixels 标记为 0。 文本中心线 也是一个二值 mask ，不同的是，它是基于文本多边形标注的 side-shrunk version . 边界偏移 为四通道图, 在文本行图对应位置上的正响应区域内，具有有效的值。当中心线样本（红点）如图4（a）所示，绘制一条垂直于其切线的法线，该法线与上下边界线相交，得到两个边界点（即粉色和橙色）。对于每个红点，通过计算从其自身到其两个相关边界点的距离来获得4个边界偏移。 SEM 的结构如图4所示，RoI变换层提取的共享特征图上的特征块，之后是两个卷积阶段（每个阶段由一个上采样层和两个3×3卷积层组成），然后使用一个带有6个输出通道的1×1卷积层，用于回归所有文本属性映射。SEM 的目标函数定义如下，其中 Ltr 和 Ltcl 使用dice-coefficient loss，Lborder 使用smooth L1 loss： 其中K表示保留IRM的文本四边形的数量，Ltr和Ltcl分别是文本区域和文本中心线的dice系数损失，Lborder是通过Smooth_L1损失计算。在实验中，权重λ1、λ2和λ3设定为0.01,0.01和1.0。 文本多边形生成 通过文本多边形生成策略来重建任意形状的文本实例表示。文本多边形生成策略包括三步：（1）center line sampling： 在预测的文本中心线图上从左到右以等距离间隔采样n个点。（2）border points generation： 基于采样中心线点，考虑同一位置由4个边界偏移图提供的信息，确定相应的边界点。通过顺时针连接所有的边界点，获得完整的文本多边形表示。（3）polygon scoring: 计算多边形内的文本区域响应的平均值，作为新的 confidence score。 训练和推理 整个的损失函数表示为: 其中Ldr，Lirm和Lsem分别代表DR，IRM和SEM的损失。权重γ1，γ2和γ3在三个模块之间折衷，并且在实验中都设置为1.0。Training：训练过程分为两个阶段，warming-up（预热） 和 fine-tuning（微调）.1）在 warming-up 阶段，使用合成数据集训练 DR 部分，迭代 10 epochs。通过这种方式，DR可以生成高召回的proposals，以涵盖实际数据中的大部分文本实例。2）在 fine-tuning 阶段, 在真实数据集上 fine-tune 所有的三个分支，包括 ICDAR2015, ICDAR2017-RCTW, SCUT-CTW1500, Total-Text 和 ICDAR2017-MLT，迭代大约 10 epochs。IRM和SEM分支都使用由DR分支生成的相同提议。非极大值抑制（NMS）用于保留前K个提议。由于DR表现不佳，这将影响IRM和SEM分支的融合，在实践中用随机扰乱的GT文本四边形替换50％的前K个proposals。注意，IRM仅在训练期间进行一次细化。Inference：1）DR 生成四边形的得分图和几何图, 然后用 NMS 生成初步的文本建议。2）文本建议和共享特征图全部输入到 IRM 中改善多次。3）精确的四边形和共享特征图输入到 SEM 中，生成精确的文本多边形和置信度得分。4）阈值 s 用于移除低置信度的多边形。在实验中将s设置为0.1。 数据集ICDAR 2015 包含1000张自然场景图像用于训练和500张用于测试。真值是以单词级别的四边形（word-level quadrangle）标注的。ICDAR2017-MLT 一个大规模的多语言（large scale multi-lingual）文本数据集，包括7200张训练图像、1800张验证图像和9000张测试图像。这个数据集包含来自于9种语言的场景文本图像。文本区域是以有四个顶点的四边形（4 vertices of the quadrangle）标注的。ICDAR2017-RCTW 它包含8034张训练图像和4229张测试图像，这些图像上的场景文本是以中文或者英文打印的。多方向的单词和文本行（Multi-oriented words and text lines）是以四边形（quadrangles）标注的。SCUT-CTW1500 包含1000张训练图像和500张测试图像。文本实例以有14个顶点的多边形标注。Total-Text 一个curved text（曲线文本）基准数据集，包括1255张训练图像和300张测试图像。标注是单词级别（word-level）的。 实验 对于所有的数据集，随机裁剪文本区域并将它们统一缩放到512x512。被裁剪的图像区域将会按照四个方向（0◦、90◦、180◦、270◦）随机旋转。 结论和未来的工作 这篇论文主要解决长文本和弯曲文本的检测问题，LOMO由 DR、IRM and SEM 三个模块组成。DR初步产生文本建议。IRM迭代式的调整DR生成的文本建议。SEM重建不规则文本的精确表示。]]></content>
      <categories>
        <category>文本检测</category>
      </categories>
      <tags>
        <tag>文本检测</tag>
        <tag>CVPR2019</tag>
        <tag>曲线文本</tag>
        <tag>LOMO</tag>
        <tag>百度</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习与深度学习常见问题总结（下）]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1、为什么随机森林能降低方差？随机森林的预测输出值是多棵决策树的均值，如果有n个独立同分布的随机变量xi，它们的方差都为σ^2，则它们的均值的方差为： 2、对于带等式和不等式约束的优化问题，KKT条件是取得极值的充分条件还是必要条件？对于SVM呢？对于一个一般的问题，KKT条件是取得极值的必要条件而不是充分条件。对于凸优化问题，则是充分条件，SVM是凸优化问题。 3、解释维数灾难的概念。当特征向量数值很少时，增加特征，可以提高算法的精度，但当特征向量的维数增加到一定数量之后，再增加特征，算法的精度反而会下降。 4、Logistic回归为什么用交叉熵而不用欧氏距离做损失函数？如果用欧氏距离，不是凸函数，而用交叉熵则是凸函数。 5、解释hinge loss损失函数。如果样本没有违反不等式约束，则损失为0；如果违反约束，则有一个正的损失值。 6、解释GBDT的核心思想。用加法模拟，更准确的说，是多棵决策树来拟合一个目标函数。每一棵决策树拟合的是之前迭代得到的模型的残差。求解的时候，对目标函数使用了一阶泰勒展开，用梯度下降法来训练决策树。 7、解释XGBoost的核心思想。在GBDT的基础上，目标函数增加了正则化项，并且在求解时做了二阶泰勒展开。 8、解释DQN中的经验回放机制，为什么需要这种机制？将执行动作后得到的状态转移构造的样本存储在一个列表中，然后从中随机抽样，来训练Q网络。为了解决训练样本之间的相关性，以及训练样本分布变化的问题。 9、什么是反卷积？反卷积也称为转置卷积，如果用矩阵乘法实现卷积操作，将卷积核平铺为矩阵，则转置卷积在正向计算时左乘这个矩阵的转置W^T，在反向传播时左乘W，与卷积操作刚好相反，需要注意的是，反卷积不是卷积的逆运算。 10、反卷积有哪些用途？实现上采样；近似重构输入图像，卷积层可视化。 11、PCA（主成分分析）优化的目标是什么？最小化重构误差/最大化投影后的方差。 12、LDA（线性判别分析）优化的目标是什么？最大化类间差异与类内差异的比值。 13、解释神经网络的万能逼近定理。只要激活函数选择得当，神经元的数值足够，至少有一个隐含层的神经网络可以逼近闭区间上任意一个连续函数到任意指定的精度。 14、softmax回归训练时的目标函数是凸函数吗？是，但有不止一个全局最优解。 15、SVM为什么要求解对偶问题？为什么对偶问题与原问题等价？原问题不容易求解，含有大量的不易处理的不等式约束。原问题满足Slater条件，强对偶成立，因此原问题与对偶问题等价。 16、神经网络是生成模型还是判别模型？判别模型，直接输出类别标签，或者输出类后验概率p(y|x)。 17、logistic回归是生成模型还是判别模型？判别模型，直接输出类后验概率p(y|x)，没有对类条件概率p(x|y)或者联合概率p(x, y)建模。 18、Batch Normalization 和 Group Normalization有何区别？BN是在batch这个维度上进行归一化，GN是计算channel方向每个group的均值和方差。 19、GAN中模型坍塌（model collapse）是指什么？模型坍塌，即产生的样本单一，没有了多样性。 20、目前GAN训练中存在的主要问题是什么？（1）训练不易收敛；（2）模型坍塌。 21、Shufflenet为什么效果会好？通过引入“通道重排”增加了组与组之间信息交换。 22、模型压缩的主要方法有哪些？（1）从模型结构上优化：模型剪枝、模型蒸馏、automl直接学习出简单的结构。（2）从模型参数上量化：将FP32的数值精度量化到FP16、INT8、二值网络、三值网络等。 23、目标检测中IOU是如何计算的？检测结果与 Ground Truth 的交集比上它们的并集，即为检测的准确率 IoU。 24、给定0-1矩阵，如何求连通域？可采用广度优先搜索。 25、OCR任务中文本序列识别的主流方法是什么？RNN+CTC。 26、在神经网络体系结构中，哪些会有权重共享？（1）卷积神经网络CNN（2）递归神经网络RNN 27、一个典型人脸识别系统的识别流程？人脸检测–&gt;人脸对齐–&gt;人脸特征提取–&gt;人脸特征比对。 28、平面内有两个矩形，如何快速计算它们的IOU？ 29、使用深度卷积网络做图像分类如果训练一个拥有1000万个类的模型会碰到什么问题？提示：内存/显存占用；模型收敛速度等。 30、HMM和CRF的区别？前者描述的是 P(X,Y)=P(X|Y)*P(Y)，是 generative model；后者描述的是 P(Y|X)，是 discriminative model。前者要加入对状态概率分布的先验知识，而后者完全是 data driven。 31、深度学习中为什么不用二阶导去优化？Hessian矩阵是n*n，在高维情况下这个矩阵非常大，计算和存储都是问题。 32、深度机器学习中的mini-batch的大小对学习效果有何影响？mini-batch太小会导致收敛变慢，太大容易陷入sharp minima，泛化性不好。 33、线性回归对于数据的假设是怎样的？http://en.wikipedia.org/wiki/Linear_regression（1）线性，y是多个自变量x之间的线性组合；（2）同方差性，不同的因变量x的方差都是相同的；（3）弱外生性，假设用来预测的自变量x是没有测量误差的；（4）预测变量之中没有多重共线性。 34、什么是共线性，跟过拟合有啥关联?共线性：多变量线性回归中，变量之间由于存在高度相关关系而使回归估计不准确。结果：共线性会造成冗余，导致过拟合。解决方法：排除变量的相关性／加入权重正则。 35、Bias和Variance的区别？Bias度量了学习算法的期望预测与真实结果的偏离程度，即刻画了算法本身的拟合能力。Variance度量了同样大小的训练集的变动所导致的学习性能变化，即刻画了数据扰动所造成的影响。 36、对于支持向量机，高斯核一般比线性核有更好的精度，但实际应用中为什么一般用线性核而不用高斯核？如果训练样本的量很大，训练得到的模型中支持向量的数量太多，在每次做预测时，高斯核需要计算待预测样本与每个支持向量的内积，然后做核函数变换，这会非常耗时；而线性核只需计算 W^T * X + b。 37、高斯混合模型中，为什么各个高斯分量的权重之和要保证为1？为了保证这个函数是一个概率密度函数，即积分值为1。 38、介绍beam search算法的原理。这是一种解码算法，每次选择概率最大的几个解作为候选解，逐步扩展。 39、介绍seq2seq的原理。整个系统由两个RNN组成，一个充当编码器，一个充当解码器；编码器依次接收输入的序列数据，当最后一个数据点输入之后，将循环层的状态向量作为语义向量，与解码器网络的输入向量一起，送入解码器中进行预测。 40、介绍CTC的原理。CTC通过引入空白符号，以及消除连续的相同符号，将RNN原始的输出序列映射为最终的目标序列。可以解决对未对齐的序列数据进行预测的问题，如语音识别。 41、介绍广义加法模型的原理。广义加法模型用多个基函数的和来拟合目标函数，训练的时候，依次确定每个基函数。 42、为什么很多时候用正态分布来对随机变量建模？现实世界中很多变量都服从或近似服从正态分布。中心极限定理指出，抽样得到的多个独立同分布的随机变量样本，当样本数趋向于正无穷时，它们的和服从正态分布。]]></content>
      <categories>
        <category>机器学习与深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CVPR2019 | 旷视科技提出PSENet文本检测算法]]></title>
    <url>%2F%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%2FCVPR2019-%E6%97%B7%E8%A7%86%E7%A7%91%E6%8A%80%E6%8F%90%E5%87%BAPSENet%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Shape Robust Text Detection with Progressive Scale Expansion NetworkKeyWords Plus: CVPR2019 Curved Text Face++paper：https://arxiv.org/abs/1903.12473reference: Wang W, Xie E, Li X, et al. Shape Robust Text Detection with Progressive Scale Expansion Network[J]. arXiv preprint arXiv:1903.12473, 2019.Github(tensorflow): https://github.com/whai362/PSENetGithub(pytorch): https://github.com/WenmuZhou/PSENet.pytorch Introduction PSENet 分好几个版本，最新的一个是19年的CVPR，这是一篇南京大学和face++合作的文章，19年出现了很多不规则文本检测算法，TextMountain、Textfield等等。 1、论文创新点 Propose a novel kernel-based framework, namely, Progressive Scale Expansion Network (PSENet) Adopt a progressive scale expansion algorithm based on Breadth-First-Search (BFS):1) Starting from the kernels with minimal scales (instances can be distinguished in this step).2) Expanding their areas by involving more pixels in larger kernels gradually.3) Finishing until the complete text instances (the largest kernels) are explored. 这个文章主要做的创新点大概就是预测多个分割结果，分别是S1,S2,S3…Sn代表不同的等级面积的结果，S1最小，基本就是文本骨架，Sn最大，就是完整的文本实例。然后在后处理的过程中，先用最小的预测结果去区分文本，再逐步扩张成正常文本大小。 2、算法主体 We firstly get four 256 channels feature maps(i.e. P2, P3, P4, P5)from the backbone. To further combine the semantic features from low to high levels, we fuse the four feature maps to get feature map F with 1024 channels via the function C(·) as: 先backbone下采样得到四层的feature maps，再通过FPN对四层feature分别进行上采样2,4,8倍进行融合得到输出结果。 如上图所示，网络有三个分割结果，分别是S1,S2,S3.首先利用最小的kernel生成的S1来区分四个文本实例，然后再逐步扩张成S2和S3。 3、label generation 产生不同尺寸的S1….Sn需要不同尺寸的labels。 不同尺寸的labels生成如上图所示，缩放比例可以用下面公式计算得出： 这个di表示的是缩小后mask边缘与正常mask边缘的距离，缩放比例rate ri可以由下面计算得出： m是最小mask的比例，n在m到1之间的值，成线性增加。 4、Loss Function Loss 主要分为分类的text instance loss和shrunk losses，L是平衡这两个loss的参数。分类loss主要用了交叉熵和dice loss。 The dice coefficient D(Si, Gi) 被计算如下： Ls被计算如下： 5、DatasetsTotalText A newly-released dataset for curve text detection. Horizontal, multi-Oriented and curve text instances are contained in Total-Text. The benchmark consists of 1255 training images and 300 testing images. CTW1500 CTW1500 dataset mainly consisting of long curved text. It consists of 1000 training images and 500 test images. Text instances are labelled by a polygon with 14 points which can describe the shape of an arbitrarily curve text. ICDAR 2015 Icdar2015 is a commonly used dataset for text detection. It contains a total of 1500 pictures, 1000 of which are used for training and the remaining are for testing. The text regions are annotated by 4 vertices of the quadrangle. ICDAR 2017 MLT ICDAR 2017 MIL is a large scale multi-lingual text dataset, which includes 7200 training images, 1800 validation images and 9000 testing images. The dataset is composed of complete scene images which come from 9 languages. 6、Experiment ResultsImplementation Details All the networks are optimized by using stochastic gradient descent (SGD).The data augmentation for training data is listed as follows:1) The images are rescaled with ratio {0.5, 1.0, 2.0, 3.0} randomly;2) The images are horizontally flipped and rotated in the range [−10◦, 10◦] randomly;3) 640 × 640 random samples are cropped from the transformed images. 7、Conclusion and Future work 这个文章其实做的只是一件事情，就是用预测得到的小的mask区分文本，然后逐渐扩张形成正常大小的文本mask。]]></content>
      <categories>
        <category>文本检测</category>
      </categories>
      <tags>
        <tag>文本检测</tag>
        <tag>CVPR2019</tag>
        <tag>曲线文本</tag>
        <tag>PSENet</tag>
        <tag>旷视</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习与深度学习常见问题总结（上）]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%8A%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1、比较Boosting和Bagging的异同。二者都是集成学习算法，都是将多个弱学习器组合成强学习器的方法。Bagging：从原始数据集中每一轮有放回地抽取训练集，训练得到k个弱学习器，将这k个弱学习器以投票的方式得到最终的分类结果。Boosting：每一轮根据上一轮的分类结果动态调整每个样本在分类器中的权重，训练得到k个弱分类器，他们都有各自的权重，通过加权组合的方式得到最终的分类结果。 2、无监督学习中存在过拟合吗？存在。我们可以使用无监督学习的某些指标或人为地去评估模型性能，以此来判断是否过拟合。 3、什么是k折交叉验证？将原始数据集划分为k个子集，将其中一个子集作为验证集，其余k-1个子集作为训练集，如此训练和验证一轮称为一次交叉验证。交叉验证重复k次，每个子集都做一次验证集，得到k个模型，加权平均k个模型的结果作为评估整体模型的依据。 4、关于k折交叉验证，需要注意什么？k越大，不一定效果越好，而且越大的k会加大训练时间；在选择k时，需要考虑最小化数据集之间的方差，比如对于2分类任务，采用2折交叉验证，即将原始数据集对半分，若此时训练集中都是A类别，验证集中都是B类别，则交叉验证效果会非常差。 5、对于一个二分类问题，我们定义超过阈值t的判定为正例，否则判定为负例。现在若将t增大，则准确率和召回率会如何变化？准确率 = TP / (TP + FP)，召回率 = TP / (TP + FN)，其中TP表示将正例正确分类为正例的数量，FP表示将负例错误分类为正例的数量，FN表示将正例错误分类为负例的数量。准确率可以理解为在所有分类为正例的样品中，分类正确的样本所占比例；召回率可以理解为在所有原始数据集中的正例样品中，正确挑出的正例样本的比例。因此若增大阈值t，更多不确定（分类概率较小）的样本将会被分为负例，剩余确定（分类概率较大）的样本所占比例将会增大（或不变），即正确率会增大（或不变）；若增大阈值t，则可能将部分不确定（分类概率较小）的正例样品误分类为负例，即召回率会减小（或不变）。 6、以下关于神经网络的说法中，正确的是（ ）？A.增加网络层数，总能减小训练集错误率B.减小网络层数，总能减小测试集错误率C.增加网络层数，可能增加测试集错误率答案：C。增加神经网络层数，确实可能提高模型的泛化性能，但不能绝对地说更深的网络能带来更小的错误率，还是要根据实际应用来判断，比如会导致过拟合等问题，因此只能选C。 7、说明Lp范数间的区别。L1范数：向量中各个元素绝对值之和L2范数：向量中各个元素平方和的开二次方根Lp范数：向量中各个元素绝对值的p次方和的开p次方根 8、用梯度下降训练神经网络的参数，为什么参数有时会被训练为nan值？输入数据本身存在nan值，或者梯度爆炸了（可以降低学习率、或者设置梯度的阈值）。 9、卷积神经网络CNN中池化层有什么作用？减小图像尺寸即数据降维，缓解过拟合，保持一定程度的旋转和平移不变性。 10、请列举几种常见的激活函数。激活函数有什么作用？sigmoid， relu，tanh。非线性化 11、神经网络中Dropout的作用？具体是怎么实现的？防止过拟合。每次训练，都对每个神经网络单元，按一定概率临时丢弃。 12、利用梯度下降法训练神经网络，发现模型loss不变，可能有哪些问题？怎么解决？很有可能是梯度消失了，它表示神经网络迭代更新时，有些权值不更新的现象。改变激活函数，改变权值的初始化等。 13、如何解决不平衡数据集的分类问题？可以扩充数据集，对数据重新采样，改变评价指标等。 14、残差网络为什么能做到很深层？神经网络在反向传播过程中要不断地传播梯度，而当网络层数加深时，梯度在逐层传播过程中会逐渐衰减，导致无法对前面网络层的权重进行有效的调整。残差网络中， 加入了 short connections 为梯度带来了一个直接向前面层的传播通道，缓解了梯度的减小问题。 15、相比sigmoid激活函数ReLU激活函数有什么优势？（1） 防止梯度消失 （sigmoid的导数只有在0附近的时候有比较好的激活性，在正负饱和区的梯度都接近于0）（2） ReLU的输出具有稀疏性（3） ReLU函数简单计算速度快 16、卷积神经网络中空洞卷积的作用是什么？空洞卷积也叫扩张卷积，在保持参数个数不变的情况下增大了卷积核的感受野，同时它可以保证输出的特征映射（feature map）的大小保持不变。一个扩张率为2的3×3卷积核，感受野与5×5的卷积核相同，但参数数量仅为9个。 17、解释下卷积神经网络中感受野的概念？在卷积神经网络中，感受野 (receptive field)的定义是：卷积神经网络每一层输出的特征图（feature map）上的像素点在原始图像上映射的区域大小。 18、模型欠拟合什么情况下会出现？有什么解决方案？模型复杂度过低，不能很好的拟合所有的数据。增加模型复杂度，如采用高阶模型（预测）或者引入更多特征（分类）等。 19、适用于移动端部署的网络结构都有哪些？MobilenetShufflenetXception 20、卷积神经网络中im2col是如何实现的？使用im2col的方法将划窗卷积转为两个大的矩阵相乘，见下图： 21、多任务学习中标签缺失如何处理？一般做法是将缺失的标签设置特殊标志，在计算梯度的时候忽略。 22、梯度爆炸的解决方法？针对梯度爆炸问题，解决方案是引入Gradient Clipping(梯度裁剪)。通过Gradient Clipping，将梯度约束在一个范围内，这样不会使得梯度过大。 23、深度学习模型参数初始化都有哪些方法？（1）Gaussian 满足mean=0，std=1的高斯分布x∼N(mean，std^2)（2）Xavier 满足x∼U(−a,+a)的均匀分布，其中 a = sqrt(3/n)（3）MSRA 满足x∼N(0,σ^2)的高斯分布，其中 σ = sqrt(2/n)（4）Uniform 满足min=0,max=1的均匀分布。x∼U(min, max) 等等 24、注意力机制在深度学习中的作用是什么？有哪些场景会使用？深度学习中的注意力机制从本质上讲和人类的选择性视觉注意力机制类似，核心目标是从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息。目前在神经机器翻译(Neural Machine Translation)、图像理解(Image caption)等场景都有广泛应用。 25、卷积神经网络为什么会具有平移等不变性？MaxPooling能保证卷积神经网络在一定范围内平移特征能得到同样的激励，具有平移不变性。 26、神经网络参数共享(parameter sharing)是指什么？所谓的权值共享就是说，用一个卷积核去卷积一张图，这张图每个位置是被同样数值的卷积核操作的，权重是一样的，也就是参数共享。 27、如何提高小型网络的精度？（1）模型蒸馏技术（2）利用AutoML进行网络结构的优化，可将网络计算复杂度作为约束条件之一，得到更优的结构。 28、什么是神经网络的梯度消失问题，为什么会有梯度消失问题？有什么办法能缓解梯度消失问题？在反向传播算法计算每一层的误差项的时候，需要乘以本层激活函数的导数值，如果导数值接近于0，则多次乘积之后误差项会趋向于0，而参数的梯度值通过误差项计算，这会导致参数的梯度值接近于0，无法用梯度下降法来有效的更新参数的值。改进激活函数，选用更不容易饱和的函数，如ReLU函数。 29、列举你所知道的神经网络中使用的损失函数。欧氏距离，交叉熵，对比损失，合页损失。 30、对于多分类问题，为什么神经网络一般使用交叉熵而不用欧氏距离损失？交叉熵在一般情况下更容易收敛到一个更好的解。 31、1x1卷积有什么用途？通道降维，保证卷积神经网络可以接受任何尺寸的输入数据。 32、随机梯度下降法，在每次迭代时能保证目标函数值一定下降吗？为什么？不能，每次迭代时目标函数不一样。 33、梯度下降法，为什么需要设置一个学习率？使得迭代之后的值在上次值的邻域内，保证可以忽略泰勒展开中的二次及二次以上的项。 34、解释梯度下降法中动量项的作用。利用之前迭代时的梯度值，减小震荡。 35、为什么现在倾向于用小尺寸的卷积核？用多个小卷积核串联可以有大卷积核同样的能力，而且参数更少，另外有更多次的激活函数作用，增强非线性。 36、解释GoogleNet的Inception模块的原理。对输入图像用多个不同尺寸的卷积核、池化操作进行同时处理，然后将输出结果按照通道拼接起来。 37、解释反卷积的原理和用途。反卷积即转置卷积，正向传播时乘以卷积核的转置矩阵，反向传播时乘以卷积核矩阵。由卷积输出结果近似重构输入数据，上采样。 38、解释批量归一化的原理。在数据送入神经网络的某一层进行处理之前，对数据做归一化。按照训练样本的批量进行处理，先减掉这批样本的均值，然后除以标准差，然后进行缩放和平移。缩放和平移参数同训练得到。预测时使用训练时确定的这些值来计算。 39、解释SVM核函数的原理。核函数将数据映射到更高维的空间后处理，但不用做这种显式映射，而是先对两个样本向量做内积，然后用核函数映射。这等价于先进行映射，然后再做内积。 40、什么是过拟合，过拟合产生的原因是什么？有什么方法能减轻过拟合？过拟合指在训练集上表现的很好，但在测试集上表现很差，推广泛化能力差。原因：训练样本的抽样误差，训练时拟合了这种误差。方法：增加训练样本，尤其是样本的代表性；正则化。 41、什么样的函数可以用作激活函数？非线性，几乎处处可到，单调。 42、什么是鞍点问题？梯度为0，Hessian矩阵不定的点，不是极值点。 43、在训练深度神经网络的过程中，遇到过哪些问题，怎么解决的？不收敛，收敛太慢，泛化能力差。调整网络结构，调整样本，调整学习率，调整参数初始化策略。 44、SVM如何解决多分类问题？多个二分类器组合。1对1方案，1对剩余方案，多类损失函数。 45、列举你知道的聚类算法。层次聚类，k均值算法，DBSCAN算法，OPTICS算法，谱聚类。 46、K均值算法中，初始类中心怎么确定？随机选择K个样本作为类中心；将样本随机划分成K个子集然后计算类中心。 47、简述EM算法的原理。EM算法用于求解带有隐变量的最大似然估计问题。由于有隐变量的存在，无法直接用最大似然估计求得对数似然函数极大值的公式解。此时通过jensen不等式构造对数似然函数的下界函数，然后优化下界函数，再用估计出的参数值构造新的下界函数，反复迭代直至收敛到局部极小值点。]]></content>
      <categories>
        <category>机器学习与深度学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令大全]]></title>
    <url>%2FLinux%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[查看当前目录下的文件数量（不包含子目录中的文件） 1ls -l|grep "^-"| wc -l 查看当前目录下的文件数量（包含子目录中的文件） 注意：R，代表子目录 1ls -lR|grep "^-"| wc -l 查看当前目录下的文件夹目录个数（不包含子目录中的目录），如果需要查看子目录，加上R 1ls -l|grep "^d"| wc -l 查询当前路径下的指定前缀名的目录下的文件数量，如统计所有以”2018”开头的目录下的全部文件数量 1ls -lR 2018\*/|grep "^-"| wc -l]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贪心算法-任务调度问题]]></title>
    <url>%2F%E7%AE%97%E6%B3%95%E9%A2%98%2F%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95-%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Problem E.任务调度问题时间限制 1000 ms内存限制 128 MB 题目描述一个单位时间任务是恰好需要一个单位时间完成的任务。给定一个单位时间任务的有限集 S 。关于S 的一个时间表用于描述S 中单位时间任务的执行次序。时间表中第 1 个任务从时间 0 开始执行直至时间 1 结束，第 2 个任务从时间 1 开始执行至时间 2 结束，…，第n个任务从时间 n-1 开始执行直至时间 n 结束。具有截止时间和误时惩罚的单位时间任务时间表问题可描述如下：(1) n 个单位时间任务的集合 S = {1,2,…,n}（n ≤ 500）；(2) 任务i的截止时间 d[i], 1 ≤ i ≤ n, 1 ≤ d[i] ≤ n，即要求任务 i 在时间 d[i] 之前结束；(3) 任务 i 的误时惩罚 1 ≤ w[i] ≤ 1000, 1 ≤ i ≤ n, 即任务 i 未在时间 d[i] 之前结束将招致 w[i] 的惩罚；若按时完成则无惩罚。任务时间表问题要求确定 S 的一个时间表（最优时间表）使得总误时惩罚达到最小。 输入数据第一行是正整数 n ，表示任务数。接下来的 2 行中，每行有 n 个正整数，分别表示各任务的截止时间和误时惩罚。 输出数据将计算出的最小总误时惩罚输出。 样例输入12374 2 4 3 1 4 670 60 50 40 30 20 10 样例输出150 题解这道题目类似于活动安排问题。首先将所有任务按照误时惩罚从大到小排序，然后依次从左到右遍历每个任务，将其安排在离截止时间点最近的（包括截止时间点处）未安排任务的时间点处完成，若无法找到这个时间点，则这个任务无法按时完成，将其误时惩罚加到总误时惩罚中。这样得到的总误时惩罚是最小的。 AC代码123456789101112131415161718192021222324252627282930313233343536373839404142434445#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;cmath&gt;using namespace std;int n, d[510], w[510], visit[510];int i, j;long long result;int main() &#123; memset(visit, 0, sizeof(visit)); cin &gt;&gt; n; for (i = 1; i &lt;= n; i++) &#123; cin &gt;&gt; d[i]; &#125; for (i = 1; i &lt;= n; i++) &#123; cin &gt;&gt; w[i]; &#125; for (i = 0; i &lt; n; i++) &#123; for (j = 1; j &lt; n; j++) &#123; if (w[j] &lt; w[j + 1]) &#123; swap(d[j], d[j + 1]); swap(w[j], w[j + 1]); &#125; &#125; &#125; for (i = 1; i &lt;= n; i++) &#123; bool flag = false; for (j = d[i]; j &gt; 0; j--) &#123; if (!visit[j]) &#123; visit[j] = 1; flag = true; break; &#125; &#125; if (!flag) &#123; result += w[i]; &#125; &#125; cout &lt;&lt; result &lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>算法题</category>
      </categories>
      <tags>
        <tag>贪心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贪心算法-合并果子]]></title>
    <url>%2F%E7%AE%97%E6%B3%95%E9%A2%98%2F%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95-%E5%90%88%E5%B9%B6%E6%9E%9C%E5%AD%90%2F</url>
    <content type="text"><![CDATA[Problem B.合并果子时间限制 1000 ms内存限制 128 MB 题目描述在一个果园里，多多已经将所有的果子打了下来，而且按果子的不同种类分成了不同的堆。多多决定把所有的果子合成一堆。每一次合并，多多可以把两堆果子合并到一起，消耗的体力等于两堆果子的重量之和。可以看出，所有的果子经过n-1次合并之后，就只剩下一堆了。多多在合并果子时总共消耗的体力等于每次合并所耗体力之和。因为还要花大力气把这些果子搬回家，所以多多在合并果子时要尽可能地节省体力。假定每个果子重量都为1，并且已知果子的种类数和每种果子的数目，你的任务是设计出合并的次序方案，使多多耗费的体力最少，并输出这个最小的体力耗费值。例如有3种果子，数目依次为1，2，9。可以先将1、2堆合并，新堆数目为3，耗费体力为3。接着，将新堆与原先的第三堆合并，又得到新的堆，数目为12，耗费体力为12。所以多多总共耗费体力=3+12=15。可以证明15为最小的体力耗费值。 输入数据输入包括两行，第一行是一个整数n(1 &lt;＝ n &lt; 10^4)，表示果子的种类数。第二行包含 n 个整数，用空格分隔，第 i 个整数ai(1 &lt;＝ ai &lt; 2 * 10^4)是第 i 种果子的数目。 输出数据输出包括一行，这一行只包含一个整数，也就是最小的体力耗费值。输入数据保证这个值小于 2^31 。 样例输入1231 2 9 样例输出115 题解这道题类似于哈夫曼编码树。首先用一个优先队列存储每种果子的数目，定义比较函数为从大到小排序。然后从队列中取出数目最少的两种果子，合并到一起，并将合并后的结果重新放入优先队列中，同时体力耗费增加相应的值。依此类推，直到将所有种类的果子合并成一堆。这样所得到的体力耗费值是最小的。 AC代码123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;queue&gt;using namespace std;long long int n, a, temp, result;priority_queue&lt;long long int, vector&lt;long long int&gt;, greater&lt;long long int&gt;&gt; fruits;int i;int main() &#123; cin &gt;&gt; n; for (i = 0; i &lt; n; i++) &#123; cin &gt;&gt; a; fruits.push(a); &#125; while (fruits.size() &gt; 1) &#123; temp = 0; for (i = 0; i &lt; 2; i++) &#123; temp += fruits.top(); fruits.pop(); &#125; fruits.push(temp); result += temp; &#125; cout &lt;&lt; result &lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>算法题</category>
      </categories>
      <tags>
        <tag>贪心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最大稳定极值区域MSER-Maximally Stable Extrernal Regions]]></title>
    <url>%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%AE%9A%E4%BD%8D%2F%E6%9C%80%E5%A4%A7%E7%A8%B3%E5%AE%9A%E6%9E%81%E5%80%BC%E5%8C%BA%E5%9F%9FMSER-Maximally-Stable-Extrernal-Regions%2F</url>
    <content type="text"><![CDATA[基本原理MSER基于分水岭的概念：对图像进行二值化，二值化阈值取[0, 255]，这样二值化图像就经历一个从全黑到全白的过程（就像水位不断上升的俯瞰图）。在这个过程中，有些连通区域面积随阈值上升的变化很小，这种区域就叫MSER。其中Qi表示第i个连通区域的面积，Δ表示微小的阈值变化（注水），当vi小于给定阈值时认为该区域为MSER。显然，这样检测得到的MSER内部灰度值是小于边界的，想象一副黑色背景白色区域的图片，显然这个区域是检测不到的。因此对原图进行一次MSER检测后需要将其反转，再做一次MSER检测，两次操作又称MSER+和MSER-。 Python源码实现1234567891011121314import cv2import matplotlib.pyplot as pltim = cv2.imread('./source.jpg')gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)mser = cv2.MSER_create(_min_area=300)regions, boxes = mser.detectRegions(gray)for box in boxes: x, y, w, h = box cv2.rectangle(im, (x, y),(x + w, y + h), (255, 0, 0), 2)cv2.imwrite("./mser.jpg", im)]]></content>
      <categories>
        <category>目标检测与定位</category>
      </categories>
      <tags>
        <tag>MSER</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贪心算法_最小差距]]></title>
    <url>%2F%E7%AE%97%E6%B3%95%E9%A2%98%2F%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95-%E6%9C%80%E5%B0%8F%E5%B7%AE%E8%B7%9D%2F</url>
    <content type="text"><![CDATA[Problem A. 最小差距时间限制 1000 ms内存限制 128 MB 题目描述给定一些不同的一位数字，你可以从这些数字中选择若干个，并将它们按一定顺序排列，组成一个整数，把剩下的数字按一定顺序排列，组成另一个整数。组成的整数不能以0开头（除非这个整数只有1位）。例如，给定6个数字，0,1,2,4,6,7，你可以用它们组成一对数10和2467，当然，还可以组成其他的很多对数，比如210和764，204和176。这些对数中两个数差的绝对值最小的是204和176，为28。给定N个不同的0-9之间的数字，请你求出用这些数字组成的每对数中，差的绝对值最小的一对（或多对）数的绝对值是多少？ 输入数据第一行包括一个数 T （T ≤ 1000），为测试数据的组数。每组数据包括两行，第一行为一个数 N （2 ≤ N ≤ 10），表示数字的个数。下面一行为 N 个不同的一位数字。 输出数据T 行，每行一个数，表示第 i 个数据的答案。即最小的差的绝对值。 样例输入12345260 1 2 4 6 741 6 3 4 样例输出12285 题解首先将所有数字从小到大排序，接着这道题可以分三种情况考虑：（1）只有两个数的情况：直接两数相减取绝对值即可。（2）奇数个数的情况：首先保证第一个数字非0，如果为0，就将第一个数与第二个数交换位置，然后从左往右连续取n/2+1个数组成第一个整数，最后从右往左连续取n/2个数，即剩下的所有数组成另一个整数。这样所求得的一对整数的差的绝对值最小。（3）偶数个数的情况：从左往右依次枚举，首先保证第一个数非0，如果为0，就从下一个数开始枚举；取第k个数作为第一个整数的第一个数字，取第k+1个数作为第二个整数的第一个数字；然后，对于剩下的数字，从右往左连续取n/2-1个数加入第一个整数中，从左往右连续取n/2-1个数加入第二个整数中；最后，取差的绝对值最小的一对整数。 AC代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;cmath&gt;using namespace std;int t, n, d[20], num1, num2;int i, j, k, l, r;int main() &#123; scanf("%d", &amp;t); for (i = 0; i &lt; t; i++) &#123; scanf("%d", &amp;n); for (j = 1; j &lt;= n; j++) &#123; scanf("%d", &amp;d[j]); &#125; sort(d + 1, d + n + 1); if (n == 2) &#123; // 两个数的情况 printf("%d\n", abs(d[2] - d[1])); &#125; else if (n % 2 == 1) &#123; // 奇数个数的情况 if (d[1] == 0) &#123; swap(d[1], d[2]); &#125; num1 = 0, num2 = 0; // 前n/2+1个数从左往右组成第一个整数 for (j = 1; j &lt;= n / 2 + 1; j++) &#123; num1 = num1 * 10 + d[j]; &#125; // 后n/2个数从右往左组成第二个整数 for (j = n; j &gt;= n / 2 + 2; j--) &#123; num2 = num2 * 10 + d[j]; &#125; printf("%d\n", abs(num1 - num2)); &#125; else &#123; // 偶数个数的情况 int result = 0x7fffffff; // 枚举，取第k个数加入第一个整数中，取第k+1个数加入第二个整数中 for (j = 1; j &lt; n; j++) &#123; if (d[j] == 0) &#123; continue; &#125; num1 = d[j], num2 = d[j + 1]; // 从右往左遍历n/2-1个数加入第一个整数中 // 从左往右遍历n/2-1个数加入第二个整数中 l = 1, r = n; for (k = 0; k &lt; n / 2 - 1; k++) &#123; if (l == j) l++; if (l == j + 1) l++; if (r == j + 1) r--; if (r == j) r--; num1 = num1 * 10 + d[r]; num2 = num2 * 10 + d[l]; l++, r--; &#125; // 取差最小的值 result = min(result, abs(num1 - num2)); &#125; printf("%d\n", result); &#125; &#125; return 0;&#125;]]></content>
      <categories>
        <category>算法题</category>
      </categories>
      <tags>
        <tag>贪心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贪心算法_旅行]]></title>
    <url>%2F%E7%AE%97%E6%B3%95%E9%A2%98%2F%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95-%E6%97%85%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[Problem D. 旅行时间限制 1000 ms内存限制 128 MB 题目描述某趟列车的最大载客容量为V人，沿途共有n个停靠站，其中始发站为第1站，终点站为第n站。在第1站至第n-1站之间，共有m个团队申请购票搭乘，若规定：（1）对于某个团队的购票申请，要么全部满足，要么全部拒绝，即不允许只满足部分。（2）每个乘客的搭乘费用为其所乘站数。问：应如何选择这些购票申请，能使该趟列车获得最大的搭乘费用？其中，每个团队的购票申请格式是以空格分隔的三个整数：a b t，即表示有t个人需要从第a站点乘至第b站点（注：每个团队的所有人员都必须同时在a站上车，且必须同时在后面的b站下车）。 输入数据输入数据有若干行。第 1 行只有三个整数 n，m，v，分别表示站点数、申请数、列车的最大载客容量。这三个整数之间都以一个空格分隔。第 2 行至第 m+1 行，每行有三个整数，中间都以一个空格分隔。其中第 k+1 行的三个整数 a，b，t 表示第 k 个申请，含义为：有 t 个人需要从第 a 站乘至第 b 站。其中：1 ≤ n ≤ 10；1 ≤ m ≤ 18 输出数据输出数据只有一行，该行只有一个整数，为该列车能获得的最大搭乘费用。 样例输入12343 3 51 2 22 3 51 3 4 样例输出18 题解这道题与0-1背包问题类似。直接枚举所有可能的方案数，对于每一个团队，只有两个选择，搭乘与不搭乘，可以采用二进制0与1枚举的方式。由于团队申请数目m最多只有18个，所以最多有2^18种方案。对于每一种方案，统计每一站车上的总人数，如果某一站车上总人数超过了列车的最大载客容量v，那么此方案不可行；否则，比较搭乘费用，取最大的搭乘费用即可。 AC代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;cmath&gt;using namespace std;int n, m, v, a[20], b[20], t[20], num[25];long long cost = 0, temp;bool flag;int i, j;int main() &#123; cin &gt;&gt; n &gt;&gt; m &gt;&gt; v; for (i = 0; i &lt; m; i++) &#123; cin &gt;&gt; a[i] &gt;&gt; b[i] &gt;&gt; t[i]; &#125; // 枚举二进制方案数 for (i = 0; i &lt; (1 &lt;&lt; m); i++) &#123; // 变量的初始化 temp = 0; flag = true; memset(num, 0, sizeof(num)); // 判断决定每一个团队是否可以搭乘 for (j = 0; j &lt; m; j++) &#123; if (i &amp; (1 &lt;&lt; j)) &#123; num[a[j]] += t[j]; num[b[j]] -= t[j]; temp += (b[j] - a[j]) * t[j]; &#125; &#125; // 统计每一站车上的总人数，如果大于容量，就不满足条件 for (j = 1; j &lt;= n; j++) &#123; num[j] += num[j - 1]; if (num[j] &gt; v) &#123; flag = false; &#125; &#125; // 满足条件则取最大搭乘费用 if (flag) &#123; cost = max(cost, temp); &#125; &#125; cout &lt;&lt; cost &lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>算法题</category>
      </categories>
      <tags>
        <tag>贪心</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[非极大值抑制NMS(Non-Maximum Suppression)]]></title>
    <url>%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%AE%9A%E4%BD%8D%2F%E9%9D%9E%E6%9E%81%E5%A4%A7%E5%80%BC%E6%8A%91%E5%88%B6NMS-Non-Maximum-Suppression%2F</url>
    <content type="text"><![CDATA[介绍NMS顾名思义就是抑制不是极大值的元素，可以理解为局部最大搜索。这个局部代表的是一个邻域，邻域有两个参数可变，一是邻域的维数，二是邻域的大小。在目标检测中NMS主要用于提取分数最高的候选框。例如在用训练好的模型进行测试时，网络会预测出一系列的候选框，这时候可以用NMS来移除一些多余的候选框，即移除一些IOU值大于某个阈值的框。再比如在行人检测中，滑动窗口经提取特征，经分类器分类识别后，每个窗口都会得到一个分数。但是滑动窗口会导致很多窗口与其他窗口存在包含或者大部分交叉的情况。这时就需要用NMS来选取那些邻域里分数最高（是行人的概率最大）的窗口，并且抑制那些分数低的窗口。NMS在计算机视觉领域有着非常重要的应用，如视频目标跟踪、数据挖掘、3D重建、目标识别以及纹理分析等等。 NMS在目标检测中的应用去除重叠人脸检测框的例子如上图所示，目的就是要去除冗余的检测框，找到最佳的候选框。 目标检测pipline中的例子如上图所示，产生proposal后使用分类网络给出每个框的每类置信度，使用回归网络修正位置，最后使用NMS去除冗余的检测框。 NMS算法原理对于bounding boxes列表B及其相应的置信度S，使用下述计算方式：选择具有最大score的检测框M，将其从B集合中移除并加入到最后的检测结果R中。通常将B中剩余检测框与M的IOU大于阈值threshold的框从B中移除。重复这个过程，直到B为空。其中用到的排序，可以按照检测框右下角的坐标或者面积排序，也可以通过SVM等分类器得到的得分或概率进行排序，R-CNN中就是按照得分进行的排序。比如上图，定位一个车辆的位置，算法找出了许多检测框，这时需要判断哪些矩形框是没用的。非极大值抑制的方法是：先假设有6个矩形框，按照候选框的类别分类概率排序，假设属于车辆的概率排序结果为A &lt; B &lt; C &lt; D &lt; E &lt; F。 先从最大概率矩形框F开始，分别判断A~E与F的IOU值是否大于某个设定的阈值； 假设B、D与F的重叠度超过设定的阈值，那么就移除B和D；并标记第一个矩形框F，是保留下来的。 从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，如果重叠度大于设定的阈值，那么就扔掉；并标记E是保留下来的第二个矩形框。 就这样一直重复，直到找到所有被保留下来的矩形框。 Python源码实现在R-CNN中使用了NMS来确定最终的bbox，其对每个候选框送入分类器，根据分类器的类别分类概率做排序(论文中称为greedy-NMS)。但其实也可以在分类之前运用简单版本的NMS来去除一些框。python实现的单类别nms：12345678910111213141516171819202122232425262728293031323334353637def py_cpu_nms(dets, thresh): """ Pure Python NMS baseline. """ # x1、y1、x2、y2、以及score赋值 x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] scores = dets[:, 4] # 每一个候选框的面积 areas = (x2 - x1 + 1) * (y2 - y1 + 1) # 按照score置信度降序排序 order = scores.argsort()[::-1] keep = [] # 保留的结果框集合 while order.size &gt; 0: i = order[0] keep.append(i) # 保留该类剩余box中得分最高的一个 # 计算当前概率最大矩形框与其他矩形框的相交框的坐标 xx1 = np.maximum(x1[i], x1[order[1:]]) yy1 = np.maximum(y1[i], y1[order[1:]]) xx2 = np.minimum(x2[i], x2[order[1:]]) yy2 = np.minimum(y2[i], y2[order[1:]]) # 计算相交的面积，不重叠时面积为0 w = np.maximum(0.0, xx2 - xx1 + 1) h = np.maximum(0.0, yy2 - yy1 + 1) inter = w * h #计算IoU：重叠面积 /（面积1+面积2-重叠面积） over = inter / (areas[i] + areas[order[1:]] - inter) # 保留IoU小于阈值的矩形框索引 inds = np.where(over &lt;= thresh)[0] # 将order序列更新，由于前面得到的矩形框索引要比矩形框在原order序列中的索引小1，所以要把这个1加回来 order = order[inds + 1] return keep # 获取保留下来的索引 Faster R-CNN的MATLAB实现与python版实现一致，代码在这里:nms.m。另外，nms_multiclass.m是多类别nms，加了一层for循环对每类进行nms。 NMS loss对多类别检测任务，如果对每类分别进行NMS，那么当检测结果中包含两个被分到不同类别的目标且其IoU较大时，会得到不可接受的结果。如下图所示：一种改进方式是在损失函数中加入一部分NMS损失。NMS损失可以定义为与分类损失相同：即真实类别u对应的log损失，p是C个类别的预测概率，相当于增加分类误差。参考论文《Rotated Region Based CNN for Ship Detection》（IEEE2017会议论文）的Multi-task for NMS部分。 Soft-NMS上述NMS算法的一个主要问题是当两个ground truth的目标的确重叠度很高时，NMS会将具有较低置信度的框去掉(置信度改成0)，如下图所示：论文:《Improving Object Detection With One Line of Code》，改进之处：改进方法在于将置信度改为IoU的函数：f(IoU)，具有较低的值而不至于从排序列表中删去。（1）线性函数函数值不连续，在某一点的值发生跳跃。（2）高斯函数时间复杂度同传统的greedy-NMS，为O(N^2)。 Soft-NMS pythonPython源码实现12345678910111213141516ua = float((x2 - x1 + 1) * (y2 - y1 + 1) + area - w * h)ov = w * h / ua # iou between max box and detection boxif method == 1: # linear if ov &gt; Nt: weight = 1 - ov else: weight = 1elif method == 2: # gaussian weight = np.exp(-(ov * ov) / sigma)else: # original NMS if ov &gt; Nt: weight = 0 else: weight = 1# re-scoring 修改置信度boxes[pos, 4] = weight * boxes[pos, 4] 在基于proposal方法的模型结果上应用比较好，检测效果提升：在R-FCN以及Faster-RCNN模型中的测试阶段运用Soft-NMS，在MS-COCO数据集上mAP@[0.5:0.95]能够获得大约1%的提升详见这里。 如果应用到训练阶段的proposal选取过程理论上也能获得提升。对易重叠的目标类型确实有提高(目标不一定真的有像素上的重叠，切斜的目标的矩形边框会有较大的重叠)。而在SSD，YOLO等非proposal方法中没有提升。 其它应用 边缘检测：** Canny算子中的非极大值抑制是沿着梯度方向进行的，即是否为梯度方向上的极值点；参考这里 特征点检测：** 在角点检测等场景下说的非极大值抑制，则是检测中心点处的值是否是某一个邻域内的最大值。]]></content>
      <categories>
        <category>目标检测与定位</category>
      </categories>
      <tags>
        <tag>NMS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[交并比IOU(Intersection over Union)]]></title>
    <url>%2F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%8E%E5%AE%9A%E4%BD%8D%2F%E4%BA%A4%E5%B9%B6%E6%AF%94IOU-Intersection-over-Union%2F</url>
    <content type="text"><![CDATA[基本原理IOU是在目标检测中使用的一个概念，是产生的预测框(Predicted bounding box)与标注框(Ground-truth bounding box)的重叠率；简单来说，即两个矩形框面积的交集和并集的比值；它是一个在特定数据集中检测相应物体准确度的测量标准。通常会在HOG + Linear SVM object detectors和Convolutional Neural Network detectors (R-CNN, Faster R-CNN, YOLO 等)中使用该方法检测其性能。IOU是一个简单的测量标准，在输出中得出一个预测范围(bounding box)的任务都可以用IOU来测量。其用于测量真实和预测之间的相关度，相关度越高，该值越高。上图展示了ground-truth和predicted的结果，绿色标线是人为标记的正确结果，红色标线是算法预测出来的结果，IOU要做的就是在这两个结果中测量算法的准确度。一般来说，这个比值 ＞ 0.5 就可以认为是一个不错的结果了。 Python源码实现123456789101112131415161718192021222324252627282930313233import numpy as npdef compute_iou(box1, box2, wh=False): """ compute the iou of two boxes. args: box1, box2: [xmin, ymin, xmax, ymax] (wh=False) or [xcenter, ycenter, w, h] (wh=True) wh: the format of coordinate. return: iou: iou of box1 and box2. """ if wh == False: xmin1, ymin1, xmax1, ymax1 = box1 xmin2, ymin2, xmax2, ymax2 = box2 else: xmin1, ymin1 = int(box1[0] - box1[2] / 2.0), int(box1[1] - box1[3] / 2.0) xmax1, ymax1 = int(box1[0] + box1[2] / 2.0), int(box1[1] + box1[3] / 2.0) xmin2, ymin2 = int(box2[0] - box2[2] / 2.0), int(box2[1] - box2[3] / 2.0) xmax2, ymax2 = int(box2[0] + box2[2] / 2.0), int(box2[1] + box2[3] / 2.0) ## 计算两个矩形框面积 area1 = (xmax1 - xmin1) * (ymax1 - ymin1) area2 = (xmax2 - xmin2) * (ymax2 - ymin2) ## 获取矩形框交集对应的左上角和右下角的坐标（intersection） inter_x1 = np.max([xmin1, xmin2]) inter_y1 = np.max([ymin1, ymin2]) inter_x2 = np.min([xmax1, xmax2]) inter_y2 = np.min([ymax1, ymax2]) inter_area = (np.max([0, inter_x2 - inter_x1])) * (np.max([0, inter_y2 - inter_y1])) # 计算交集面积 iou = inter_area / (area1 + area2 - inter_area + 1e-6) ＃ 计算交并比 return iou]]></content>
      <categories>
        <category>目标检测与定位</category>
      </categories>
      <tags>
        <tag>IOU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git基本常用命令]]></title>
    <url>%2F%E5%B7%A5%E5%85%B7%2FGit%E5%9F%BA%E6%9C%AC%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[git init 把当前的目录变成可以管理的git仓库，生成隐藏.git文件。 git add XX 把xx文件添加到暂存区去。 git commit -m ‘XX’ 提交文件， –m 后面的是注释。 git status 查看本地仓库状态。 git diff XX 查看XX文件修改了那些内容。 git log 查看历史记录。 git reset -hard HEAD^ 或者 git reset –hard HEAD~ 回退到上一个版本(如果想回退到100个版本，使用git reset –hard HEAD~100 )。 cat XX 查看XX文件内容。 git reflog 查看历史记录的版本号id。 git checkout – XX 把XX文件在工作区的修改全部撤销。 git rm XX 删除XX文件。 git remote add origin https://github.com/zhaopeng0103/test.git 关联一个远程库。 git push –u(第一次要用-u 以后不需要) origin master 把当前master分支推送到远程库。 git clone https://github.com/zhaopeng0103/test.git 从远程库中克隆。 git checkout –b dev 创建dev分支，并切换到dev分支上。 git branch 查看当前所有的分支。 git checkout master 切换回master分支。 git merge dev 在当前的分支上合并dev分支。 git branch –d dev 删除dev分支。 git branch name 创建分支。 git stash 把当前的工作隐藏起来 等以后恢复现场后继续工作。 git stash list 查看所有被隐藏的文件列表。 git stash apply 恢复被隐藏的文件，但是内容不删除。 git stash drop 删除文件。 git stash pop 恢复文件的同时 也删除文件。 git remote 查看远程库的信息。 git remote –v 查看远程库的详细信息。 git push origin master Git会把master分支推送到远程库对应的远程分支上。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>git使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好听的歌曲]]></title>
    <url>%2Fmusic%2F%E5%A5%BD%E5%90%AC%E7%9A%84%E6%AD%8C%E6%9B%B2%2F</url>
    <content type="text"><![CDATA[var options = {"narrow":false,"autoplay":false,"showlrc":3,"mode":"random","music":[{"title":"小白兔遇上卡布奇诺","author":"兔子牙","url":"/music/小白兔遇上卡布奇诺.mp3","pic":"/images/小白兔1.jpg","lrc":"/documents/小白兔.txt"}]}; options.element = document.getElementById("aplayer-SPaozMIi"); var ap = new APlayer(options); window.aplayers || (window.aplayers = []); window.aplayers.push(ap); (function(){var player = new DPlayer({"container":document.getElementById("dplayer0"),"theme":"#FADFA3","video":{"url":"/videos/小白兔.mp4","pic":"/images/小白兔.jpg"}});window.dplayers||(window.dplayers=[]);window.dplayers.push(player);})()]]></content>
      <categories>
        <category>music</category>
      </categories>
      <tags>
        <tag>music</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序设计竞赛:五子棋]]></title>
    <url>%2F%E7%AE%97%E6%B3%95%E9%A2%98%2F%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B-%E4%BA%94%E5%AD%90%E6%A3%8B%2F</url>
    <content type="text"><![CDATA[Problem E. 五子棋时间限制 1000 ms内存限制 64 MB 题目描述在一个nxn的棋盘上，有一些黑色的棋子和白色的棋子，如果能找出任意五个同色的棋子连成直线（横着、竖着、斜着都可以），那么该颜色方加1分。求黑色方得分和白色方得分。 输入数据第一行为一个正整数n，代表棋盘的大小。 接下来为一个nxn的矩阵，’#’代表没有棋子，’B’代表黑色棋子，’W’代表白色棋子 n&lt;=20 输出数据两个正整数，分别代表黑色方得分和白色方得分 样例输入12345676WBBBBBWBB###W###B#W###B#W###B#W###B# 样例输出11 2 题解直接遍历每一个棋子，如果在下图中的任一个方向连成5个，则对应方加1分。 AC代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;cmath&gt;using namespace std;const int maxN = 40;int n;int a[maxN][maxN], b_score, w_score;string s;int i, j;int main() &#123; memset(a, 0, sizeof(a)); cin &gt;&gt; n; for (i = 3; i &lt; n + 3; i++) &#123; cin &gt;&gt; s; for (j = 0; j &lt; s.length(); j++) &#123; if (s[j] == 'B') &#123; a[i][j + 3] = 1; &#125; if (s[j] == 'W') &#123; a[i][j + 3] = 2; &#125; &#125; &#125; for (i = 3; i &lt; n + 3; i++) &#123; for (j = 3; j &lt; n + 3; j++) &#123; if (a[i][j] == 1) &#123; if (a[i][j - 1] == a[i][j] &amp;&amp; a[i][j - 2] == a[i][j] &amp;&amp; a[i][j + 1] == a[i][j] &amp;&amp; a[i][j + 2] == a[i][j]) &#123; b_score++; &#125; if (a[i - 1][j] == a[i][j] &amp;&amp; a[i - 2][j] == a[i][j] &amp;&amp; a[i + 1][j] == a[i][j] &amp;&amp; a[i + 2][j] == a[i][j]) &#123; b_score++; &#125; if (a[i + 1][j + 1] == a[i][j] &amp;&amp; a[i + 2][j + 2] == a[i][j] &amp;&amp; a[i - 1][j - 1] == a[i][j] &amp;&amp; a[i - 2][j - 2] == a[i][j]) &#123; b_score++; &#125; if (a[i + 1][j - 1] == a[i][j] &amp;&amp; a[i + 2][j - 2] == a[i][j] &amp;&amp; a[i - 1][j + 1] == a[i][j] &amp;&amp; a[i - 2][j + 2] == a[i][j]) &#123; b_score++; &#125; &#125; if (a[i][j] == 2) &#123; if (a[i][j - 1] == a[i][j] &amp;&amp; a[i][j - 2] == a[i][j] &amp;&amp; a[i][j + 1] == a[i][j] &amp;&amp; a[i][j + 2] == a[i][j]) &#123; w_score++; &#125; if (a[i - 1][j] == a[i][j] &amp;&amp; a[i - 2][j] == a[i][j] &amp;&amp; a[i + 1][j] == a[i][j] &amp;&amp; a[i + 2][j] == a[i][j]) &#123; w_score++; &#125; if (a[i + 1][j + 1] == a[i][j] &amp;&amp; a[i + 2][j + 2] == a[i][j] &amp;&amp; a[i - 1][j - 1] == a[i][j] &amp;&amp; a[i - 2][j - 2] == a[i][j]) &#123; w_score++; &#125; if (a[i + 1][j - 1] == a[i][j] &amp;&amp; a[i + 2][j - 2] == a[i][j] &amp;&amp; a[i - 1][j + 1] == a[i][j] &amp;&amp; a[i - 2][j + 2] == a[i][j]) &#123; w_score++; &#125; &#125; &#125; &#125; cout &lt;&lt; b_score &lt;&lt; " " &lt;&lt; w_score &lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>算法题</category>
      </categories>
      <tags>
        <tag>模拟</tag>
        <tag>程序设计竞赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序设计竞赛:讨厌的数字]]></title>
    <url>%2F%E7%AE%97%E6%B3%95%E9%A2%98%2F%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B-%E8%AE%A8%E5%8E%8C%E7%9A%84%E6%95%B0%E5%AD%97%2F</url>
    <content type="text"><![CDATA[Problem D. 讨厌的数字时间限制 1000 ms内存限制 64 MB 题目描述奶牛的生日快到了，你准备送给他一个数x作为生日礼物，x是十进制下的一个n位数，但是奶牛向你提出了一些要求。 1 奶牛准备了一个数字d，他希望x是d的倍数 2 奶牛不喜欢0和3，他不希望x中有0或3 请问有多少个不同的n位数可以作为奶牛的生日礼物呢？ 答案mod1000000007输出。 输入数据两个数字n和d，代表数字位数和奶牛给出的数字d0 &lt;= n &lt;= 10000 &lt;= d &lt;= 1000 输出数据一个1000000007之内的整数代表答案 样例输入12 3 样例输出122 题解dp[i][j]表示i位数中模d等于j的方案数。首先计算只有一位数时，满足条件的方案数作为初始值。然后依次遍历i位数下对d求余为j的方案个数；每增加一位数，就在其最后加k，因为要求数字中不能出现0和3，那么遍历时直接去除这两个数字就可以了。最后n位数下是d的倍数的方案个数就是dp[n][0]。 AC代码123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;cmath&gt;using namespace std;const int maxN = 1010;int n, d, dp[maxN][maxN];int i, j, k;int main() &#123; cin &gt;&gt; n &gt;&gt; d; // 计算一位数的方案数 for (k = 1; k &lt;= 9; k++) &#123; if (k == 3) &#123; continue; &#125; dp[1][k % d]++; &#125; // dp[i][j]表示i位数中模d等于j的方案数 for (i = 2; i &lt;= n; i++) &#123; for (j = 0; j &lt; d; j++) &#123; for (k = 1; k &lt;= 9; k++) &#123; if (k == 3) &#123; continue; &#125; dp[i][(j * 10 + k) % d] += dp[i - 1][j]; dp[i][(j * 10 + k) % d] %= 1000000007; &#125; &#125; &#125; cout &lt;&lt; dp[n][0] &lt;&lt; endl;// 输出n位数中能被d整除的满足条件的方案数 return 0;&#125;]]></content>
      <categories>
        <category>算法题</category>
      </categories>
      <tags>
        <tag>程序设计竞赛</tag>
        <tag>动态规划dp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序设计竞赛:约瑟夫环plus]]></title>
    <url>%2F%E7%AE%97%E6%B3%95%E9%A2%98%2F%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B-%E7%BA%A6%E7%91%9F%E5%A4%AB%E7%8E%AFplus%2F</url>
    <content type="text"><![CDATA[Problem C. 约瑟夫环plus时间限制 2000 ms内存限制 64 MB 题目描述考虑经典的约瑟夫环模型：n个人按顺序围成一圈，从第一个人开始报数，从1开始报，报到k这个数的人会被移出去，然后下一个人从1开始重新报数，第n个人报完数之后第1个人接着报数，问整个过程中第1个人报了几次数。 输入数据两个正整数n，k1 &lt;= n &lt;= 10000000000000000001 &lt;= k &lt;= 200 输出数据第1个人被移除之前一共报了几次数 样例输入14 4 样例输出13 样例说明注意n需要用long long存 题解cur_number存储第一个人每次轮到他时所报的号，最开始时，第一个人报号的次数res为1，cur_number也为1；从上一次这个人报号到下一次轮到他报号为止为一个周期，在这个周期内有(n + cur_number) / k个人被移出去，这个人报的号更新为(n + cur_number) % k；所以当(n + cur_number) % k = 0时，第一个人会被移出去，也就是下一次轮到他时cur_number = 0。 AC代码1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;cmath&gt;using namespace std;long long n, k, cur_number, res, temp;int main() &#123; cin &gt;&gt; n &gt;&gt; k; res = 1; cur_number = 1; while (cur_number != 0) &#123; temp = n + cur_number; n -= temp / k; cur_number = temp % k; res++; &#125; cout &lt;&lt; res &lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>算法题</category>
      </categories>
      <tags>
        <tag>模拟</tag>
        <tag>程序设计竞赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序设计竞赛:魔法师排队]]></title>
    <url>%2F%E7%AE%97%E6%B3%95%E9%A2%98%2F%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B-%E9%AD%94%E6%B3%95%E5%B8%88%E6%8E%92%E9%98%9F%2F</url>
    <content type="text"><![CDATA[Problem B. 魔法师排队时间限制 2000 ms内存限制 64 MB 题目描述有n个魔法师在排队买魔法面包，每个魔法师都有自己的魔力值，用一个正整数表示。 魔法师都不喜欢排队，如果任意时刻某个魔法师发现前面的魔法师的魔力值比自己小，那么这个魔法师就会用法术把前面的人传送到异空间。 请输出有多少个魔法师会被传送到异空间。 输入数据第一行为一个正整数n，代表魔法师的人数。 接下来一行位n个正整数，第i个正整数ai代表队伍中第i个魔法师的魔力值。（第1个魔法师在队头，第n个魔法师在队尾）1 &lt;= n &lt;= 10000001 &lt;= ai &lt;= 100000000 输出数据被传送到异空间的魔法师个数 样例输入1254 5 1 3 2 样例输出12 题解一开始的想法是：找从第一个魔法师到最后一个魔法师中的拥有最大魔力值的魔法师max_index，那么位于它之前的魔法师都将会被传送走，此具有最大魔力值的魔法师会留下来；接着寻找从max_index + 1到最后一个魔法师之间的拥有最大魔力值的魔法师，进行同样的操作，依次循环下去，直到剩下最后一个魔法师为止（最后一个魔法师肯定会留下来）；然而结果却超时了。于是换了种思维方式，既然是位于魔法师前面且比其魔力值小的魔法师会被传送走，那么最后一个魔法师肯定会留下来；直接定义一个存储最大魔力值的变量max_num，初始化为最后一个魔法师的魔力值，从后向前倒序遍历，如果当前魔法师的魔力值比max_num小，那么就把他传送走；否则，更新最大魔力值。 TLE代码1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;cmath&gt;#include &lt;vector&gt;using namespace std;const int maxN = int(1e6 + 10);int n, a[maxN], l, r, max_index, cont;int i;int main() &#123; cin &gt;&gt; n; for (i = 0; i &lt; n; i++) &#123; scanf("%d", &amp;a[i]); &#125; l = 0; r = n; while(l &lt; r) &#123; vector&lt;int&gt; v(a + l, a + r); auto max_value = max_element(v.begin(), v.end()); max_index = distance(begin(v), max_value); cont += max_index; l = l + max_index + 1; &#125; cout &lt;&lt; cont &lt;&lt; endl; return 0;&#125; AC代码1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstdio&gt;#include &lt;cstring&gt;#include &lt;cmath&gt;#include &lt;vector&gt;using namespace std;const int maxN = int(1e6 + 10);int n, a[maxN], max_num, cont;int i;int main() &#123; cin &gt;&gt; n; for (i = 1; i &lt;= n; i++) &#123; scanf("%d", &amp;a[i]); &#125; max_num = a[n]; for (i = n - 1; i &gt; 0; i--) &#123; if (a[i] &lt; max_num) &#123; cont++; &#125; else &#123; max_num = a[i]; &#125; &#125; cout &lt;&lt; cont &lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>算法题</category>
      </categories>
      <tags>
        <tag>模拟</tag>
        <tag>程序设计竞赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fdefault%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. 快速入门创建一篇新文章1$ hexo new "My New Post" 更多信息请查看： Writing 标签插件（Tag Plugins） Markdown语法(GFM)写博客 HexoEditor, 一个写 Hexo 非常好用的 Markdown 编辑器 HexoEditor 运行服务1$ hexo server 更多信息请查看： Server 生成静态文件1$ hexo generate 更多信息请查看： Generating 发布到远程站点1$ hexo deploy 更多信息请查看： Deployment 常用命令1234$ hexo clean$ hexo g$ gulp$ hexo d or 1$ hexo clean &amp;&amp; hexo g &amp;&amp; gulp &amp;&amp; hexo d 关于图片素材图片素材按官方教程说法，可统一放置在source/images目录中，并以![](/images/image.jpg) 方式引用。或者在_config.yml打开 post_asset_folder 功能，将当前文章所用的图片放置到source目录下的文章同名资源目录下，以![](image.jpg)方式引用 使用 Hexo Admin 插件Hexo Admin是一个本地在线式文章管理器，可以用直观可视化的方式新建、编辑博客文章、page页面，添加标签、分类等，并且支持剪贴板粘贴图片（自动在source_images_目录中创建文件）。 在Hexo网站目录下，安装 Hexo Admin 插件 1$ npm install --save hexo-admin 启动本地服务器并打开管理界面，即可使用 12$ hexo server -d$ open http://localhost:4000/admin/ 使用HexoEditor Markdown 编辑器 设置 npm 缓存路径 Windows 下:12npm config set prefix "C:/Program Files/nodejs/npm_global"npm config set cache "C:/Program Files/nodejs/npm_cache" Linux\Mac 下:12npm config set prefix "~/nodejs/npm_global"npm config set cache "~/nodejs/npm_cache" 注意：这里的路径是你安装 nodejs 的子目录下对应的路径 设置下载来源（镜像），加速下载12npm config set registry "https://registry.npm.taobao.org/"npm config set electron_mirror "https://npm.taobao.org/mirrors/electron/" 下载 GitHub 上最新的版本并安装123git clone https://github.com/zhuzhuyule/HexoEditor.gitcd HexoEditornpm install 启动1npm start 使用github pages服务搭建博客的好处 全是静态文件，访问速度快； 免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台； 可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的； 数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行； 博客内容可以轻松打包、转移、发布到其它平台。]]></content>
      <categories>
        <category>default</category>
      </categories>
      <tags>
        <tag>default</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于我]]></title>
    <url>%2Fabout%2Findex.html</url>
    <content type="text"><![CDATA[关于我 我叫赵鹏 男 石家庄人 现居北京 学生 联系我 Email：424107420@qq.com GitHub：zhaopeng0103 WeChat：zp18713598785 QQ：424107420]]></content>
  </entry>
  <entry>
    <title><![CDATA[分类]]></title>
    <url>%2Fcategories%2Findex.html</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[标签云]]></title>
    <url>%2Ftags%2Findex.html</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
</search>
