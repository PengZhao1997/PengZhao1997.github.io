<!DOCTYPE html><html class="theme-next gemini use-motion" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/head.gif?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/head.gif?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/head.gif?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="文本检测,旷视,CVPR2017,多方向文本,"><link rel="alternate" href="/atom.xml" title="赵鹏的博客" type="application/atom+xml"><meta name="description" content="本文提出了由全卷积网络（FCN）和非极大值抑制（NMS）两个阶段组成的场景文本检测方法，可以直接预测图像中任意方向与四边形形状的单词或文本行，消除了如候选区域聚合、文本分割等冗余过程，减少了检测时间。优点是可以检测不同方向的文本块；缺点是由于感受野不够长，对长文本的检测效果不好。"><meta name="keywords" content="文本检测,旷视,CVPR2017,多方向文本"><meta property="og:type" content="article"><meta property="og:title" content="CVPR2017 | 旷视提出自然场景文本检测方法：EAST"><meta property="og:url" content="https://zhaopeng0103.github.io/文本检测/CVPR2017-旷视提出自然场景文本检测方法：EAST/index.html"><meta property="og:site_name" content="赵鹏的博客"><meta property="og:description" content="本文提出了由全卷积网络（FCN）和非极大值抑制（NMS）两个阶段组成的场景文本检测方法，可以直接预测图像中任意方向与四边形形状的单词或文本行，消除了如候选区域聚合、文本分割等冗余过程，减少了检测时间。优点是可以检测不同方向的文本块；缺点是由于感受野不够长，对长文本的检测效果不好。"><meta property="og:locale" content="zh-Hans"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/1.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/2.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/3.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/5.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/4.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/6.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/7.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/8.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/9.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/10.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/11.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/12.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/13.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/14.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/15.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/16.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/17.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/18.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/19.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/20.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/21.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/22.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/23.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/24.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/25.png"><meta property="og:image" content="https://zhaopeng0103.github.io/images/EAST/26.png"><meta property="og:updated_time" content="2019-07-08T14:52:39.190Z"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="CVPR2017 | 旷视提出自然场景文本检测方法：EAST"><meta name="twitter:description" content="本文提出了由全卷积网络（FCN）和非极大值抑制（NMS）两个阶段组成的场景文本检测方法，可以直接预测图像中任意方向与四边形形状的单词或文本行，消除了如候选区域聚合、文本分割等冗余过程，减少了检测时间。优点是可以检测不同方向的文本块；缺点是由于感受野不够长，对长文本的检测效果不好。"><meta name="twitter:image" content="https://zhaopeng0103.github.io/images/EAST/1.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",display:"always",offset:12,b2t:!1,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="https://zhaopeng0103.github.io/文本检测/CVPR2017-旷视提出自然场景文本检测方法：EAST/"><title>CVPR2017 | 旷视提出自然场景文本检测方法：EAST | 赵鹏的博客</title></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"> <a href="https://github.com/zhaopeng0103/"><img style="position:absolute;top:0;right:0;border:0" src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png" alt="Fork me on GitHub"></a></div><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">赵鹏的博客</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">写下学习的知识，总结阅读的论文，记录遇到的问题</p></div><div class="site-nav-toggle"> <button><span class="btn-bar"></span><span class="btn-bar"></span><span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br> 归档</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br> 分类</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br> 标签</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br> 关于</a></li><li class="menu-item menu-item-search"><a href="javascript:;" class="popup-trigger"><i class="menu-item-icon fa fa-search fa-fw"></i><br> 搜索</a></li></ul><div class="site-search"><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class="search-icon"><i class="fa fa-search"></i></span><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span><div class="local-search-input-wrapper"> <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input"></div></div><div id="local-search-result"></div></div></div></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="https://zhaopeng0103.github.io/文本检测/CVPR2017-旷视提出自然场景文本检测方法：EAST/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="赵鹏"><meta itemprop="description" content=""><meta itemprop="image" content="/images/head.gif"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="赵鹏的博客"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">CVPR2017 | 旷视提出自然场景文本检测方法：EAST</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-08T13:09:59+08:00">2019-07-08</time></span> <span class="post-category"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/文本检测/" itemprop="url" rel="index"><span itemprop="name">文本检测</span></a></span></span> <span class="post-comments-count"><span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-comment-o"></i></span><a href="/文本检测/CVPR2017-旷视提出自然场景文本检测方法：EAST/#comments" itemprop="discussionUrl"><span class="post-comments-count gitment-comments-count" data-xid="/文本检测/CVPR2017-旷视提出自然场景文本检测方法：EAST/" itemprop="commentsCount"></span></a></span> <span class="post-meta-divider">|</span><span id="busuanzi_value_page_pv"></span> 次阅读<div class="post-description"> 本文提出了由全卷积网络（FCN）和非极大值抑制（NMS）两个阶段组成的场景文本检测方法，可以直接预测图像中任意方向与四边形形状的单词或文本行，消除了如候选区域聚合、文本分割等冗余过程，减少了检测时间。优点是可以检测不同方向的文本块；缺点是由于感受野不够长，对长文本的检测效果不好。</div></div></header><div class="post-body" itemprop="articleBody"><p><img src="/images/EAST/1.png" alt=""><br><strong>EAST: An Efficient and Accurate Scene Text Detector</strong><br><strong>KeyWords Plus</strong>: CVPR2017 Multi-Oriented Text<br><strong>paper</strong>：<a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdf" target="_blank" rel="noopener">http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_EAST_An_Efficient_CVPR_2017_paper.pdf</a><br><strong>reference</strong>: Zhou X, Yao C, Wen H, et al. EAST: an efficient and accurate scene text detector[C]//Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2017: 5551-5560.<br><strong>Github</strong>: <a href="https://github.com/argman/EAST" target="_blank" rel="noopener">https://github.com/argman/EAST</a></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>　　先前的场景文本检测方法已经在各种基准测试中取得了很好的效果。然而，在处理具有挑战性的情况时，即使配备了深度神经网络模型，它们通常也会达不到，因为整体性能取决于方法中多个阶段和组件的相互作用。在这项工作中，我们提出了一个简单而强大的方法，可以在自然场景中快速而准确地进行文本检测。这个方法直接预测完整图像中任意方向和四边形形状的单词或文本行，消除了使用单个神经网络的不必要的中间步骤（例如，候选聚合和字分区）。我们的方法简单，可以集中精力设计损失函数和神经网络架构。对标准数据集（包括ICDAR 2015，COCO-Text和MSRA-TD500）的实验表明，所提出的算法在准确性和效率方面明显优于最先进的方法。在ICDAR 2015数据集上，所提出的算法在720p分辨率下以13.2fps达到0.7820的F分数。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>　　最近，提取和理解自然场景中包含的文本信息变得越来越重要和受欢迎，ICDAR系列竞赛的前所未有的大量参与者以及NIST对TRAIT 2016评估的启动证明了这一点。<br>　　文本检测作为后续过程的先决条件，在文本信息提取和理解的整个过程中起着至关重要的作用。先前的文本检测方法已经在该领域的各种基准上获得了不错的表现。文本检测的核心是区分文本和背景的函数设计。传统上，函数是手动设计来捕捉场景文本的属性，而基于深度学习的方法有效的函数直接从训练数据中学习。<br><img src="/images/EAST/2.png" alt=""></p><center><font color="#0099ff" face="黑体">图1. ICDAR 2015 文本鲁棒挑战的性能与速度。可以看出，我们的算法在准确性方面明显优于竞争对手，同时运行速度非常快。使用的硬件规格列于表6。</font></center><br>　　然而，现有的方法，无论是传统的还是基于深度神经网络的，主要由几个阶段和组件组成，这些阶段和组件可能是次优的和耗时的。因此，这些方法的准确性和效率仍远远不能令人满意。<br>　　在本文中，我们提出了一个快速准确的场景文本检测方法，它只有两个阶段。该方法使用完全卷积网络（FCN）模型，该模型直接生成单词或文本行级别预测，不包括冗余和慢速中间步骤。生成的文本预测（可以是旋转的矩形或四边形）直接进行非最大值抑制以产生最终结果。根据标准基准的定性和定量实验，与现有方法相比，该算法实现了显著提高的性能，同时运行速度更快。<br>　　具体而言，所提出的算法在ICDAR 2015（在多尺度下测试时为0.8072），在MSRA-TD500上为0.7608，在COCO-Text上为0.3945时达到了0.7820的F分数，优于以前的结果：性能最先进的算法，平均花费的时间少得多（在Titan-X GPU上，我们最佳性能模型的分辨率为720p，分辨率为13.2fps，最快模型为16.8fps）。<br>　　这项工作的贡献分为三方面：<br>1. 我们提出了一种场景文本检测方法，包括两个阶段：完全卷积网络和NMS合并阶段。FCN直接生成文本区域，不包括冗余和耗时的中间步骤。<br>2. 这个方法可以灵活地生成字级或线级预测，其几何形状可以是旋转框或四边形，具体取决于具体应用。<br>3. 所提出的算法在精度和速度方面明显优于最先进的方法。<br><br>## Related Work<br>　　场景文本的检测和识别已经成为计算机视觉领域长期积极的研究课题。众多鼓舞人心的思想和有效的方法已经进行过调查。综合评论和详细分析可以在调查论文中找到。本节将重点介绍与所提算法最相关的工作。<br>　　传统方法依赖于手动设计的特征。基于笔画宽度变换（SWT）和最大稳定极值区域（MSER）的方法通常通过边缘检测或极值区域提取来寻找候选字符。张等人，利用文本的局部对称性，并为文本区域检测设计了各种特征。FASText是一个快速文本检测系统，适应和修改了众所周知的用于笔画提取的快速关键点检测器。然而，就精度和适应性而言，这些方法落后于基于深度神经网络的方法，尤其是在处理具有挑战性的场景时，例如低分辨率和几何失真。<br>　　最近，场景文本检测领域进入了一个新的时代，基于深度神经网络的算法逐渐成为主流。黄等人，首先找到使用MSER的候选框，然后使用深度卷积网络作为强分类器来修剪误报。Jaderberg等人的方法，以滑动窗口的方式扫描图像，并使用卷积神经网络模型为每个尺度生成密集的热图。后来，Jaderberg等人，同时使用CNN和ACF来搜索候选词，并使用回归进一步细化它们。田等人，开发了垂直锚点，并构建了CNN-RNN联合模型来检测水平文本行。与这些方法不同，张等人，建议利用FCN进行热图生成，并使用分量投影进行方位估计。这些方法在标准基准测试中获得了优异的性能。然而，如图2（a-d）所示，它们主要由多个阶段和组件组成，例如通过后置滤波的假阳性去除、候选聚合、线形成和字分区。多个阶段和组件可能需要进行详尽的调整，从而导致次优性能，并增加整个流水线的处理时间。<br><img src="/images/EAST/3.png" alt=""><br><center><font color="#0099ff" face="黑体">图2. 最近几个关于场景文本检测的作品的方法比较：（a）Jaderberg等人提出的水平文字检测和识别方法。（b）Zhang等人提出的多方向文本检测流程。（c）Yao等人提出的多方向文本检测流程。（d）使用CTPN进行水平文本检测，由Tian等人提出。（e）我们的方法消除了大多数中间步骤，只包含两个阶段，比以前的解决方案简单得多。</font></center><br>　　在本文中，我们设计了一个基于FCN的深层方法，直接针对文本检测的最终目标：单词或文本行级别检测。如图2（e）所示，该模型放弃了不必要的中间组件和步骤，并允许端到端的训练和优化。 由此产生的系统配备了单个轻量级神经网络，在性能和速度方面都明显优于所有以前的方法。<br><br>## Methodology<br>　　所提出的算法的关键组成部分是神经网络模型，其被训练以从完整图像直接预测文本实例及其几何形状的存在。该模型是一个完全卷积的神经网络，适用于文本检测，输出密集的逐像素单词或文本行预测。这消除了候选建议、文本区域生成和文字分区等中间步骤。后处理步骤仅包括预测几何形状的阈值和NMS。检测器被命名为EAST，因为它是一个高效精确的场景文本检测方法。<br><br>### Pipeline<br>　　我们的方法的高级概述如图2（e）所示。该算法遵循DenseBox的一般设计，其中图像被馈送到FCN，并且生成多个像素级文本得分图和几何通道。<br>　　预测通道之一是得分图，其像素值在[0;1]。其余通道表示从每个像素的视图中包围该单词的几何。分数代表在相同位置预测的几何形状的置信度。<br>　　我们已经为文本区域，旋转框（RBOX）和四边形（QUAD）实验了两种几何形状，并为每个几何设计了不同的损失函数。然后将阈值应用于每个预测区域，其中分数超过预定义阈值的几何形状被认为是有效的并保存用于以后的非极大值抑制。NMS之后的结果被认为是方法的最终输出。<br><br>### Network Design<br>　　在设计用于文本检测的神经网络时必须考虑几个因素。由于字区域的大小（如图5所示）变化很大，因此确定大字的存在需要来自神经网络后期的特征，而预测包含小字区域的精确几何需要早期阶段的低级信息。因此，网络必须使用不同等级的函数来满足这些要求。 HyperNet在特征映射上满足这些条件，但在大特征映射上合并大量通道会显著增加后续阶段的计算开销。<br>　　为了解决这个问题，我们采用U形的思想逐步合并特征图，同时保持上采样分支小。我们结合最终得到的网络既可以利用不同级别的函数，又可以保持较低的计算成本。<br><img src="/images/EAST/5.png" alt=""><br><center><font color="#0099ff" face="黑体">图3. 文本检测FCN的结构</font></center><br>　　我们模型的示意图如图3所示。该模型可以分解为三个部分：特征提取器主干、特征合并分支和输出层。主干可以是在ImageNet数据集上预先训练的卷积网络，具有交叉卷积和池化层。从主干中提取四个级别的特征图，表示为fi，其大小分别为输入图像的1/32，1/16，1/8和1/4。在图3中，描绘了PVANet。在我们的实验中，我们还采用了众所周知的VGG16模型，其中提取了pooling-2到pooling-5之后的特征映射。在特征合并分支中，我们逐渐合并它们：<br><img src="/images/EAST/4.png" alt=""><br>　　其中gi是合并基础，hi是合并的特征映射，运算符[·;·]表示沿通道轴的连接。在每个合并阶段，来自最后一个阶段的特征映射首先被馈送到反池化层以使其大小加倍，然后与当前特征映射连接。接下来，conv1×1的瓶颈减少了通道的数量并减少了计算量，接着是融合信息的conv3×3，最终产生该合并阶段的输出。在最后一个合并阶段之后，conv3×3层产生合并分支的最终特征图并将其馈送到输出层。<br>　　每个卷积的输出通道数量如图3所示。我们将分支中的卷积通道数保持较小，这样只增加了一小部分计算开销，使网络计算效率更高。最终输出层包含几个conv1×1操作，以将32个通道的特征映射投影到1通道的得分图Fs和多通道几何图Fg中。几何输出可以是RBOX或QUAD之一，在表1中。<br>　　对于RBOX，几何形状由4个通道的轴对齐边界框（AABB）R和1个通道旋转角θ表示。R的公式与下文中的公式相同，其中4个通道分别表示从像素位置到矩形的顶部、右侧，底部、左侧边界的4个距离。<br>　　对于QUAD Q，我们使用8个数字来表示从四边形的四个角点{pi|i属于{1,2,3,4}}到像素位置的坐标偏移。由于每个距离偏移包含两个数字（Δxi;Δyi），因此几何输出包含8个通道。<br><img src="/images/EAST/6.png" alt=""><br><center><font color="#0099ff" face="黑体">表1. 输出几何设计</font></center><h3 id="Label-Generation"><a href="#Label-Generation" class="headerlink" title="Label Generation"></a>Label Generation</h3><h4 id="Score-Map-Generation-for-Quadrangle"><a href="#Score-Map-Generation-for-Quadrangle" class="headerlink" title="Score Map Generation for Quadrangle"></a>Score Map Generation for Quadrangle</h4><p>　　不失一般性，我们只考虑几何是四边形的情况。 分数图上的四边形的正面积被设计为大致缩小的原始面积，如图4（a）所示。<br>　　对于四边形Q = {pi|i属于{1,2,3,4}}，其中pi = {xi; yi}是四边形上的顶点，顺时针顺序。为了缩小Q，我们首先计算每个顶点pi的参考长度ri为：<br><img src="/images/EAST/7.png" alt=""><br>　　其中D（pi; pj）是pi和pj之间的L2距离。<br>　　我们首先缩小四边形的两个较长边，然后缩小两个较短边。 对于每对两个相对的边，我们通过比较它们的长度的平均值来确定“更长”的对。 对于每个边缘&lt;pi; p（i mod 4）+ 1&gt;，我们通过将两个端点沿边缘向内移动0.3ri和0.3r（i mod 4）+1来缩小它。<br><img src="/images/EAST/8.png" alt=""></p><center><font color="#0099ff" face="黑体">图4. 标签生成过程：（a）文本四边形（黄色虚线）和收缩四边形（绿色实心）; （b）文本分数图; （c）RBOX几何图生成; （d）每个像素到矩形边界的4个通道距离; （e）旋转角度。</font></center><h4 id="Geometry-Map-Generation"><a href="#Geometry-Map-Generation" class="headerlink" title="Geometry Map Generation"></a>Geometry Map Generation</h4><p>　　如第二节所述。3.2，几何图可以是RBOX或QUAD之一。RBOX的生成过程如图4（c-e）所示。<br>　　对于那些文本区域以QUAD样式注释的数据集（例如，ICDAR 2015），我们首先生成一个旋转的矩形，覆盖区域最小的区域。然后对于每个具有正分数的像素，我们计算它到文本框的4个边界的距离，并将它们放到RBOX真值的4个通道中。对于QUAD真值，8通道几何图中具有正分数的每个像素的值是其从四边形的4个顶点的坐标偏移。</p><h3 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions"></a>Loss Functions</h3><p>　　损失可以表述为：<br><img src="/images/EAST/9.png" alt=""><br>　　其中Ls和Lg分别代表得分图和几何的损失，λg衡量两次损失之间的重要性。在我们的实验中，我们将λg设置为1。</p><h4 id="Loss-for-Score-Map"><a href="#Loss-for-Score-Map" class="headerlink" title="Loss for Score Map"></a>Loss for Score Map</h4><p>　　在大多数最先进的检测方法中，通过平衡采样和硬负采样精心处理训练图像，以解决目标物体的不平衡分布。这样做可能会提高网络性能。然而，使用这些技术不可避免地引入了不可微分的阶段和更多的参数来调谐和更复杂的方法，这与我们的设计原理相矛盾。<br>　　为了便于更简单的训练过程，我们使用文中引入的class-balanced交叉熵，由下式给出：<br><img src="/images/EAST/10.png" alt=""><br>　　其中Y ^ = Fs是得分图的预测，Y* 是真值。参数β是正样本和负样本之间的平衡因子，由下式给出：<br><img src="/images/EAST/11.png" alt=""><br>　　这种平衡的交叉熵损失首先在Yao等人的文本检测中被采用。下文作为得分图预测的目标函数。我们发现它在实践中表现良好。</p><h4 id="Loss-for-Geometries"><a href="#Loss-for-Geometries" class="headerlink" title="Loss for Geometries"></a>Loss for Geometries</h4><p>　　文本检测的一个挑战是自然场景图像中文本的大小差别很大。直接使用L1或L2损失进行回归将导致较大和较长文本区域的损失偏差。由于我们需要为大文本区域和小文本区域生成准确的文本几何预测，因此回归损失应该是规模不变的。因此，我们采用RBOX回归的AABB部分中的IoU损失，以及规模归一化的平滑L1损失用于QUAD回归。<br><strong>RBOX</strong><br>　　对于AABB部分，我们采用文中的IoU损失，因为它对不同尺度的物体是不变的。<br><img src="/images/EAST/12.png" alt=""><br>　　其中R^表示预测的AABB几何，R* 是其对应的真值。很容易看出相交矩形的宽度和高度为|R^ 交 R*|：<br><img src="/images/EAST/13.png" alt=""><br>　　其中d1、d2、d3和d4分别表示从像素到其对应矩形的顶部、右侧、底部和左侧边界的距离。并集区域由下式给出：<br><img src="/images/EAST/14.png" alt=""><br>　　因此，可以容易地计算交集/并集区域。接下来，计算旋转角度的损失：<br><img src="/images/EAST/15.png" alt=""><br>　　其中θ^是对旋转角度的预测，θ* 表示真值。最后，整体几何损失是AABB损失和角度损失的加权和，由下式给出：<br><img src="/images/EAST/16.png" alt=""><br>　　在我们的实验中λθ设置为10。<br>　　请注意，无论旋转角度如何，我们都会计算LAABB。当角度被完美预测时，这可以看作是四边形IoU的近似值。虽然在训练期间不是这种情况，但它仍然可以为网络施加正确的梯度以学习预测R^。<br><strong>QUAD</strong><br>　　我们通过添加为单词四边形设计的额外归一化项来扩展文中提出的平滑L1损失，这通常在一个方向上更长。设Q的所有坐标值都是有序集：<br><img src="/images/EAST/17.png" alt=""><br>　　其中归一化项 NQ* 是四边形的短边长度，由下式给出：<br><img src="/images/EAST/18.png" alt=""><br>和 PQ 是 Q* 的所有等效四边形的集合，具有不同的顶点排序。由于公共训练数据集中的四边形标注不一致，因此需要这种排序排列。</p><h4 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h4><p>　　使用ADAM优化器对网络进行端到端的训练。为了加快学习速度，我们从图像中均匀地采集512x512大小，形成24小时的小批量。ADAM的学习率从1e-3开始，每27300个小批量衰减到十分之一，停在1e-5。训练网络直到性能停止改善。</p><h4 id="Locality-Aware-NMS"><a href="#Locality-Aware-NMS" class="headerlink" title="Locality-Aware NMS"></a>Locality-Aware NMS</h4><p>　　为了形成最终结果，在阈值处理之后存在的几何结构应该由NMS合并。一个简单的NMS算法在O（n2）中运行，其中n是候选几何的数量，这是不可接受的，因为我们正面临来自密集预测的数万个几何形状。<br>　　假设来自附近像素的几何图形往往高度相关，我们建议逐行合并几何图形，同时在同一行中合并几何图形时，我们将迭代地合并当前遇到的几何图形和最后合并的几何图形。这种改进的技术在最佳场景下以O（n）运行。即使最坏的情况与天真的情况相同，只要位置假设成立，算法在实践中运行得足够快。该过程总结在算法1中。<br>　　值得一提的是，在WEIGHTEDMERGE（g; p）中，合并四边形的坐标通过两个给定四边形的分数进行加权平均。具体而言，如果a = WEIGHTEDMERGE（g; p），则ai = V（g）gi + V（p）pi和V（a）= V（g）+ V（p），其中ai是其中之一i的下标坐标，V（a）是几何a的得分。<br>　　实际上，我们正在“平均”而不是“选择”几何形状存在一个微妙的差异，就像在标准的NMS程序中那样，作为一种投票机制，这反过来又会在播放视频时引入稳定效果。但是，我们仍然采用“NMS”这个词进行功能描述。<br><img src="/images/EAST/19.png" alt=""></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>　　为了将提出的算法与现有方法进行比较，我们对三个公共基准集进行了定性和定量实验：ICDAR2015，COCO-Text和MSRA-TD500。</p><h3 id="Benchmark-Datasets"><a href="#Benchmark-Datasets" class="headerlink" title="Benchmark Datasets"></a>Benchmark Datasets</h3><p>　　<strong>ICDAR 2015</strong>用于ICDAR 2015鲁棒阅读比赛的挑战4。它包括总共1500张图片，其中1000张用于训练，其余用于测试。文本区域由四边形的4个顶点标注，对应于本文中的QUAD几何。我们还通过拟合具有最小面积的旋转矩形来生成RBOX输出。这些图像由Google Glass以随机的方式拍摄。因此，场景中的文本可以处于任意方向，或者遭受模糊和低分辨率。我们还使用了ICDAR 2013的229张训练图像。<br>　　<strong>COCO-Text</strong>是迄今为止最大的文本检测数据集。它重用了MS-COCO数据集中的图像。共标注了63686张图像，其中选择了43486作为训练集，其余20000作为测试。字区域以轴对齐边界框（AABB）的形式标注，这是RBOX的特例。对于此数据集，我们将角度θ设置为零。我们使用与ICDAR 2015中相同的数据处理和测试方法。<br>　　<strong>MSRA-TD500</strong>是一个包含300个训练图像和200个测试图像的数据集。文本区域具有任意方向，并在句子级别注释。与其他数据集不同，它包含英文和中文文本。文本区域以RBOX格式标注。由于训练图像的数量太少而无法学习深层模型，因此我们还利用来自HUSTTR400数据集中的400幅图像作为训练数据。</p><h3 id="Base-Networks"><a href="#Base-Networks" class="headerlink" title="Base Networks"></a>Base Networks</h3><p>　　除了COCO-Text之外，与一般物体检测的数据集相比，所有文本检测数据集相对较小，因此如果所有基准都采用单一网络，则可能会出现过拟合或欠拟合的情况。我们在所有数据集上尝试了三种不同的基本网络，具有不同的输出几何，以评估所提出的框架。表2中总结了这些网络。<br><img src="/images/EAST/20.png" alt=""><br>　　<strong>VGG16</strong>在许多任务中被广泛用作基础网络，以支持随后的任务特定的微调，包括文本检测。这个网络有两个缺点：（1）该网络的接收领域很小。conv5_3的输出中的每个像素仅具有196的感受域。（2）这是一个相当大的网络。<br>　　<strong>PVANET</strong>是文中引入的轻量级网络，旨在替代Faster-RCNN框架中的特征提取器。由于GPU太小而无法充分利用计算并行性，我们还采用PVANET2x，使原始PVANET的通道加倍，利用更多的计算并行性，同时运行速度比PVANET稍慢。这在4.5中详述。最后一个卷积层输出的感受域是809，比VGG16大得多。<br>　　模型在ImageNet数据集上进行了预训练。</p><h3 id="Qualitative-Results"><a href="#Qualitative-Results" class="headerlink" title="Qualitative Results"></a>Qualitative Results</h3><p>　　图5描绘了所提出的算法的几个检测示例。 它能够处理各种具有挑战性的场景，例如非均匀照明、低分辨率、方向多变和透视失真。此外，由于NMS程序中的投票机制，所提出的方法对具有各种形式的文本实例的视频表现出高度的稳定性。<br>　　所提出的方法的中间结果在图6中示出。可以看出，训练的模型产生高度精确的几何图和分数图，其中容易形成不同方向的文本实例的检测结果。<br><img src="/images/EAST/21.png" alt=""><br><img src="/images/EAST/22.png" alt=""></p><h3 id="Quantitative-Results"><a href="#Quantitative-Results" class="headerlink" title="Quantitative Results"></a>Quantitative Results</h3><p>　　如表3和表4所示。我们的方法在ICDAR 2015和COCO-Text上大大优于以前最先进的方法。<br>　　在ICDAR 2015 Challenge 4中，当图像以其原始比例进给时，所提出的方法实现了0.7820的Fscore。当使用相同网络在多个尺度进行测试时，我们的方法在F分数中达到0.8072，在绝对值（0.8072对0.6477）方面比最佳方法高近0.16。<br>　　使用VGG16网络比较结果，当使用QUAD输出时，所提出的方法也优于之前的最佳工作0.0924，使用RBOX输出时优于0.116。同时，这些网络非常有效，如第4.5节所示。<br><img src="/images/EAST/23.png" alt=""><br>　　在COCO-Text中，所提出的算法的所有三个设置都比先前的最佳性能者具有更高的准确度。具体而言，Fscore中的提升为0.0614，而召回时的提升为0.053，这证实了所提算法的优势，因为COCO-Text是迄今为止最大和最具挑战性的基准数据集。请注意，我们还将文中的结果作为参考包含在内，但这些结果实际上不是有效的基准方法，因为方法（A，B和C）用于数据标注。<br><img src="/images/EAST/24.png" alt=""><br>　　与先前方法相比，所提算法的提升证明，直接针对最终目标并消除冗余过程的简单文本检测方法可以击败精心设计的方法，甚至是那些与大型神经网络模型集成的方法。<br>　　如表5所示，在MSRA-TD500上我们所有的三种设置方法都取得了优异的效果。表现最佳的Fscore（Ours + PVANET2x）略高于其他方法。与Zhang等人的方法相比，先前发表的最先进的系统，表现最佳的（Ours + PVANET2x）F分数提高了0.0208，精度提高了0.0428。<br><img src="/images/EAST/25.png" alt=""><br>　　请注意，在MSRA-TD500上，加上VGG16的算法比使用PVANET和PVANET2x（0.7023与0.7445和0.7608）相比差得多，主要原因是VGG16的有效感受野小于PVANET和PVANET2x，而MSRA-TD500的评估协议要求文本检测算法输出行级而不是字级预测框。<br>　　此外，我们还根据ICDAR 2013基准评估了Ours + PVANET2x。它在召回率，精度和F值方面达到0.8267,0.9264和0.8737，与之前的最新方法相当，后者在召回率，精度和F值方面分别获得0.8298,0.9298和0.8769。</p><h3 id="Speed-Comparison"><a href="#Speed-Comparison" class="headerlink" title="Speed Comparison"></a>Speed Comparison</h3><p>　　表6中展示了整体速度比较。<br><img src="/images/EAST/26.png" alt=""><br>　　我们报告的数字是使用我们表现最佳的网络，通过ICDAR 2015数据集以原始分辨率（1280x720）运行500个测试图像的平均值。这些实验是在服务器上使用具有Maxwell架构和Intel E5-2670 v3 @ 2.30GHz CPU的单个NVIDIA Titan X图形卡进行的。对于所提出的方法，后处理包括阈值处理和NMS，而其他的应该参考他们的原始论文。<br>　　虽然所提出的方法明显优于最先进的方法，但计算成本保持很低，归因于简单而有效的流水线。从Tab可以看出。 6，我们方法的最快设置以16.8 FPS的速度运行，而最慢设置以6.52 FPS运行。即使是性能最佳的型号Ours + PVANET2x也能以13.2 FPS的速度运行。这证实了我们的方法是最有效的文本检测器之一，可以在基准测试中实现最先进的性能。</p><h3 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h3><p>　　检测器可以处理的文本实例的最大大小与网络的感受野成比例。这限制了网络预测更长文本区域的能力，例如跨越图像的文本行。<br>　　此外，该算法可能会漏掉或给出垂直文本实例的不精确预测，因为它们仅占ICDAR 2015训练集中的一小部分文本区域。</p><h2 id="Conclusion-and-Future-Work"><a href="#Conclusion-and-Future-Work" class="headerlink" title="Conclusion and Future Work"></a>Conclusion and Future Work</h2><p>　　我们已经提出了一个场景文本检测器，它使用单个神经网络直接从完整图像生成单词或行级预测。通过结合适当的损失函数，检测器可以根据具体应用预测文本区域的旋转矩形或四边形。在标准基准测试上的实验证实，所提出的算法在准确性和效率方面基本上优于先前的方法。<br>　　未来研究的可能方向包括：（1）调整几何公式以允许直接检测弯曲文本; （2）将检测器与文本识别器集成; （3）将思想扩展到一般物体检测。</p></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/文本检测/" rel="tag"># 文本检测</a> <a href="/tags/旷视/" rel="tag"># 旷视</a> <a href="/tags/CVPR2017/" rel="tag"># CVPR2017</a> <a href="/tags/多方向文本/" rel="tag"># 多方向文本</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/语义分割/未知2018-北理、旷视、北大联合提出PAN，用于语义分割/" rel="next" title="未知2018 | 北理、旷视、北大联合提出PAN，用于语义分割"><i class="fa fa-chevron-left"></i> 未知2018 | 北理、旷视、北大联合提出PAN，用于语义分割</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"></div></div></footer></div></article><div class="post-spread"><div class="bdsharebuttonbox"><a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a><a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a><a href="#" class="bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a><a href="#" class="bds_qzone" data-cmd="qzone" title="分享到QQ空间"></a><a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a><a href="#" class="bds_tieba" data-cmd="tieba" title="分享到百度贴吧"></a><a href="#" class="bds_twi" data-cmd="twi" title="分享到Twitter"></a><a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a><a href="#" class="bds_more" data-cmd="more"></a><a class="bds_count" data-cmd="count"></a></div><script>window._bd_share_config={common:{bdText:"",bdMini:"2",bdMiniList:!1,bdPic:""},share:{bdSize:"16",bdStyle:"0"},image:{viewList:["tsina","douban","sqq","qzone","weixin","twi","fbook"],viewText:"分享到：",viewSize:"16"}}</script><script>with(document)(getElementsByTagName("head")[0]||body).appendChild(createElement("script")).src="//bdimg.share.baidu.com/static/api/js/share.js?cdnversion="+~(-new Date/36e5)</script></div></div></div><div class="comments" id="comments"><div id="SOHUCS"></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap"> 文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap"> 站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" src="/images/head.gif" alt="赵鹏"><p class="site-author-name" itemprop="name">赵鹏</p><p class="site-description motion-element" itemprop="description">愿你出走半生，归来仍是少年。愿你三冬暖，愿你春不寒；愿你天黑有灯，下雨有伞。愿你路上有良人相伴。</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">28</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/index.html"><span class="site-state-item-count">10</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/index.html"><span class="site-state-item-count">37</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zhaopeng0103/" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i> GitHub</a></span><span class="links-of-author-item"><a href="mailto:424107420@qq.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i> E-Mail</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-block"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://blog.csdn.net/zhuzhuyule/article/details/58347687/" title="Hexo下的Markdown语法(GFM)写博客" target="_blank">Hexo下的Markdown语法(GFM)写博客</a></li><li class="links-of-blogroll-item"> <a href="https://www.simon96.online/2018/10/12/hexo-tutorial/" title="最全Hexo博客搭建+主题优化+插件配置+常用操作+错误分析" target="_blank">最全Hexo博客搭建+主题优化+插件配置+常用操作+错误分析</a></li><li class="links-of-blogroll-item"> <a href="http://www.dnbbn.com/" title="达牛帮帮你" target="_blank">达牛帮帮你</a></li><li class="links-of-blogroll-item"> <a href="https://www.jianshu.com/p/08b7048ec925" title="Git与Bitbucket配合使用教程" target="_blank">Git与Bitbucket配合使用教程</a></li></ul></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">2.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Label-Generation"><span class="nav-number">2.1.</span> <span class="nav-text">Label Generation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Score-Map-Generation-for-Quadrangle"><span class="nav-number">2.1.1.</span> <span class="nav-text">Score Map Generation for Quadrangle</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Geometry-Map-Generation"><span class="nav-number">2.1.2.</span> <span class="nav-text">Geometry Map Generation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Loss-Functions"><span class="nav-number">2.2.</span> <span class="nav-text">Loss Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Loss-for-Score-Map"><span class="nav-number">2.2.1.</span> <span class="nav-text">Loss for Score Map</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Loss-for-Geometries"><span class="nav-number">2.2.2.</span> <span class="nav-text">Loss for Geometries</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training"><span class="nav-number">2.2.3.</span> <span class="nav-text">Training</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Locality-Aware-NMS"><span class="nav-number">2.2.4.</span> <span class="nav-text">Locality-Aware NMS</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Experiments"><span class="nav-number">3.</span> <span class="nav-text">Experiments</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Benchmark-Datasets"><span class="nav-number">3.1.</span> <span class="nav-text">Benchmark Datasets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Base-Networks"><span class="nav-number">3.2.</span> <span class="nav-text">Base Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Qualitative-Results"><span class="nav-number">3.3.</span> <span class="nav-text">Qualitative Results</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Quantitative-Results"><span class="nav-number">3.4.</span> <span class="nav-text">Quantitative Results</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Speed-Comparison"><span class="nav-number">3.5.</span> <span class="nav-text">Speed Comparison</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Limitations"><span class="nav-number">3.6.</span> <span class="nav-text">Limitations</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion-and-Future-Work"><span class="nav-number">4.</span> <span class="nav-text">Conclusion and Future Work</span></a></li></ol></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2019</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">赵鹏</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div><div><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script> <span id="busuanzi_container_site_pv" style="display:none">本站总访问量<span id="busuanzi_value_site_pv"></span> 次 <span class="post-meta-divider">|</span></span> <span id="busuanzi_container_site_uv" style="display:none">本站总访问人数<span id="busuanzi_value_site_uv"></span> 人次</span></div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script><script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css"><script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script><script type="text/javascript">function renderGitment(){new Gitmint({id:window.location.pathname,owner:"zhaopeng0103",repo:"zhaopeng0103.github.io",lang:navigator.language||navigator.systemLanguage||navigator.userLanguage,oauth:{client_secret:"58842d639b5bdbc203cf162e37e593720b27514d",client_id:"c39120ca34af3609aa2c"}}).render("gitment-container")}renderGitment()</script><script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script></body></html>